---
title: "Application 1 - Donn√©es spatiales"
custom-callout:
  exercice:
    icon-symbol: "üìù"
    color: "pink"
filters:
- custom-callout
format: 
   html:
     df-print: paged
---

<a href="https://datalab.sspcloud.fr/launcher/ide/rstudio?name=Appli1&version=2.1.17&s3=region-ec97c721&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2FInseeFrLab%2Fcours-nouvelles-donnees-site%2Frefs%2Fheads%2Fmain%2Fapplications%2Fapp1-init.sh¬ª&networking.user.enabled=true&autoLaunch=true" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSP%20Cloud-Ouvrir_dans_RStudio-blue?logo=rstudio&amp;logoColor=blue" alt="Onyxia"></a>


::: {.callout-important collapse="true"}
## Pr√©parer son environnement pour ce TD

Les donn√©es ont √©t√© pr√©par√©es en amont de ce TD. Un lien de lancement rapide est disponible ci-dessus qui met √† disposition un environnement pr√© √† l'emploi sur le [SSPCloud](https://datalab.sspcloud.fr/). 

Apr√®s avoir cliqu√© sur le bouton, il convient de cr√©er un projet `RStudio` depuis le dossier `appli1` (`File > New Project`) :

![](/applications/new_project.png)

`Git` est normalement pr√©configur√© dans ce dossier, vous pourrez donc _pusher_ votre travail sur `Github` si vous cr√©ez un d√©p√¥t dessus. 

Si la r√©cup√©ration des donn√©es a √©chou√© pour une raison _x_ ou _y_, vous pouvez lancer la r√©cup√©ration des donn√©es en copiant ce code dans un terminal 

```{.bash}
#!/bin/bash

mkdir -p appli1
cd appli1
echo "data/" >> .gitignore
git init
git branch -m main

mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet data/dvf.parquet
mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet data/carreaux.parquet
mc cp s3/projet-formation/nouvelles-sources/data/triangle.geojson data/triangle.geojson
mc cp s3/projet-formation/nouvelles-sources/data/malakoff.geojson data/malakoff.geojson
mc cp s3/projet-formation/nouvelles-sources/data/montrouge.geojson data/montrouge.geojson
```

```{r}
#| echo: false
#| output: false
dir.create("data")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/triangle.geojson", "data/triangle.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/malakoff.geojson", "data/malakoff.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/montrouge.geojson", "data/montrouge.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet", "data/dvf.parquet")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet", "data/carreaux.parquet")
```

:::


En premier lieu, ce TD utilise une source administrative nomm√©e __DVF__ (_¬´ Demandes de Valeurs Fonci√®res ¬ª_). 

- Les donn√©es DVF recensent l'ensemble des ventes de biens fonciers r√©alis√©es au cours des derni√®res ann√©es, en m√©tropole et dans les d√©partements et territoires d'outre-mer ‚Äî sauf √† Mayotte et en Alsace-Moselle. Les biens concern√©s peuvent √™tre b√¢tis (appartement et maison) ou non b√¢tis (parcelles et exploitations). Les donn√©es sont produites par la Direction g√©n√©rale des finances publiques (DGFip). Elles proviennent des actes enregistr√©s chez les notaires et des informations contenues dans le cadastre. Cette base a √©t√© filtr√©e de mani√®re √† √™tre la plus p√©dagogique possible pour cette formation.

L'analyse de ces donn√©es sera compl√©t√©e des donn√©es **Filosofi** produites par l'Insee:

- Les donn√©es spatiales carroy√©es √† 200m, produites par l'Insee √† partir du dispositif **Filosofi**, contentant des informations socio-√©conomiques sur les m√©nages.

Enfin, nous proposons trois contours g√©ographiques _ad hoc_:

- La commune de Malakoff
- La commune de Montrouge
- Le "Triangle d'or" de Malakoff (autrement dit, son centre-ville √† peu de choses pr√®s)

L'objectif de ce TD est d'illustrer la mani√®re dont peuvent √™tre trait√©es des donn√©es spatiales de mani√®re flexible avec `duckdb`.


# Pr√©paration de l'environnement

Les librairies suivantes seront utilis√©es dans ce TD, vous pouvez d'ores et d√©j√† les charger dans votre environnement. 

```{r}
#| output: false

library(duckdb)
library(glue)
library(dplyr)
library(dbplyr)
library(mapview)
```

Si celles-ci ne sont pas install√©es, vous pouvez faire en console un `install.packages` (voir @note-bp-install).

::: {#note-bp-install .callout-note}

Les installations de _packages_ sont √† faire en console mais ne doivent pas √™tre √©crites dans le code. Bien que ce ne soit pas l'objet de ce cours, il est utile de suivre les bonnes pratiques recommand√©es √† l'Insee et plus largement dans le monde {{< fa brands r-project >}}. 

Pour en savoir plus, vous pourrez explorer [le portail de formation aux bonnes pratiques](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/). 

:::

Nous allons avoir besoin des codes Insee suivants pour notre application:


```{r}
cog_malakoff <- "92046"
cog_montrouge <- "92049"
```


## Import des donn√©es

L'import des contours en {{< fa brands r-project >}} se fait assez naturellement gr√¢ce √† [`sf`](https://r-spatial.github.io/sf/).

```{r}
triangle <- sf::st_read("data/triangle.geojson", quiet=TRUE)
malakoff <- sf::st_read("data/malakoff.geojson", quiet=TRUE)
montrouge <- sf::st_read("data/montrouge.geojson", quiet=TRUE)
```

En premier lieu, on peut visualiser la ville de `Malakoff`:

```{r}
mapview(malakoff) + mapview(triangle, col.regions = "#ffff00")
```

Et ensuite les contours de Montrouge:

```{r}
mapview(montrouge)
```


## Pr√©paration de DuckDB

`DuckDB` est un moteur de base de donn√©es analytique en m√©moire, optimis√© pour les requ√™tes SQL sur des donn√©es volumineuses, particuli√®rement adapt√© aux fichiers plats comme `Parquet` ou CSV, et int√©grable dans des langages comme Python, R ou SQL.

En principe, `duckdb` fonctionne √† la mani√®re d'une base de donn√©es. Autrement dit, on d√©finit une base de donn√©es et effectue des requ√™tes (SQL ou verbes `tidyverse`) dessus. Pour cr√©er une base de donn√©es, il suffit de faire un `read_parquet` avec le chemin du fichier.  

La base de donn√©es se cr√©e tout simplement de la mani√®re suivante:

```{r}
#| output: false
#| echo: true

con <- dbConnect(duckdb::duckdb())
dbExecute(con, "INSTALL spatial;")
dbExecute(con, "LOAD spatial;")
```

Nous verrons ult√©rieurement pourquoi nous avons besoin de cette extension spatiale. 

Cette connexion `duckdb` peut √™tre utilis√©e de plusieurs mani√®res. En premier lieu, par le biais d'une requ√™te SQL. `dbGetQuery` permet d'avoir le r√©sultat sous forme de _dataframe_ puisque la requ√™te est d√©l√©gu√©e √† l'utilitaire `duckdb` qui est embarqu√© dans les fichiers de la librairie:

```{r}
#| label: show-dbquery-principle
#| output: false
out <- dbGetQuery(
  con,
  glue(  
    'SELECT * EXCLUDE (geometry) FROM read_parquet("data/dvf.parquet") LIMIT 5'
  )
)
out
```


La cha√Æne d'ex√©cution ressemble ainsi √† celle-ci:

![](https://raw.githubusercontent.com/linogaliana/parquet-recensement-tutomate/main/img/duckdb-delegation1.png){fig-align="center"}

M√™me si `DuckDB` simplifie l'utilisation du SQL en proposant de nombreux verbes auxquels on est familier en `R` ou `Python`, SQL n'est n√©anmoins pas toujours le langage le plus pratique pour cha√Æner des op√©rations nombreuses. Pour ce type de besoin,  le `tidyverse` offre une grammaire riche et coh√©rente. Il est tout √† fait possible d'interfacer une base `duckdb` au `tidyverse`. On pourra donc utiliser nos verbes pr√©f√©r√©s (`mutate`, `filter`, etc.) sur un objet `duckdb`: une phase pr√©liminaire de traduction en SQL sera automatiquement mise en oeuvre:

![](https://raw.githubusercontent.com/linogaliana/parquet-recensement-tutomate/main/img/duckdb-delegation2.png){fig-align="center"}

```{r}
#| message: false
#| label: create-connections
table_logement <- tbl(con, glue('read_parquet("data/dvf.parquet")'))
table_logement %>% head(5)
```


# Partie 1 : Prix immobiliers √† Malakoff et √† Montrouge

Dans cette partie, l'objectif est d'extraire de l'informations d'une base de donn√©es volumineuse √† l'aide de DuckDB. Pour le moment, le caract√®re spatial des donn√©es est mis de c√¥t√© : on d√©couvre et on traite les donn√©es via des requ√™tes attributaires classiques.

Tentons, par une premi√®re s√©rie d'exercices, de comparer la m√©diane des prix des transactions immobili√®res √† Malakoff et √† Montrouge.

Dans cette partie, nous allons pouvoir faire nos traitements de donn√©es avec SQL et/ou `tidyverse`. Cela illustre l'une des forces de duckdb, √† savoir son excellente int√©gration avec d'autres √©cosyst√®mes dont nous sommes familiers.

Lorsque nous irons sur l'aspect spatial, on devra passer en SQL pur, l'√©cosyst√®me tidyverse n'√©tant pas encore finalis√© pour le traitement de donn√©es spatiales avec `duckdb`. 


## Premi√®res requ√™tes SQL: description des donn√©es DVF

Tout d'abord, il convient de se familiariser avec les donn√©es. Les requ√™tes propos√©es pour l'exercice 1 permettent d'obtenir des informations primordiale de mani√®re tr√®s rapide et sans n√©cessit√© de charger l'ensemble des donn√©es dans la m√©moire vive.


::: {.exercice}
## Exercice 1

Cet exercice nous fera rentrer progressivement dans les donn√©es √† partir de quelques requ√™tes basiques.

1. Lire les 10 premi√®res lignes des donn√©es par l'approche SQL et par l'approche tidyverse.
2. Afficher les noms des colonnes selon les deux approches.
3. Regarder les valeurs uniques de la colonne `nature_mutation` selon les deux approches. 
4. Calculer les bornes min et max des prix des transactions selon ces deux approches.
:::

A la question 1, vous devriez avoir:

```{r}
#| echo: false
preview_q1 <- dbGetQuery(con, "SELECT * FROM read_parquet('data/dvf.parquet') LIMIT 10")
preview_q1_bis <- table_logement %>% head(10)
preview_q1
```

A la question 2, la liste des colonnes donnera plut√¥t

```{r}
#| echo: false
describe_dvf <- dbGetQuery(con, "DESCRIBE SELECT * FROM read_parquet('data/dvf.parquet')")
table_logement %>% colnames(.)
```

Que contient le champ `nature_mutation` ? (il a √©t√© filtr√© au ventes classiques pour simplifi√© cette application ; les vraies donn√©es sont plus riches).


```{r}
#| echo: false
unique_nature_mutation <- dbGetQuery(con, "SELECT DISTINCT nature_mutation FROM read_parquet('data/dvf.parquet')")
unique_nature_mutation_bis <- table_logement %>% distinct(nature_mutation)
unique_nature_mutation_bis
```

A la question 4, vous devriez obtenir des statistiques similaires √† celles-ci:

```{r}
#| echo: false
stats_q4 <- dbGetQuery(con, "
        SELECT
            MIN(valeur_fonciere) AS min_valeur,
            MAX(valeur_fonciere) AS max_valeur
        FROM read_parquet('data/dvf.parquet')
           ")
stats_q4_bis <- table_logement %>%
  summarise(
      min_valeur = min(valeur_fonciere, na.rm = TRUE),
      max_valeur = max(valeur_fonciere, na.rm = TRUE)
    )
stats_q4_bis
```

Nous venons de voir comment faire quelques requ√™tes basiques sur un geoparquet avec `duckdb` et l'√©quivalence entre les approches SQL et tidyverse. La derni√®re question √©tait d√©j√† une introduction au calcul √† la vol√©e de statistiques descriptives, ajoutons quelques statistiques avec ce nouvel exercice. 

::: {.exercice}
## Exercice 2: statistiques descriptives sur la dimension attributaire

Ne garder que les seules transactions effectu√©es √† Montrouge ou Malakoff et faire une m√©diane par communes des montants des transactions

_Faire ceci avec SQL et dplyr_[^chatGPT]

:::

[^chatGPT]: Vous avez le droit d'utiliser `chatGPT` ou `Claude` ou vous IA assistante pr√©f√©r√©e! Mais ne prenez pas pour argent comptant ce qu'elle vous propose.

Avec l'approche SQL vous devriez obtenir


```{r}
#| echo: false
query1 <- glue("
    FROM read_parquet('data/dvf.parquet')
    SELECT
        code_commune,
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY valeur_fonciere) AS mediane_valeur_fonciere
    WHERE code_commune IN ('{cog_malakoff}', '{cog_montrouge}')
    GROUP BY code_commune
")

result1 <- dbGetQuery(con, query1)
print(result1)

```

On peut se rassurer, on obtient la m√™me chose avec `duckdb`:

```{r}
#| echo: false
table_logement %>%
  filter(code_commune %in% c(cog_malakoff, cog_montrouge)) %>% 
  group_by(code_commune) %>%
  summarise(mediane_valeur_fonciere = median(valeur_fonciere, na.rm = TRUE))
```


On peut en conclure que les biens vendus √† Montrouge (dans notre base) ont une m√©diane un peu plus √©lev√©e qu'√† Malakoff.


# Partie 2 : les prix immobiliers √† Malakoff, dans le centre et en-dehors.

√Ä pr√©sent, nous souhaitons avoir des informations sur les transactions effectu√©es dans le ¬´ fameux ¬ª Triangle d'Or de Malakoff (plus prosa√Øquement, dans son centre-ville commer√ßant).

Comme il n'est pas possible de distinguer cette zone par requ√™tes attributaires, nous proposons de : 

1. Via `DuckDB`, extraire les transactions de l'ensemble de la commune de Malakoff tout en conservant leur caract√®re spatial (chaque transaction correspond √† un point g√©ographique, avec ses coordonn√©es xy).
2. Utiliser localement le package `sf` pour distinguer spatialement les transactions effectu√©es √† l'int√©rieur ou √† l'ext√©rieur du Triangle d'Or (dont nous fournissons les contours).
3. Calculer la m√©diane des prix dans les deux sous-zones.

::: {.callout-tip collapse="true"}
## Format des g√©om√©tries
On extrait les transactions de Malakoff. Pour information, dans le fichier `dvf.parquet`, les coordonn√©es spatiales sont stock√©es dans un format binaire sp√©cifique (Well-Known Binary - WKB). Ce format est efficace pour le stockage et les calculs, mais n'est pas directement lisible ou interpr√©table par les humains.


En transformant ces g√©om√©tries en une repr√©sentation texte lisible (Well-Known Text - WKT) avec `ST_AsText`, on rend les donn√©es spatiales faciles √† afficher, interpr√©ter ou manipuler dans des contextes qui ne supportent pas directement les formats binaires g√©ospatiaux.
:::


Pour le prochain exercice, nous aurons besoin de la structure de requ√™te suivante pour bien interpr√©ter la dimension g√©ographique:

```{.r}
FROM ...
SELECT
  x, y, ..., ST_AsText(geometry) AS geom_text
WHERE ...
```

::: {.exercice}
## Exercice 3

1. En vous inspirant du _template_ ci-dessus, cr√©er un _dataframe_ `transactions_malakoff` qui recense les transactions dans cette charmante bourgade. 

2. A ce niveau, les transactions extraites sont maintenant charg√©es en m√©moire et on les transforme dans un format qui facilite leur manipulation en R via le package `sf`.


```{.r}
transactions_malakoff <- 
  sf::st_as_sf(transactions_malakoff, wkt = "geom_text", crs = 2154) |>
  rename(geometry=geom_text)
```

3. Nous allons cr√©er un masque pour reconna√Ætre les transactions qui sont situ√©es ou non dans le triangle d'or. Utiliser la structure suivante pour cr√©er ce masque

```{.r}
bool_mask <- transactions_malakoff |> 
  # ... |>
  sf::st_intersects(triangle, sparse = FALSE)
```

‚ö†Ô∏è il faut tenir compte des projections g√©ographiques avant de faire l'op√©ration d'intersection. Ce code est donc √† amender √† la marge pour pouvoir faire l'intersection. 

Cela donne un vecteur de bool√©en, on peut donc identifier les transactions dans le triangle d'or ou en dehors √† partir de celui-ci. 

:::

```{r}
#| echo: false
query2 <- glue("
    FROM read_parquet('data/dvf.parquet')
    SELECT
        code_commune,
        valeur_fonciere,
        ST_AsText(geometry) AS geom_text
    WHERE code_commune = '{cog_malakoff}'
")

transactions_malakoff <- dbGetQuery(con, query2)

transactions_malakoff
```


```{r}
#| echo: false
transactions_malakoff <- 
  sf::st_as_sf(transactions_malakoff, wkt = "geom_text", crs = 2154) |>
  rename(geometry=geom_text)

transactions_malakoff
```

Une fois les donn√©es pr√™tes, on intersecte les points avec le triangle repr√©sentant le centre-ville de Malakoff (question 3)


```{r}
#| echo: false
bool_mask <- transactions_malakoff |> 
  sf::st_transform(4326) |> 
  sf::st_intersects(triangle, sparse = FALSE)

head(bool_mask)
```

On peut ensuite facilement cr√©er nos deux espaces de Malakoff:

```{r}
#| echo: true
in_triangle <- transactions_malakoff[bool_mask,]
out_triangle <- transactions_malakoff[!bool_mask,]
```

Une fois que chaque transaction est identifi√©e comme √©tant √† l'int√©rieur ou √† l'ext√©rieur du Triangle, le calcul de la m√©diane des prix est imm√©diat.

```{r}
median_in <- median(in_triangle$valeur_fonciere)
median_out <- median(out_triangle$valeur_fonciere)

print(glue("M√©diane des prix dans le Triangle d'Or de Malakoff : ", median_in))
print(glue("M√©diane des prix dans le reste de Malakoff : ", median_out))
```

La m√©diane des prix est un peu plus √©lev√©e dans le Triangle qu'en dehors. On peut aller au-del√† et √©tudier la distribution des transactions. Bien que la taille d'√©chantillon soit r√©duite, on a ainsi une id√©e de la diversit√© des prix √† Malakoff.


```{r}
#| code-fold: true
#| code-summary: "Produire la figure sur la distribution du prix des biens"
library(ggplot2)
library(scales)

malakoff_identified <- transactions_malakoff %>%
  mutate(
    region = if_else(as.logical(bool_mask), "Triangle d'or", "Hors triangle d'or")
  ) 

ggplot(
  malakoff_identified,
  aes(y = valeur_fonciere, x = region, fill = region)
) +
  geom_violin() +
  scale_y_continuous(
    trans = "log10",
    labels = comma_format(),
    breaks = scales::trans_breaks("log10", function(x) 10^x)
  ) +
  geom_jitter(height = 0, width = 0.1) +
  labs(y = "Valeur de vente (‚Ç¨)") +
  theme_minimal()
```

Tout ceci ne nous dit rien de la diff√©rence entre les biens dans le triangle et en dehors de celui-ci. Nous n'avons fait aucun contr√¥le sur les caract√©ristiques des biens. Nous laissons les curieux explorer la mine d'or qu'est cette base. 


# Partie 3 : Part de m√©nages pauvres √† Malakoff et √† Montrouge

Pour finir, on se place dans le cas o√π : 

- On souhaite extraire des informations d'un fichier volumineux (les donn√©es carroy√©es de l'Insee).
- Mais il n'est pas possible de filtrer les donn√©es par des requ√™tes attributaires (par exemple, il n'est pas possible de faire `code_commune = 92049`).

Ainsi, nous allons : 

- Utiliser les contours g√©ographiques des deux communes
- Filtrer les donn√©es par intersections g√©ographiques des carreaux et des communes, √† l'aide de `DuckDB`
- Faire les calculs localement apr√®s l'extraction des carreaux d'int√©r√™t.

Pour commencer, on d√©crit les donn√©es carroy√©es comme pr√©c√©demment :

```{r}
describe_dvf <- dbGetQuery(con, "DESCRIBE SELECT * FROM read_parquet('carreaux.parquet')")
describe_dvf
```

```{r}
preview <- dbGetQuery(con, "SELECT * FROM read_parquet('carreaux.parquet') LIMIT 10")
preview
```

On va faire une petite transformation de donn√©es pr√©liminaire √† cet exercice afin que la g√©om√©trie de Malakoff soit reconnue par `DuckDB`. 

```{r}
malakoff_2154 <- sf::st_transform(malakoff, 2154)
malakoff_wkt <- sf::st_as_text(sf::st_geometry(malakoff_2154))
```

Voici comment faire une requ√™te g√©ographique sur les carreaux de Malakoff

```{r}
geo_query <- glue("
  FROM read_parquet('data/carreaux.parquet')
  SELECT
      *, ST_AsText(geometry) AS geom_text
  WHERE ST_Intersects(
      geometry,
      ST_GeomFromText('{malakoff_wkt}')
  )
")

carr_malakoff <- dbGetQuery(con, geo_query)

carr_malakoff <-
  carr_malakoff |>
  sf::st_as_sf(wkt = "geom_text", crs = 2154) |>
  select(-geometry) |>
  rename(geometry=geom_text)
```

On peut les visualiser de la mani√®re suivante

```{r}
mapview(carr_malakoff) + mapview(sf::st_boundary(malakoff)) 
```


::: {.exercice}
## Exercice 4: extraction des carreaux intersectant Malakoff

1. R√©it√®rer l'op√©ration pour Montrouge
2. Calculer la proportion moyenne de m√©nages pauvre dans l'ensemble des carreaux extraits √† partir des deux objets obtenus.

:::

Le masque des carreaux de Montrouge est le suivant:

```{r}
#| echo: false
montrouge_2154 <- sf::st_transform(montrouge, 2154)
montrouge_wkt <- sf::st_as_text(sf::st_geometry(montrouge_2154))

geo_query <- glue("
  FROM read_parquet('data/carreaux.parquet')
  SELECT
      *, ST_AsText(geometry) AS geom_text
  WHERE ST_Intersects(
      geometry,
      ST_GeomFromText('{montrouge_wkt}')
  )
")

carr_montrouge <- dbGetQuery(con, geo_query)

carr_montrouge <-
  carr_montrouge |>
  sf::st_as_sf(wkt = "geom_text", crs = 2154) |>
  select(-geometry) |>
  rename(geometry=geom_text)

mapview(carr_montrouge) + mapview(sf::st_boundary(montrouge)) 
```

On obtient, _in fine_, les statistiques suivantes

```{r}
#| echo: false
mean_menpauvres_malakoff <- round(100 * sum(carr_malakoff$men_pauv) / sum(carr_malakoff$men), 2)
mean_menpauvres_montrouge <- round(100 * sum(carr_montrouge$men_pauv) / sum(carr_montrouge$men), 2)

print(glue("Part de m√©nages pauvres dans les carreaux de Malakoff : ", mean_menpauvres_malakoff))
print(glue("Part de m√©nages pauvres dans les carreaux de Montrouge : ", mean_menpauvres_montrouge))
```


::: {.exercice}
## Exercice optionnel

Calculer la m√™me statistique dans et hors du triangle d'or de Malakoff

:::


# Conclusion

Nous avons donc r√©ussi √† lire des donn√©es avec `DuckDB` et √† faire des statistiques dessus. Pourquoi est-ce pertinent de passer par `DuckDB` ? Car ce package permet de faire ceci de mani√®re tr√®s efficace sur de gros volumes de donn√©es. Il passe tr√®s bien √† l'√©chelle. 

A noter que notre d√©marche est une introduction √† ce sujet bien plus large qu'est l'analyse g√©ographique. Notre approche serait am√©liorable sur plusieurs plans : 

- rationalisation des requ√™tes,
- pertinence statistique des r√©sultats
- r√©plicabilit√© du code


