---
title: "Application 1 - Donn√©es spatiales"
custom-callout:
  exercice:
    icon-symbol: "üìù"
    color: "pink"
filters:
- custom-callout
format: 
   html:
     df-print: paged
echo: true
---

<a href="https://datalab.sspcloud.fr/launcher/ide/rstudio?name=Appli1&version=2.1.17&s3=region-ec97c721&init.personalInit=¬´https%3A%2F%2Fraw.githubusercontent.com%2FInseeFrLab%2Fcours-nouvelles-donnees-site%2Frefs%2Fheads%2Fmain%2Fapplications%2Fapp1-init.sh¬ª&networking.user.enabled=true&autoLaunch=true" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSP%20Cloud-Ouvrir_dans_RStudio-blue?logo=rstudio&amp;logoColor=blue" alt="Onyxia"></a>


::: {.callout-important collapse="true"}
## Pr√©parer son environnement pour ce TD

Les donn√©es ont √©t√© pr√©par√©es en amont de ce TD. Un lien de lancement rapide est disponible ci-dessus qui met √† disposition un environnement pr√™t √† l'emploi sur le [SSPCloud](https://datalab.sspcloud.fr/). 

Apr√®s avoir cliqu√© sur le bouton, il convient de cr√©er un projet `RStudio` depuis le dossier `appli1` (`File > New Project`) :

![](/applications/new_project.png)

`Git` est normalement pr√©configur√© dans ce dossier, vous pourrez donc _pusher_ votre travail sur `Github` si vous cr√©ez un d√©p√¥t dessus. 

Si la r√©cup√©ration des donn√©es a √©chou√© pour une raison _x_ ou _y_, vous pouvez lancer la r√©cup√©ration des donn√©es en copiant ce code dans un terminal 

```{.bash}
#!/bin/bash

mkdir -p appli1
cd appli1
echo "data/" >> .gitignore
git init
git branch -m main

mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet data/dvf.parquet
mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet data/carreaux.parquet
mc cp s3/projet-formation/nouvelles-sources/data/triangle.geojson data/triangle.geojson
mc cp s3/projet-formation/nouvelles-sources/data/malakoff.geojson data/malakoff.geojson
mc cp s3/projet-formation/nouvelles-sources/data/montrouge.geojson data/montrouge.geojson
```

```{r}
#| echo: false
#| output: false
dir.create("data")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/triangle.geojson", "data/triangle.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/malakoff.geojson", "data/malakoff.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/montrouge.geojson", "data/montrouge.geojson")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet", "data/dvf.parquet")
download.file("https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet", "data/carreaux.parquet")
```

:::


En premier lieu, ce TD utilise une source administrative nomm√©e __DVF__ (_¬´ Demandes de Valeurs Fonci√®res ¬ª_). 

- Les donn√©es DVF recensent l'ensemble des ventes de biens fonciers r√©alis√©es au cours des derni√®res ann√©es, en m√©tropole et dans les d√©partements et territoires d'outre-mer ‚Äî sauf √† Mayotte et en Alsace-Moselle. Les biens concern√©s peuvent √™tre b√¢tis (appartement et maison) ou non b√¢tis (parcelles et exploitations). Les donn√©es sont produites par la Direction g√©n√©rale des finances publiques (DGFip). Elles proviennent des actes enregistr√©s chez les notaires et des informations contenues dans le cadastre. Cette base a √©t√© filtr√©e de mani√®re √† √™tre la plus p√©dagogique possible pour cette formation.

L'analyse de ces donn√©es sera compl√©t√©e des donn√©es **Filosofi** produites par l'Insee :

- Les donn√©es spatiales carroy√©es √† 200m, produites par l'Insee √† partir du dispositif **Filosofi**, contentant des informations socio-√©conomiques sur les m√©nages.

Enfin, nous proposons trois contours g√©ographiques _ad hoc_ :

- La commune de Malakoff
- La commune de Montrouge
- Le "Triangle d'or" de Malakoff (autrement dit, son centre-ville √† peu de choses pr√®s)

L'objectif de ce TD est d'illustrer la mani√®re dont peuvent √™tre trait√©es des donn√©es spatiales de mani√®re flexible avec `duckdb`.


# Pr√©paration de l'environnement

Les librairies suivantes seront utilis√©es dans ce TD, vous pouvez d'ores et d√©j√† les charger dans votre environnement. 

```{r}
#| output: false

library(duckdb)
library(glue)
library(dplyr)
library(dbplyr)
library(mapview)
```

Si celles-ci ne sont pas install√©es, vous pouvez faire en console un `install.packages` (voir @note-bp-install).

::: {#note-bp-install .callout-note}

Les installations de _packages_ sont √† faire en console mais ne doivent pas √™tre √©crites dans le code. Bien que ce ne soit pas l'objet de ce cours, il est utile de suivre les bonnes pratiques recommand√©es √† l'Insee et plus largement dans le monde {{< fa brands r-project >}}. 

Pour en savoir plus, vous pourrez explorer [le portail de formation aux bonnes pratiques](https://inseefrlab.github.io/formation-bonnes-pratiques-git-R/). 

:::

Nous allons avoir besoin des codes Insee suivants pour notre application :


```{r}
cog_malakoff <- "92046"
cog_montrouge <- "92049"
```


## Import des donn√©es

L'import des contours en {{< fa brands r-project >}} se fait assez naturellement gr√¢ce √† [`sf`](https://r-spatial.github.io/sf/).



```{r}
triangle <- sf::st_read("data/triangle.geojson", quiet=TRUE)
malakoff <- sf::st_read("data/malakoff.geojson", quiet=TRUE)
montrouge <- sf::st_read("data/montrouge.geojson", quiet=TRUE)
```

En premier lieu, on peut visualiser la ville de `Malakoff` :

```{r}
mapview(malakoff) + mapview(triangle, col.regions = "#ffff00")
```

Et ensuite les contours de Montrouge :

```{r}
mapview(montrouge)
```


## Pr√©paration de DuckDB

`DuckDB` est un moteur de base de donn√©es analytique en m√©moire, optimis√© pour les requ√™tes SQL sur des donn√©es volumineuses, particuli√®rement adapt√© aux fichiers plats comme `Parquet` ou CSV, et int√©grable dans des langages comme Python, R ou SQL.

En principe, `duckdb` fonctionne √† la mani√®re d'une base de donn√©es. Autrement dit, on d√©finit une base de donn√©es et effectue des requ√™tes (SQL ou verbes `tidyverse`) dessus. Pour cr√©er une base de donn√©es, il suffit de faire un `read_parquet` avec le chemin du fichier.  

La base de donn√©es se cr√©e tout simplement de la mani√®re suivante :

```{r}
#| output: false
#| echo: true

con <- dbConnect(duckdb::duckdb())
dbExecute(con, "INSTALL spatial;")
dbExecute(con, "LOAD spatial;")
```

Nous verrons ult√©rieurement pourquoi nous avons besoin de cette extension spatiale. 

Cette connexion `duckdb` peut √™tre utilis√©e de plusieurs mani√®res. En premier lieu, par le biais d'une requ√™te SQL. `dbGetQuery` permet d'avoir le r√©sultat sous forme de _dataframe_ puisque la requ√™te est d√©l√©gu√©e √† l'utilitaire `duckdb` qui est embarqu√© dans les fichiers de la librairie :

```{r}
#| label: show-dbquery-principle
#| output: false

out <- dbGetQuery(
  con,
    'SELECT * EXCLUDE (geometry) FROM read_parquet("data/dvf.parquet") LIMIT 5'
)
out
```


La cha√Æne d'ex√©cution ressemble ainsi √† celle-ci :

![](https://raw.githubusercontent.com/linogaliana/parquet-recensement-tutomate/main/img/duckdb-delegation1.png){fig-align="center"}

M√™me si `DuckDB` simplifie l'utilisation du SQL en proposant de nombreux verbes auxquels nous sommes familier en `R` ou `Python`, SQL n'est n√©anmoins pas toujours le langage le plus pratique pour cha√Æner des op√©rations nombreuses. Pour ce type de besoin,  le `tidyverse` offre une grammaire riche et coh√©rente. Il est tout √† fait possible d'interfacer une base `duckdb` au `tidyverse`. Nous pourrons donc utiliser nos verbes pr√©f√©r√©s (`mutate`, `filter`, etc.) sur un objet `duckdb` : une phase pr√©liminaire de traduction en SQL sera automatiquement mise en oeuvre :

![](https://raw.githubusercontent.com/linogaliana/parquet-recensement-tutomate/main/img/duckdb-delegation2.png){fig-align="center"}

```{r}
#| message: false
#| label: create-connections
table_logement <- tbl(con, 'read_parquet("data/dvf.parquet")')
table_logement %>% head(5)
```


# Partie 1 : Prix immobiliers √† Malakoff et √† Montrouge

Dans cette partie, l'objectif est d'extraire de l'informations d'une base de donn√©es volumineuse √† l'aide de DuckDB. Pour le moment, le caract√®re spatial des donn√©es est mis de c√¥t√© : on d√©couvre et on traite les donn√©es via des requ√™tes attributaires classiques.

Tentons, par une premi√®re s√©rie d'exercices, de comparer la m√©diane des prix des transactions immobili√®res √† Malakoff et √† Montrouge.

Dans cette partie, nous allons pouvoir faire nos traitements de donn√©es avec SQL et/ou `tidyverse`. Cela illustre l'une des forces de duckdb, √† savoir son excellente int√©gration avec d'autres √©cosyst√®mes dont nous sommes familiers.

Lorsque nous irons sur l'aspect spatial, nous passerons en SQL pur, l'√©cosyst√®me tidyverse n'√©tant pas encore finalis√© pour le traitement de donn√©es spatiales avec `duckdb`. 


## Premi√®res requ√™tes SQL : description des donn√©es DVF

Tout d'abord, il convient de se familiariser avec les donn√©es. Les requ√™tes propos√©es pour l'exercice 1 permettent d'obtenir des informations primordiales de mani√®re tr√®s rapide et sans n√©cessit√© de charger l'ensemble des donn√©es dans la m√©moire vive.


::: {.exercice}
## Exercice 1

Cet exercice nous fera entrer progressivement dans les donn√©es √† partir de quelques requ√™tes basiques.

1. Lire les 10 premi√®res lignes des donn√©es par l'approche SQL et par l'approche tidyverse.
2. Afficher les noms des colonnes selon les deux approches.
3. Regarder les valeurs uniques de la colonne `nature_mutation` selon les deux approches. 
4. Calculer les bornes min et max des prix des transactions selon ces deux approches.
:::

√Ä la question 1, vous devriez avoir :


```{r}
#| echo: true
preview_q1 <- dbGetQuery(con, "SELECT * FROM read_parquet('data/dvf.parquet') LIMIT 10")
preview_q1_bis <- table_logement %>% head(10)
preview_q1
```

A la question 2, la liste des colonnes donnera plut√¥t


```{r}
#| echo: true
describe_dvf <- dbGetQuery(con, "DESCRIBE SELECT * FROM read_parquet('data/dvf.parquet')")
table_logement %>% colnames(.)
```

Que contient le champ `nature_mutation` ? (il a √©t√© filtr√© au ventes classiques pour simplifi√© cette application ; les vraies donn√©es sont plus riches).


```{r}
#| echo: true
unique_nature_mutation <- dbGetQuery(con, "SELECT DISTINCT nature_mutation FROM read_parquet('data/dvf.parquet')")
unique_nature_mutation_bis <- table_logement %>% distinct(nature_mutation)
unique_nature_mutation_bis
```

A la question 4, vous devriez obtenir des statistiques similaires √† celles-ci :

```{r}
#| echo: true
stats_q4 <- dbGetQuery(con, "
        SELECT
            MIN(valeur_fonciere) AS min_valeur,
            MAX(valeur_fonciere) AS max_valeur
        FROM read_parquet('data/dvf.parquet')
           ")
stats_q4_bis <- table_logement %>%
  summarise(
      min_valeur = min(valeur_fonciere, na.rm = TRUE),
      max_valeur = max(valeur_fonciere, na.rm = TRUE)
    )
stats_q4_bis
```

Nous venons de voir comment faire quelques requ√™tes basiques sur un fichier .parquet avec `duckdb` et l'√©quivalence entre les approches SQL et tidyverse. La derni√®re question √©tait d√©j√† une introduction au calcul √† la vol√©e de statistiques descriptives, ajoutons quelques statistiques avec ce nouvel exercice. 

::: {.exercice}
## Exercice 2 : statistiques descriptives sur la dimension attributaire

Ne garder que les seules transactions effectu√©es √† Montrouge ou Malakoff et faire une m√©diane par commune des montants des transactions

_Faire ceci avec SQL et dplyr_[^IA]

:::

[^IA] : Vous avez le droit d'utiliser votre assistant IA pr√©f√©r√© ! Mais ne prenez pas pour argent comptant ce qu'il vous propose.

Avec l'approche SQL vous devriez obtenir


```{r}
#| echo: true
query1 <- glue("
    SELECT
        code_commune,
        MEDIAN(valeur_fonciere) AS mediane_valeur_fonciere
    FROM read_parquet('data/dvf.parquet')
    WHERE code_commune IN ('{cog_malakoff}', '{cog_montrouge}')
    GROUP BY code_commune
")

result1 <- dbGetQuery(con, query1)
print(result1)

```

On peut se rassurer, on obtient la m√™me chose l'approche `dplyr` :

```{r}
#| echo: true
table_logement %>%
  filter(code_commune %in% c(cog_malakoff, cog_montrouge)) %>% 
  group_by(code_commune) %>%
  summarise(mediane_valeur_fonciere = median(valeur_fonciere, na.rm = TRUE))
```


On peut en conclure que les biens vendus √† Montrouge (dans notre base) ont une m√©diane un peu plus √©lev√©e qu'√† Malakoff.


# Partie 2 : les prix immobiliers √† Malakoff, dans le centre et en-dehors.

√Ä pr√©sent, nous souhaitons avoir des informations sur les transactions effectu√©es dans le ¬´ glorieux ¬ª Triangle d'Or de Malakoff (plus prosa√Øquement, dans son centre-ville commer√ßant).

Il n'est pas possible de distinguer cette zone par requ√™tes attributaires : il n'existe pas de colonne permettant de distinguer cette zone : 


::: {.exercice}
## Exercice 3

Via `DuckDB`, calculer la m√©diane des prix des transactions √† l'int√©rieur et √† l'ext√©rieur du Triangle.

D√©marche : 

- 3.1 : Transformer l'objet sf `triangle` en WKT apr√®s l'avoir reprojet√© en Lambert 2154 (m√™me syst√®me de projection que les transactions DVF, voir @tip-geom).
- 3.2 : Faire un filtre attributaire sur la commune de Malakoff.
- 3.3 : Cr√©er une colonne "in_triangle" bool√©enne et √©gale √† TRUE pour les transactions pr√©sentes dans le triangle (utiliser la colonne "geometry", au format WKB, voir @tip-wkb).
- 3.4 : Calculer les m√©dianes en groupant sur la variable "in_triangle".
:::


::: {#tip-wkb .callout-tip}
## Format des g√©om√©tries (WKT, WKB)

Dans le fichier `dvf.parquet`, les coordonn√©es spatiales sont stock√©es dans un format binaire sp√©cifique (Well-Known Binary - WKB). Ce format est efficace pour le stockage et les calculs, mais n'est pas directement lisible ou interpr√©table par les humains.

En revanche, le triangle est disponible sous forme d'objet `sf` (propre √† notre environnement R) et nous devons le transformer en WKT (Well-Known Text - repr√©sentation texte lisible) avec `sf::st_as_text()` pour pouvoir √™tre consomm√© par DuckDB.
:::

::: {#tip-geom .callout-tip}
## Projection g√©ographique

Pour se mettre dans des conditions proches du r√©el, nous mettons √† disposition le triangle dans un syst√®me de projection diff√©rent des transactions. Cette situation arrive couramment en pratique et elle n√©c√©ssite de faire des reprojections adapt√©es avant toute op√©ration g√©om√©trique entre deux sources de donn√©es spatiales.
:::

3.1 : aide √† la reprojection du Triangle

```{r}
triangle_wkt <- triangle %>% 
  sf::st_transform(2154) %>%  # Reprojection
  sf::st_geometry() %>%  # Extraction de la seule dimension spatiale
  sf::st_as_text()  # Transformation en WKT
triangle_wkt
```


Pour l'exercice 3.2, nous aurons besoin de la structure de requ√™te suivante pour cr√©er la variable "in_triangle" :

```{.r}
ST_Within(
  ..., # nom de la colonne spatiale du fichier des transactions 
  ST_GeomFromText(...)  # Coordonn√©es du triangle en WKT
) as in_triangle
```

Voici les transactions de Malakoff avec la colonne "in_triangle" (masque bool√©en).

Ces transactions sont import√©es en m√©moire vive par souci p√©dagogique mais le calcul des m√©dianes d'int√©r√™t pourrait √™tre directement r√©alis√© dans la m√™me requ√™te.

```{r}
#| echo: true

requete <- glue("
  SELECT
    *,
    ST_Within(geometry, ST_GeomFromText('{triangle_wkt}')) as in_triangle
  FROM read_parquet('data/dvf.parquet')
  WHERE code_commune = '{cog_malakoff}'
")

transactions_malakoff <- dbGetQuery(con, requete)
transactions_malakoff

```

Une fois que chaque transaction est identifi√©e comme √©tant √† l'int√©rieur ou √† l'ext√©rieur du Triangle, le calcul de la m√©diane des prix se fait simplement par group_by (ici avec `dplyr`).

```{r}
#| echo: true

medians_in_out <- transactions_malakoff %>% 
  group_by(in_triangle) %>% 
  summarise(median = median(valeur_fonciere))

median_in <- medians_in_out %>% filter(in_triangle) %>% pull(median)
median_out <- medians_in_out %>% filter(!in_triangle) %>% pull(median)

print(glue("M√©diane des prix dans le Triangle d'Or de Malakoff : ", median_in))
print(glue("M√©diane des prix dans le reste de Malakoff : ", median_out))
```

Qu√™te secondaire : calculer directement les m√©dianes avec une seul requ√™te SQL.

```{r}
#| echo: true
#| 
requete <- glue("
  SELECT
    ST_Within(geometry, ST_GeomFromText('{triangle_wkt}')) as in_triangle,
    MEDIAN(valeur_fonciere) as mediane_valeur_fonciere,
    COUNT(*) as nb_transactions
  FROM read_parquet('data/dvf.parquet')
  WHERE code_commune = '{cog_malakoff}'
  GROUP BY in_triangle
")

medianes_par_zone <- dbGetQuery(con, requete)
medianes_par_zone
```


La m√©diane des prix est un peu plus √©lev√©e dans le Triangle qu'en dehors. On peut aller au-del√† et √©tudier la distribution des transactions. Bien que la taille d'√©chantillon soit r√©duite, on a ainsi une id√©e de la diversit√© des prix dans cette bucolique commune de Malakoff.


```{r}
#| code-fold: true
#| code-summary: "Produire la figure sur la distribution du prix des biens"
library(ggplot2)
library(scales)

ggplot(
  transactions_malakoff,
  aes(y = valeur_fonciere, x = in_triangle, fill = in_triangle)
) +
  geom_violin() +
  scale_y_continuous(
    trans = "log10",
    labels = comma_format(),
    breaks = scales::trans_breaks("log10", function(x) 10^x)
  ) +
  geom_jitter(height = 0, width = 0.1) +
  labs(y = "Valeur de vente (‚Ç¨)") +
  theme_minimal()
```

Tout ceci ne nous dit rien de la diff√©rence entre les biens dans le triangle et en dehors de celui-ci. Nous n'avons fait aucun contr√¥le sur les caract√©ristiques des biens. Nous laissons les curieux explorer la mine d'or qu'est cette base. 


# Partie 3 : Part de m√©nages pauvres √† Malakoff et √† Montrouge

Pour finir, on se place dans le cas o√π : 

- On souhaite extraire des informations d'un fichier volumineux (les donn√©es carroy√©es de l'Insee).
- Mais il n'est pas possible de filtrer les donn√©es par des requ√™tes attributaires (par exemple, il n'est pas possible de faire `code_commune = 92049`).

Ainsi, nous allons : 

- Utiliser les contours g√©ographiques des deux communes
- Filtrer les donn√©es par intersections g√©ographiques des carreaux et des communes, √† l'aide de `DuckDB`
- Faire les calculs localement apr√®s l'extraction des carreaux d'int√©r√™t.

Pour commencer, nous d√©crivons les donn√©es carroy√©es comme pr√©c√©demment :

```{r}
describe_dvf <- dbGetQuery(con, "DESCRIBE SELECT * FROM read_parquet('data/carreaux.parquet')")
describe_dvf
```

```{r}
preview <- dbGetQuery(con, "SELECT * FROM read_parquet('data/carreaux.parquet') LIMIT 10")
preview
```

On va faire une petite transformation de donn√©es pr√©liminaire √† cet exercice afin que la g√©om√©trie du contour de Malakoff soit reconnue par `DuckDB`. 

```{r}
malakoff_2154 <- sf::st_transform(malakoff, 2154)
malakoff_wkt <- sf::st_as_text(sf::st_geometry(malakoff_2154))
```

Voici comment faire une requ√™te g√©ographique sur les carreaux de Malakoff

```{r}
geo_query <- glue("
  FROM read_parquet('data/carreaux.parquet')
  SELECT
      *, ST_AsText(geometry) AS geom_text
  WHERE ST_Intersects(
      geometry,
      ST_GeomFromText('{malakoff_wkt}')
  )
")

carr_malakoff <- dbGetQuery(con, geo_query)

carr_malakoff <-
  carr_malakoff |>
  sf::st_as_sf(wkt = "geom_text", crs = 2154) |>
  select(-geometry) |>
  rename(geometry=geom_text)
```

On peut les visualiser de la mani√®re suivante

```{r}
mapview(carr_malakoff) + mapview(sf::st_boundary(malakoff)) 
```


::: {.exercice}
## Exercice 4 : extraction des carreaux intersectant Malakoff

1. R√©it√®rer l'op√©ration pour Montrouge
2. Calculer la proportion moyenne de m√©nages pauvre dans l'ensemble des carreaux extraits √† partir des deux objets obtenus.

:::

Le masque des carreaux de Montrouge est le suivant :

```{r}
#| echo: true
montrouge_2154 <- sf::st_transform(montrouge, 2154)
montrouge_wkt <- sf::st_as_text(sf::st_geometry(montrouge_2154))

geo_query <- glue("
  FROM read_parquet('data/carreaux.parquet')
  SELECT
      *, ST_AsText(geometry) AS geom_text
  WHERE ST_Intersects(
      geometry,
      ST_GeomFromText('{montrouge_wkt}')
  )
")

carr_montrouge <- dbGetQuery(con, geo_query)

carr_montrouge <-
  carr_montrouge |>
  sf::st_as_sf(wkt = "geom_text", crs = 2154) |>
  select(-geometry) |>
  rename(geometry=geom_text)

mapview(carr_montrouge) + mapview(sf::st_boundary(montrouge)) 
```

On obtient, _in fine_, les statistiques suivantes

```{r}
#| echo: true
mean_menpauvres_malakoff <- round(100 * sum(carr_malakoff$men_pauv) / sum(carr_malakoff$men), 2)
mean_menpauvres_montrouge <- round(100 * sum(carr_montrouge$men_pauv) / sum(carr_montrouge$men), 2)

print(glue("Part de m√©nages pauvres dans les carreaux de Malakoff : ", mean_menpauvres_malakoff))
print(glue("Part de m√©nages pauvres dans les carreaux de Montrouge : ", mean_menpauvres_montrouge))
```


::: {.exercice}
## Exercice optionnel

Calculer la m√™me statistique dans et hors du triangle d'or de Malakoff

:::


# Conclusion

Nous avons donc r√©ussi √† lire des donn√©es avec `DuckDB` et √† faire des statistiques dessus. Pourquoi est-ce pertinent de passer par `DuckDB` ? Car ce package permet de faire ceci de mani√®re tr√®s efficace sur de gros volumes de donn√©es. Il passe tr√®s bien √† l'√©chelle. 

A noter que notre d√©marche est une introduction √† ce sujet bien plus large qu'est l'analyse g√©ographique. Notre approche serait am√©liorable sur plusieurs plans : 

- rationalisation des requ√™tes,
- pertinence statistique des r√©sultats
- r√©plicabilit√© du code


