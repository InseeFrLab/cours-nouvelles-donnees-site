---
title: "Application 1: classification automatique de textes"
echo: false
output-dir: _site
filters:
- custom-callout
format: 
   html:
     df-print: paged
echo: true
---

Cette application illustrera certains apports des outils du 
NLP pour la codification automatique des déclarations d'activité
dans la nomenclature des activités françaises. On pourra coder dans un 
notebook au sein de l'environnement `SSP Cloud` suivant:

<a href="https://datalab.sspcloud.fr/launcher/ide/vscode-python?name=vscode-python&version=2.1.23&s3=region-ec97c721" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSPcloud-Ouvrir sur le sspcloud (VSCode)-informational&amp;color=yellow?logo=Python" alt="Onyxia"></a>

<a href="https://datalab.sspcloud.fr/launcher/ide/jupyter-python?name=jupyter-python&version=2.1.22&s3=region-ec97c721" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/SSPcloud-Ouvrir sur le sspcloud (Jupyter)-informational&amp;color=yellow?logo=Python" alt="Onyxia"></a>


Ce tutoriel n'a pas vocation à introduire aux principaux concepts du NLP (tokenisation, sac de mot, _embedding_, etc.) mais à être une introduction pratique à la thématique de la classification textuelle. Pour découvrir les concepts centraux du NLP, se référer au cours de [`Python` pour la _data science_](https://pythonds.linogaliana.fr/content/NLP/) de l'ENSAE. 


## Exploration du jeu de données

Ce tutoriel se propose d'illustrer la problématique de
la classification automatique par le biais de
l'algorithme d'apprentissage
supervisé `FastText`, développé par Meta,
à partir des données issues des déclarations Sirene. 

L'idée de ce tutoriel est de classer des déclarations d'entreprise dans une nomenclature générique des activités productives. Celle-ci permet de produire de nombreuses statistiques économiques sectorielles sur le tissu productif français. L'Insee ayant vocation à produire des statistiques agrégées sur de nombreuses questions, cette approche constitue l'un des principaux cas d'application du NLP pour l'Institut dans des domaines aussi divers que la classification dans une nomenclature d'activités (NAF), une nomenclature de professions (PCS), de produits (COICOP), de lieux géographiques, etc.

Comment passe-t-on d'une déclaration en langage naturel, qui fait sens pour un entrepreneur, à une représentation plus générique, et forcément plus simpliste, de l'activité d'une entreprise, qui fait sens à une institution statistique et espérons, pour le débat public ? Grâce à des algorithmes de classement _ad hoc_ qui permettent d'extraire une information à partir de libellés textuels. Historiquement l'Insee utilisait des règles déterministes de classement à partir d'un algorithme nommé [Sicore](https://www.insee.fr/fr/information/4497076?sommaire=4497095), une IA symbolique qui classait à partir de règles métiers préconfigurée. 

Avec l'avènement du _machine learning_, la possibilité d'entraîner un algorithme apprenant par induction plutôt que de manière déductive, est apparue être une voie d'investissement intéressante pour l'Insee. L'objet de ce TD est d'illustrer la démarche adoptée avec cette approche à partir d'un modèle un peu plus simple que celui mis en oeuvre à l'Insee mais reprenant les principales caractéristiques de celui-ci. 

Le code pour lire les données est directement fourni:

```{python}
#| echo: true
import matplotlib.pyplot as plt
import pandas as pd
from wordcloud import WordCloud

DATA_PATH = "https://minio.lab.sspcloud.fr/projet-formation/diffusion/mlops/data/firm_activity_data.parquet"
NAF_PATH = "https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/naf2008_liste_n5.xls"
naf = pd.read_excel(NAF_PATH, skiprows = 2)
naf['Code'] = naf['Code'].str.replace(".","")
train = pd.read_parquet(DATA_PATH)
train = train.merge(naf, left_on = "nace", right_on = "Code")
train.head(5)
```


Le premier exercice a vocation à 
illustrer la manière classique de rentrer
dans un corpus de données textuelles. La démarche
n'est pas particulièrement originale mais permet d'illustrer 
les enjeux du nettoyage de texte. 

::: {.exercice}
## Exercice 1

0. Lancer le code ci-dessous pour préparer votre environnement de travail :

```{.python}
import spacy
!python -m spacy download fr_core_news_sm
import nltk
nltk.download('punkt_tab')
nltk.download('stopwords')
```

1. Créer une fonction pour compter le nombre de textes contenant une séquence de caractères
donnée dans le corpus. La tester avec _"data science"_ et _"boulanger"_.

2. Faire une fonction pour afficher le _wordcloud_
de notre corpus dans son ensemble et de certaines
catégories pour comprendre la nature de notre
corpus. 

3. Retirer les stopwords à partir de la liste
des mots disponibles dans `SpaCy`.

<details>
<summary>
Aide
</summary>

```{.python}
from nltk.tokenize import word_tokenize
import spacy

nlp = spacy.load("fr_core_news_sm")
stop_words = #liste de stopwords

# Function to remove stopwords
def remove_stopwords(text):
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]
    return ' '.join(filtered_text)

def remove_single_letters(text):
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if len(word) > 1]
    return ' '.join(filtered_text)

# Apply the function to the 'text' column
train['text_clean'] = (train['text']
    .apply(remove_stopwords)
    .apply(remove_single_letters)
)
```

</details>


4. Refaire quelques uns des nuages de mots et étudier la différence avant nettoyage.


:::

Dans une démarche exploratoire,
le plus simple est de commencer par compter les mots
de manière indépendante (approche sac de mot). 
Par exemple, de manière naturelle, nous avons
beaucoup plus de déclarations liées à la boulangerie que 
liées à la _data science_:

```{python}
train_data=train.copy()

def filter_train_data(train_data, sequence):
    sequence_capitalized = sequence.upper() 
    mask = train_data['text'].str.contains(sequence_capitalized)
    nb_occurrence = mask.astype(int).sum()
    print(
        f"Nombre d'occurrences de la séquence '{sequence}': {nb_occurrence}"
    )
    return train_data.loc[mask]
```

```{python}
#| echo: true
filter_train_data(train, "data science").head(5)
filter_train_data(train, "boulanger").head(5)
```

```{python}
#1. Wordclouds
def graph_wordcloud(train_data, text_var="text", naf=None):
    if naf is not None:
        train_data = train_data.loc[train_data['nace'] == naf]

    txt = train_data[text_var]
    all_text = ' '.join([text for text in txt])
    wordcloud = WordCloud(
        width=800,
        height=500,
        random_state=21,
        max_words=2000,
        background_color="white",
        colormap='Set2'
    ).generate(all_text)
    return wordcloud
```

Les _wordclouds_ peuvent servir à rapidement visualiser
la structure d'un corpus. On voit ici que notre
corpus est très bruité car nous n'avons pas 
nettoyé celui-ci:

```{python}
wordcloud_corpus = graph_wordcloud(train)
plt.figure()
plt.imshow(wordcloud_corpus, interpolation="bilinear")
plt.axis("off")
plt.show()
```

Pour commencer à se faire une idée sur les spécificités
des catégories, on peut représenter le corpus de certaines
d'entre elles ? Arrivez-vous à inférer la catégorie de la NAF 
en question ? Si oui, vous utilisez sans doute des heuristiques
proches de celles que nous allons mettre en oeuvre dans notre
algorithme de classification.

```{python}
wordcloud_corpus = graph_wordcloud(train, naf = "1071C")
plt.figure()
plt.imshow(wordcloud_corpus, interpolation="bilinear")
plt.axis("off")
plt.show()
```

```{python}
wordcloud_corpus = graph_wordcloud(train, naf = "4942Z")
plt.figure()
plt.imshow(wordcloud_corpus, interpolation="bilinear")
plt.axis("off")
plt.show()
```

Néanmoins, à ce stade, les données sont encore très
bruitées. La première étape classique est de retirer
les _stop words_ et éventuellement des 
termes spécifiques à notre corpus. Par exemple,
pour des données de caisse, on retirera les bruits, 
les abréviations, etc. qui peuvent bruiter notre
corpus. 

```{python}
from nltk.tokenize import word_tokenize
import spacy

nlp = spacy.load("fr_core_news_sm")
stop_words = nlp.Defaults.stop_words

#stop_words = set(stopwords.words('french'))
stop_words = set(stop_words)

# Function to remove stopwords
def remove_stopwords(text):
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]
    return ' '.join(filtered_text)

def remove_single_letters(text):
    word_tokens = word_tokenize(text)
    filtered_text = [word for word in word_tokens if len(word) > 1]
    return ' '.join(filtered_text)

# Apply the function to the 'text' column
train['text_clean'] = (train['text']
    .apply(remove_stopwords)
    .apply(remove_single_letters)
)
```


## Premier algorithme d'apprentissage supervisé

Nous avons nettoyé nos données. Cela devrait améliorer
la pertinence de nos modèles en réduisant le ratio signal/bruit. 
Nous allons généraliser notre nettoyage de texte
en appliquant un peu plus d'étapes que précédemment. Nous allons
notamment raciniser nos mots. 

Pour cela, récupérer les fichiers suivants:

- [`constants.py`](https://github.com/InseeFrLab/cours-nouvelles-donnees-site/blob/main/applications/constants.py)
- [`processor.py`](https://github.com/InseeFrLab/cours-nouvelles-donnees-site/blob/main/applications/processor.py)
- [`utils.py`](https://github.com/InseeFrLab/cours-nouvelles-donnees-site/blob/main/applications/utils.py)

et mettre ceux-ci dans le même dossier que votre _notebook_ `Jupyter`.

Le code de nettoyage est directement fourni:

```{python}
#| echo: true
from processor import Preprocessor
preprocessor = Preprocessor()

# Preprocess data before training and testing
TEXT_FEATURE = "text"
Y = "nace"

df = preprocessor.clean_text(train, TEXT_FEATURE).drop('text_clean', axis = "columns")
df.head(2)
```

Nous allons commencer à entraîner un modèle dont le plongement de mot est de 
faible dimension. 
Voici les paramètres qui seront utiles pour le prochain exercice.

```{python}
#| echo: true
#| label: create-training-text-example
import pathlib

params = {
    "dim": 25,
    "label_prefix": "__label__"
}

data_path = pathlib.Path("./data")
data_path.mkdir(parents=True, exist_ok=True)

def write_training_data(df, params, training_data_path=None):
    warnings.filterwarnings("ignore", "Setuptools is replacing distutils.")
    if training_data_path is None:
        training_data_path = get_root_path() / "data/training_data.txt"

    with open(training_data_path, "w", encoding="utf-8") as file:
        for _, item in df.iterrows():
            formatted_item = f"{params['label_prefix']}{item[Y]} {item[TEXT_FEATURE]}"
            file.write(f"{formatted_item}\n")
    return training_data_path.as_posix()
```

::: {.exercice}

1. Découper notre échantillon complet en _train_ et _test_.
2. `FastText` effectue son entraînement à partir d'objets stockés
dans un `.txt`. Utiliser la fonction `write_training_data` 
de la manière suivante pour l'écrire.

```{.python}
# Write training data in a .txt file (fasttext-specific)
training_data_path = write_training_data(df_train, params, pathlib.Path(str(data_path.absolute()) + "/training_data.txt"))
```

3. Avec l'aide de la documentation de la librairie `FastText`, entraîner
votre modèle de classification.

4. Sauvegarder le modèle sous forme de binaire, cela pourra éventuellement
servir ultérieurement. 

5. Renvoyer les trois catégories les plus probables pour les nouveaux
libellés suivants:

```{.python}
list_libs = ["vendeur d'huitres", "boulanger"]
```

6. Sur l'ensemble du jeu de test, renvoyer la meilleure prédiction
pour chaque descriptif d'activités. Evaluer la performance globale
et la performance classe par classe, par exemple en calculant le rappel (pour les classes de plus
de 200 cas). 

:::

```{python}
#| label: train-test-split
import warnings
from sklearn.model_selection import train_test_split

df = df.dropna(subset = [Y, TEXT_FEATURE])

X_train, X_test, y_train, y_test = train_test_split(
            df[TEXT_FEATURE],
            df[Y],
            test_size=0.2,
            random_state=0,
            shuffle=True,
        )

df_train = pd.concat([X_train, y_train], axis=1)
df_test = pd.concat([X_test, y_test], axis=1)

def write_training_data(df, params, training_data_path=None):
    warnings.filterwarnings("ignore", "Setuptools is replacing distutils.")
    if training_data_path is None:
        training_data_path = get_root_path() / "data/training_data.txt"

    with open(training_data_path, "w", encoding="utf-8") as file:
        for _, item in df.iterrows():
            formatted_item = f"{params['label_prefix']}{item[Y]} {item[TEXT_FEATURE]}"
            file.write(f"{formatted_item}\n")
    return training_data_path.as_posix()
```


```{python}
#| label: write-as-txt
# Write training data in a .txt file (fasttext-specific)
training_data_path = write_training_data(df_train, params, pathlib.Path(str(data_path.absolute()) + "/training_data.txt"))
```


```{python}
#| eval: false
#| label: fasttext-training
import fasttext

# Train the fasttext model
model = fasttext.train_supervised(
            training_data_path,
            **params,
            lr=0.5,
            verbose=2
        )
```

```{python}
#| eval: false
#| label: save-model
model.save_model("model.bin")
```


```{python}
#| eval: false
#| label: fasttext-predict
list_libs = ["vendeur d'huitres", "boulanger"]

results = model.predict(list_libs, k = 3)
labels, probabilities = results

flattened_labels = [label for sublist in labels for label in sublist]
flattened_probabilities = [prob for sublist in probabilities for prob in sublist]

# Create a DataFrame
df = pd.DataFrame({
    'Label': flattened_labels,
    'IC': flattened_probabilities
})

df['Label'] = df['Label'].str.replace(r'__label__', '')
```

```{python}
#| eval: false
# label: merge-to-check
df.merge(naf, left_on = "Label", right_on="Code")
```


```{python}
#| eval: false
# label: question6
output_column = "prediction"
predictions = pd.DataFrame(
    {
    output_column: \
        [k[0] for k in model.predict(
            [str(libel) for libel in df_test["text"]], k = 1
            )[0]]
    })

df_test[output_column] = predictions['prediction'].astype(str).str.replace(r'__label__', '')
df_test = df_test.reset_index(drop = True)

df_test['prediction'] = predictions['prediction'].astype(str).str.replace(r'__label__', '')
```

```{python}
#| eval: false
from sklearn.metrics import accuracy_score
accuracy_score(df_test['nace'], df_test['prediction'])

df_test['match'] = (df_test['nace'] == df_test['prediction'])

# Aggregating by 'nace' to count correct predictions and calculate accuracy
aggregation = df_test.groupby('nace').agg(
    correct_predictions=('match', 'sum'),
    total=('nace', 'count'),
    recall=('match', 'mean')
)

aggregation.loc[aggregation['total']>200].sort_values('recall', ascending=True)
```

```{python}
#| eval: false
from plotnine import *

ggplot(
    aggregation.loc[aggregation['total']>200].sort_values('recall', ascending=True),
    aes(x='recall')
) + geom_histogram(bins=20)
```

## Pour aller plus loin, introduction au MLOps

On utilise dans cette application un modèle de Machine 
Learning (ML) pour prédire l'activité des entreprises à 
partir de texte descriptifs. Les méthodes de 
ML sont quasiment indispensables pour traiter du texte, 
mais utiliser des modèles de ML pour servir des cas d'usage 
réels demande de respecter un certain nombre de bonnes pratiques 
pour que tout se passe convenablement, en particulier:

- Tracking propre des expérimentations
- Versioning des modèles, en même temps que des données et du code correspondants
- Mise à disposition efficace du modèle aux utilisateurs
- Monitoring de l'activité du modèle servi
- Réentraînement du modèle

Une introduction à ces bonnes pratiques, auxquelles on fait 
régulièrement référence à travers le terme MLOps, est donné dans 
[cette formation](https://inseefrlab.github.io/formation-mlops/slides/fr/index.html#/title-slide) 
([dépôt associé](https://github.com/InseeFrLab/formation-mlops)).
