{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Couverture du terrain à partir d'images satellites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut tout d'abord installer les librairies qui ne sont pas installées par défaut sur le service. Cela é été fait automatiquement dans le script d'initialisation:\n",
    "- tifffile\n",
    "- tqdm\n",
    "- matplotlib \n",
    "- torch\n",
    "- torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avant d'importer les modules dans Jupyter, il faut **choisir la bonne image** où sont pré-installés les modules. \n",
    "Dans \"Kernels\", en bas à gauche, choisissez donc \"application-image\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois cela fait, vous pouvez importer les librairies et modules utiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext autoreload\n",
    "#%autoreload 2\n",
    "\n",
    "import os\n",
    "import s3fs\n",
    "import shutil\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "from tifffile import TiffFile\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivant indique si un ou plusieurs GPUs (Graphics Processing Units) sont disponibles et peuvent être utilisés pour faire les calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device= torch.device(\"cuda:0\" )\n",
    "else: device = \"cpu\"\n",
    "\n",
    "print(\"Using {} device\".format(device))\n",
    "if torch.cuda.is_available():\n",
    "    print(\"nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on veut réaliser une segmentation d'images satellites. Ce problème est plus complexe que celui de la classificaton d'image dans la mesure où l'on souhaite attribuer à chaque pixel d'une image satellite donnée une classe, en l'occurrence un type de sol (eau, bâtiment, surface cultivée, etc..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n",
    "if not os.path.exists('satellite'):\n",
    "    os.mkdir('satellite')\n",
    "try:\n",
    "    fs.get('projet-funathon/2022/diffusion/Sujet9_deep_learning_donnees_satellites/additional_files_earthcube_emu4zqr.zip', 'additional_files_earthcube_emu4zqr.zip')\n",
    "except:\n",
    "    !curl 'https://minio.lab.sspcloud.fr/projet-funathon/2022/diffusion/Sujet9_deep_learning_donnees_satellites/additional_files_earthcube_emu4zqr.zip' --output 'additional_files_earthcube_emu4zqr.zip'  --retry 3 --retry-all-errors --max-time 5\n",
    "shutil.unpack_archive('additional_files_earthcube_emu4zqr.zip',\n",
    "                      extract_dir='satellite')\n",
    "os.remove('additional_files_earthcube_emu4zqr.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans l'exemple précédent on récupère l'ensemble des chemins de fichiers correspondants aux images satellites utilisées ainsi que l'ensemble des annotations associées.\n",
    "\n",
    "Notez qu'ici chaque annotation n'est plus une unique valeur numérique (0, 1) mais un tableau de valeurs de même dimension que l'image. Pour chaque pixel, la valeur correspond au type d'occupation du sol (de 1 à 9). On appelle ces annotations particulières des **masques de segmentation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les images utlisées ici ont en plus des 3 canaux habituels (R, G, B), un quatrième canal correspondant à la mesure d'une bande infra-rouge. En pratique beaucoup d'autres bandes spectrales sont disponibles pour les images satellites. Cet exercice se restreint seulement à 4 bandes (R, G, B, infra-rouge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER_STR = 'satellite/dataset'\n",
    "DATA_FOLDER = Path(DATA_FOLDER_STR).expanduser()\n",
    "DATASET_FOLDER = DATA_FOLDER\n",
    "\n",
    "# Get all train images and masks\n",
    "train_images_paths = sorted(list(DATASET_FOLDER.glob('train/images/*.tif')))\n",
    "train_masks_paths = sorted(list(DATASET_FOLDER.glob('train/masks/*.tif')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée ici la classe `CustomDataset` qui nous permettra d'accéder aux images satellites et aux masques, comme dans l'exemple précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        with TiffFile(self.mask_paths[idx]) as tif :\n",
    "            mask = tif.asarray()\n",
    "        with TiffFile(self.image_paths[idx]) as tif :\n",
    "            image = np.array(tif.asarray())\n",
    "\n",
    "        t_mask = torch.tensor(mask, dtype=torch.long)\n",
    "        image = torch.tensor(np.array(image, dtype=float), dtype=torch.float)\n",
    "        identifier = str(self.mask_paths[idx])\n",
    "        return {\"image\": image, \"masque\" : t_mask, \"id\" : identifier}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.mask_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La classe `LandCoverData` contient des informations en dur sur notre jeu d'image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LandCoverData():\n",
    "    IMG_SIZE = 256\n",
    "    N_CHANNELS = 4\n",
    "    N_CLASSES = 10\n",
    "    MEAN_CHANNEL = [339.42029674, 570.98497474,  539.11161384, 2634.49868179]\n",
    "    STD_CHANNEL = [339.79895785, 404.86935149,  549.41877854, 1071.38939764]\n",
    "    COUNT_CLASS =  np.array([0, 20643, 60971025, 404760981, 277012377, 96473046, 333407133, 9775295, 1071, 29404605])\n",
    "    WEIGHT_CLASS = np.array(\n",
    "        [0.0000e+00, 0.0000e+00, 1.6401e-08, 2.4706e-09, 3.6099e-09, 1.0366e-08,\n",
    "         2.9993e-09, 1.0230e-07, 9.3371e-04, 3.4008e-08]\n",
    "    ) * np.sum(COUNT_CLASS)\n",
    "    CLASSES = [\n",
    "    'no_data',\n",
    "    'clouds',\n",
    "    'artificial',\n",
    "    'cultivated',\n",
    "    'broadleaf',\n",
    "    'coniferous',\n",
    "    'herbaceous',\n",
    "    'natural',\n",
    "    'snow',\n",
    "    'water']\n",
    "\n",
    "    TRAINSET_SIZE = 18491\n",
    "    TESTSET_SIZE = 5043\n",
    "\n",
    "    CLASSES_COLORPALETTE = {\n",
    "    0: [0,0,0],\n",
    "    1: [255,25,236],\n",
    "    2: [215,25,28],\n",
    "    3: [211,154,92],\n",
    "    4: [33,115,55],\n",
    "    5: [21,75,35],\n",
    "    6: [118,209,93],\n",
    "    7: [130,130,130],\n",
    "    8: [255,255,255],\n",
    "    9: [43,61,255]\n",
    "    }\n",
    "    CLASSES_COLORPALETTE = {c: np.asarray(color) for (c, color) in CLASSES_COLORPALETTE.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test de la classe `CustomDataset`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_images_paths, train_masks_paths)\n",
    "\n",
    "# Construction d'un itérateur\n",
    "iterateur = iter(dataset)\n",
    "\n",
    "# Récupération du premier jeu (image,masque) du dataset\n",
    "element_dataset = next(iterateur)\n",
    "image = element_dataset[\"image\"]\n",
    "masque = element_dataset[\"masque\"]\n",
    "\n",
    "print(image.shape)\n",
    "print(masque.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le masque a bien les mêmes dimensions que l'image associée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage d'une image et d'un masque"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les fonctions suivantes permettent d'afficher une image et un masque respectivement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(image, display_min=50, display_max=400, ax=None):\n",
    "    \"\"\"\n",
    "    Show an image.\n",
    "\n",
    "    Args:\n",
    "        image (numpay.array[uint16]): the image. If the image is 16-bit,\n",
    "        apply bytescaling to convert to 8-bit.\n",
    "    \"\"\"\n",
    "    if image.dtype == np.uint16:\n",
    "        iscale = display_max - display_min\n",
    "        scale = 255 / iscale\n",
    "        byte_im = (image) * scale\n",
    "        byte_im = (byte_im.clip(0, 255) + 0.5).astype(np.uint8)\n",
    "        image = byte_im\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_mask(mask, classes_colorpalette, classes=None, add_legend=True, ax=None):\n",
    "    \"\"\"\n",
    "    Show a a semantic segmentation mask.\n",
    "\n",
    "    Args:\n",
    "       mask (numpy.array[uint8]): the mask in 8-bit\n",
    "       classes_colorpalette (dict[int, tuple]): dict mapping class index to an RGB color in [0, 1]\n",
    "       classes (list[str], optional): list of class labels\n",
    "       add_legend\n",
    "    \"\"\"\n",
    "    show_mask = np.empty((*mask.shape, 3))\n",
    "    for c, color in classes_colorpalette.items():\n",
    "        show_mask[mask == c, :] = color\n",
    "    show_mask = show_mask.astype(np.uint8)\n",
    "\n",
    "    plt.imshow(show_mask)\n",
    "    handles = []\n",
    "    for c, color in LandCoverData.CLASSES_COLORPALETTE.items():\n",
    "        handles.append(mpatches.Patch(color=color/255, label=LandCoverData.CLASSES[c]))\n",
    "    plt.legend(handles=handles)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image :\")\n",
    "show_image(np.array(image).astype(np.uint16),display_min = 0, display_max = 2200)\n",
    "\n",
    "print(\"Masque :\")\n",
    "show_mask(np.array(masque),LandCoverData.CLASSES_COLORPALETTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le dictionnaire suivant contient tous les paramètres nécessaires pour l'entraînement du futur modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'monitoring': True,\n",
    "    'freq monitoring': 50,\n",
    "    'n_epoch': 80,\n",
    "    'train_size': 15000,\n",
    "    'batch_size':  28,\n",
    "    'optimizer': \"SGD\",\n",
    "    'lr': 0.003,\n",
    "    'momentum': 0.9,\n",
    "    'model type': \"segmentation mask\",\n",
    "    'init_features': 16,\n",
    "    'validation_n_batch': 2000,\n",
    "    'descriptif': \"Entrainement avec un Unet pour segmentation + cross entropy\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée ici le `DataLoader` qui va nous permettre de charger les images et leur labels par batchs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  config['batch_size']\n",
    "all_dataset = CustomDataset(train_images_paths,train_masks_paths)\n",
    "all_loader = DataLoader(all_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "train_size = config['train_size']\n",
    "val_size = len(all_dataset.mask_paths) - train_size\n",
    "\n",
    "train_dataset, valid_dataset = random_split(all_dataset, [train_size,val_size], generator=torch.Generator().manual_seed(42))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition d'un modèle U-net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dans l'exemple sur la classification, on va utiliser un réseau de neurones convolutifs pour prédires des masques de segmentation. On définit un modèle convolutif bien plus complexe que le précédent. Ce type de modèles est très utilisé dans la littérature pour répondre au problème de segmentation sémantique.\n",
    "\n",
    "Ce U-net est composé : \n",
    "- d'une phase descendante qui extrait des caractéristiques de l'image \n",
    "- d'une phase descendante qui à partir des caractéristiques extraites réalisera la classification de l'image en entrée pixel par pixel\n",
    "\n",
    "Pour plus d'information sur le U-net, voir [ce lien](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj0746Sva34AhWSBhoKHVtvDrIQFnoECAoQAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1505.04597&usg=AOvVaw2VMp5nhjei6r-QgaG3XTpUhttps://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwj0746Sva34AhWSBhoKHVtvDrIQFnoECAoQAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1505.04597&usg=AOvVaw2VMp5nhjei6r-QgaG3XTpU).\n",
    "\n",
    "La construction est détaillée ci-dessous. Le paramètre `init_features` permettra de jouer sur la taille de réseau que l'on souhaite obtenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, init_features,in_channels=4, out_channels=10):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        features = init_features\n",
    "        self.encoder1 = UNet._block(in_channels, features, name=\"enc1\")\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.encoder2 = UNet._block(features, features * 4, name=\"enc2\")\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.bottleneck = UNet._block(features * 4, features * 8, name=\"bottleneck\")\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(\n",
    "            features * 8, features * 4, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder2 = UNet._block((features * 4) * 2, features * 4, name=\"dec2\")\n",
    "        self.upconv1 = nn.ConvTranspose2d(\n",
    "            features * 4, features, kernel_size=2, stride=2\n",
    "        )\n",
    "        self.decoder1 = UNet._block(features * 2, features, name=\"dec1\")\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=features, out_channels=out_channels, kernel_size=1\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        enc1 = self.encoder1(x)\n",
    "        enc2 = self.encoder2(self.pool1(enc1))\n",
    "\n",
    "        bottleneck = self.bottleneck(self.pool2(enc2))\n",
    "\n",
    "        dec2 = self.upconv2(bottleneck)\n",
    "        dec2 = torch.cat((dec2, enc2), dim=1)\n",
    "        dec2 = self.decoder2(dec2)\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat((dec1, enc1), dim=1)\n",
    "        dec1 = self.decoder1(dec1)\n",
    "\n",
    "        return torch.sigmoid(self.conv(dec1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _block(in_channels, features, name):\n",
    "        return nn.Sequential(\n",
    "            OrderedDict(\n",
    "                [\n",
    "                    (name + \"conv1\", nn.Conv2d(in_channels=in_channels, out_channels=features, kernel_size=3, padding=1, bias=False,),),\n",
    "                    (name + \"Batchnorm1\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu1\", nn.ReLU(inplace=True)),\n",
    "                    (name + \"conv2\",nn.Conv2d(in_channels=features, out_channels=features, kernel_size=3, padding=1, bias=False,),),\n",
    "                    (name + \"Batchnorm2\", nn.BatchNorm2d(num_features=features)),\n",
    "                    (name + \"relu2\", nn.ReLU(inplace=True)),\n",
    "                ]\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour information, la cellule ci-dessous présente une méthode de construction du U-net\n",
    "où on récupère ici une structure de modèle sur https://github.com/mateuszbuda/brain-segmentation-pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetDl(nn.Module):\n",
    "\n",
    "    def __init__(self,init_features):\n",
    "        super().__init__()\n",
    "        self.unet = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet',\n",
    "        in_channels= 4, out_channels=10, init_features= init_features, pretrained=False, verbose = False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.unet(x)\n",
    "        return(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comptons le nombre de paramètres du modèle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(config['init_features'])\n",
    "get_n_params(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque :** Près d'un demi-million de paramètres pour ce réseau !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entraînement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De la même façon que précedemment, on entraîne notre modèle. La logique reste la même : on actualise les paramètres du modèle, batch après batch en calculant une erreur commise par le modèle sur le batch et son gradient.\n",
    "\n",
    "L'erreur commise pour une image est la moyenne des erreurs commises sur les labels de chacun de ses pixels (entropie croisée)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = UNet(config['init_features'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque :** Même avec un GPU l'entraînement d'un modèle aussi gros peut  prendre une quinzaine d'heures. C'est pourquoi nous vous invitons à lancer la cellule ci-dessous pour constater que l'entraînement se lance bien. Puis vous pouvez arrêter l'entraînement et passer à la partie suivante où on charge des modèles qui ont été pré-entrainés par nos soins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "net = net.to(device)\n",
    "entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(config['n_epoch']):\n",
    "        net = net.to(device)\n",
    "        running_loss = 0.0\n",
    "        t = tqdm(train_loader, desc=\"epoch %i\" % (epoch+1),position = 0, leave=True)\n",
    "        epoch_loop = enumerate(t)\n",
    "\n",
    "        for i, data in epoch_loop:\n",
    "            taille_batch = data['image'].shape[0]\n",
    "            images = data['image'].permute(0, 3, 1, 2)\n",
    "            masques  =  data['masque']\n",
    "            images, masques = images.to(device), masques.long().to(device)\n",
    "            y_hat = net(images)\n",
    "            optimizer.zero_grad()\n",
    "            loss = entropy(y_hat,masques)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            del images, masques, y_hat # libéreer un peu d'espace\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if (i+1) % config['freq monitoring'] == 0:\n",
    "                t.set_description(\"epoch %i, 'mean loss: %.6f'\" % (epoch+1, running_loss/config['freq monitoring']))\n",
    "                t.refresh()\n",
    "                running_loss =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupération de modèles pré-entrainés et sauvegarde de modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère les modèles pré-entrainés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('weights'):\n",
    "    os.mkdir('weights')\n",
    "\n",
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n",
    "fs.get('projet-funathon/diffusion/Sujet9_deep_learning_donnees_satellites/model_Unet_initfeatures_6.pth', 'weights/Unet_pre_entraine_peu_profond.pth')\n",
    "fs.get('projet-funathon/diffusion/Sujet9_deep_learning_donnees_satellites/model_config_init_16.pth', 'weights/Unet_pre_entraine_profond.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces modèles ont été entrainés sur une centaine d'epochs (10h d'entraînement pour le plus profond).\n",
    "Lorsqu'on veut récupérer le modèle entraîné, il faut d'abord initialiser un `UNet` avec les mêmes paramètres que ceux qui ont été utilisés pour le modèle pré entrainés Puis on peut charger les paramètres.\n",
    "Le `UNet` peu profond a été entrainé avec `init_features` égal à 6, le profond avec `init features` égal à 16. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_peu_profond = Unet(6)  # Création d'un modèle avec 6 init features en entrée non entrainé\n",
    "unet_profond = Unet(16)  # Création d'un modèle avec 6 init features en entrée non entrainé\n",
    "\n",
    "# Avec GPU :\n",
    "# unet_profond.load_state_dict(torch.load('weights/Unet_pre_entraine_profond.pth'))  # Remplacement des paramètres par les paramètres issus du modèle pré entraîné\n",
    "# unet_peu_profond.load_state_dict(torch.load('weights/Unet_pre_entraine_peu_profond.pth'))  # Remplacement des paramètres par les paramètres issus du modèle pré entraîné\n",
    "\n",
    "# Sans GPU :\n",
    "unet_peu_profond.load_state_dict(torch.load('weights/Unet_pre_entraine_peu_profond.pth', map_location=torch.device('cpu')))\n",
    "unet_profond.load_state_dict(torch.load('weights/Unet_pre_entraine_profond.pth', map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est intéressant de comparer le nombre de paramètres contenus dans chaque modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_params(unet_peu_profond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_n_params(unet_profond)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le U-Net profond a beaucoup plus de paramètres, il sera plus long a entraîner. Cependant, à nombre d'epochs égal ce dernier sera plus précis dans la tâche de segmentation d'images satellites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour information, pour **sauvegarder** les paramètres d'un modèle les instructions suivantes suffisent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'model_test_enregistrement.pth')  # Devrait apparaître à gauche dans l'explorateur de fichiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation qualitative des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on prend des images de l'échantillon de validation, images n'ayant pas servi à l'entraînement. On compare les masques obtenus avec les deux modèles entraînés (U-Net profond et peu profond) aux vrais masques. Relancez plusieurs fois pour constater que les masques produits par le modèle profond sont plus fidèles aux vrais masques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement\n",
    "tmp = iter(valid_loader)\n",
    "dico = next(tmp)\n",
    "images = dico['image']\n",
    "masques_true = dico['masque']\n",
    "\n",
    "net = unet_peu_profond.to(device)\n",
    "masque_predit = net(images.permute(0,3,1,2).to(device))\n",
    "masque_predit = masque_predit[0]\n",
    "masque_unet_peu_profond = np.array(torch.argmax(masque_predit, dim=0).cpu()).astype(np.uint8)\n",
    "\n",
    "net = unet_profond.to(device)\n",
    "masque_predit = net(images.permute(0,3,1,2).to(device))\n",
    "masque_predit = masque_predit[0]\n",
    "masque_unet_profond = np.array(torch.argmax(masque_predit, dim=0).cpu()).astype(np.uint8)\n",
    "\n",
    "masque_true = masques_true[0]\n",
    "image_select = images[0]\n",
    "\n",
    "img = np.array(image_select).astype(np.uint16)\n",
    "\n",
    "print(\"Image initiale :\")\n",
    "show_image(img, display_min = 0, display_max = 2200)\n",
    "\n",
    "print(\"Masque prédit par le U-Net peu profond :\")\n",
    "show_mask(masque_unet_peu_profond, LandCoverData.CLASSES_COLORPALETTE)\n",
    "\n",
    "print(\"Masque prédit par le U-Net profond :\")\n",
    "show_mask(masque_unet_profond, LandCoverData.CLASSES_COLORPALETTE)\n",
    "\n",
    "print(\"Vrai masque :\")\n",
    "show_mask(masque_true, LandCoverData.CLASSES_COLORPALETTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
