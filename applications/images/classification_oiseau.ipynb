{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction à la classification d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation de librairies qui ne sont pas installées par défaut sur le service (déjà fait):\n",
    "- `tqdm` : librairie pour afficher le progrès lorsqu'une boucle se trouve dans le code\n",
    "- `torch` : une des librairies de référence pour le Deep Learning\n",
    "- `torchvision` : librairie utilitaire mettant à disposition des données, des architectures de modèles, des fonctions de transformations d'images, etc.\n",
    "- `pandas` : librairie de manipulations de données tabulaires\n",
    "- `imageio` : librairie pour travailler avec des images\n",
    "- `scikit-learn` : machine learning.\n",
    "\n",
    "Avant d'importer les modules, il faut choisir la bonne image où sotn pré-installés les modules. Dans \"Kernels\", en bas à gauche, choisissez donc \"application-image\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import des modules nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import s3fs\n",
    "import shutil\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import random\n",
    "import imageio.v3 as iio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La cellule suivant indique si un ou plusieurs GPUs (Graphics Processing Units) sont disponibles et peuvent être utilisés pour faire les calculs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() : device = torch.device(\"cuda:0\" )\n",
    "else : device  = \"cpu\"\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "if device != \"cpu\" :\n",
    "    print(\"Nom du GPU :\", torch.cuda.get_device_name(device=None))\n",
    "    print(\"GPU initialisé : \", torch.cuda.is_initialized())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chargement des données "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère des images et fichiers utiles sur un espace de stockage distant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1530M  100 1530M    0     0   121M      0  0:00:12  0:00:12 --:--:--  118M\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 28] No space left on device",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m      7\u001b[39m     get_ipython().system(\u001b[33m\"\u001b[39m\u001b[33mcurl \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://minio.lab.sspcloud.fr/projet-funathon/2022/diffusion/Sujet9_deep_learning_donnees_satellites/archive.zip\u001b[39m\u001b[33m'\u001b[39m\u001b[33m --output \u001b[39m\u001b[33m'\u001b[39m\u001b[33moiseaux/oiseau.zip\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mshutil\u001b[49m\u001b[43m.\u001b[49m\u001b[43munpack_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moiseaux/oiseau.zip\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moiseaux\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m os.remove(\u001b[33m'\u001b[39m\u001b[33moiseaux/oiseau.zip\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/shutil.py:1360\u001b[39m, in \u001b[36munpack_archive\u001b[39m\u001b[34m(filename, extract_dir, format, filter)\u001b[39m\n\u001b[32m   1358\u001b[39m func = _UNPACK_FORMATS[\u001b[38;5;28mformat\u001b[39m][\u001b[32m1\u001b[39m]\n\u001b[32m   1359\u001b[39m kwargs = \u001b[38;5;28mdict\u001b[39m(_UNPACK_FORMATS[\u001b[38;5;28mformat\u001b[39m][\u001b[32m2\u001b[39m]) | filter_kwargs\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/shutil.py:1267\u001b[39m, in \u001b[36m_unpack_zipfile\u001b[39m\u001b[34m(filename, extract_dir)\u001b[39m\n\u001b[32m   1263\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m name.endswith(\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m   1264\u001b[39m             \u001b[38;5;66;03m# file\u001b[39;00m\n\u001b[32m   1265\u001b[39m             \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mzip\u001b[39m.open(name, \u001b[33m'\u001b[39m\u001b[33mr\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m source, \\\n\u001b[32m   1266\u001b[39m                     \u001b[38;5;28mopen\u001b[39m(targetpath, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m target:\n\u001b[32m-> \u001b[39m\u001b[32m1267\u001b[39m                 \u001b[43mcopyfileobj\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1268\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   1269\u001b[39m     \u001b[38;5;28mzip\u001b[39m.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/shutil.py:204\u001b[39m, in \u001b[36mcopyfileobj\u001b[39m\u001b[34m(fsrc, fdst, length)\u001b[39m\n\u001b[32m    202\u001b[39m fdst_write = fdst.write\n\u001b[32m    203\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m buf := fsrc_read(length):\n\u001b[32m--> \u001b[39m\u001b[32m204\u001b[39m     \u001b[43mfdst_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 28] No space left on device"
     ]
    }
   ],
   "source": [
    "fs = s3fs.S3FileSystem(client_kwargs={'endpoint_url': 'https://minio.lab.sspcloud.fr'})\n",
    "if not os.path.exists('oiseaux'):\n",
    "    os.mkdir('oiseaux')\n",
    "try:\n",
    "    fs.get('projet-funathon/2022/diffusion/Sujet9_deep_learning_donnees_satellites/archive.zip', 'oiseaux/oiseau.zip')\n",
    "except:\n",
    "    !curl 'https://minio.lab.sspcloud.fr/projet-funathon/2022/diffusion/Sujet9_deep_learning_donnees_satellites/archive.zip' --output 'oiseaux/oiseau.zip'\n",
    "shutil.unpack_archive('oiseaux/oiseau.zip', extract_dir='oiseaux')\n",
    "os.remove('oiseaux/oiseau.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On souhaite faire de la classification d'images. Dans un premier temps, on se limite à un problème à 2 classes. On veut disposer d'un algorithme permettant de classer correctement des images de 2 espèces d'oiseaux (dans un premier temps).\n",
    "\n",
    "Dans la cellule suivante, on importe une table `birds.csv` qui contient des chemins vers des images d'oiseaux ainsi que les espèces correspondantes associées. On crée un array `train_images_paths` qui contient l'ensemble des chemins permettant d'accéder aux images des classes `0` et `1` (respectivement *abbotts babbler* et *abbotts booby*). L'array `train_images_labels` contient les annotations des images.\n",
    "\n",
    "Le dictionnaire `dic_id_to_label` contient la correspondance entre les noms des espèces les identifiants associés. Ici, 366 images d'entraînement sont à notre disposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'ABBOTTS BABBLER', 1: 'ABBOTTS BOOBY'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bird_df = pd.read_csv('oiseaux/birds.csv')\n",
    "\n",
    "NB_CLASSES = 2\n",
    "# On se limite à 2 classes pour le moment\n",
    "bird_df = bird_df[bird_df['class index'] < NB_CLASSES]\n",
    "\n",
    "train_images_paths = np.array(['oiseaux/' + path for path in bird_df['filepaths']])\n",
    "train_images_labels = np.array(bird_df['class index'])\n",
    "\n",
    "# Création d'un dictionnaire des annotations\n",
    "nom_oiseau = np.array(bird_df.labels.unique())\n",
    "dic_id_to_label = {i : oiseau for i, oiseau in zip(range(NB_CLASSES), nom_oiseau)}\n",
    "dic_id_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(366,)\n",
      "(366,)\n"
     ]
    }
   ],
   "source": [
    "print(train_images_labels.shape)\n",
    "print(train_images_paths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'une classe `Dataset`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée ici une classe `CustomDataset` qui hérite de la classe `Dataset`, une classe utilitaire de librairie `Pytorch` qu'on initialise avec nos données et qui permet ensuite d'accéder facilement aux observations. Un `CustomDataset` sera initialisé avec l'ensemble des chemins vers les images et des annotations correspondantes.\n",
    "\n",
    "Un objet `Dataset` est *itérable* dans la mesure où il est possible boucler dessus pour en récupérer les éléments. La méthode `_getitem_` permet justement d'accéder à un élément à partir d'un indice. Ci-dessous on demande à chaque itération de retourner un dictionnnaire dont les items (clef, valeur) sont : \n",
    "- (`image`, l'image sous la forme d'un `np.array`) \n",
    "- (`label`, l'annotation associée (0 ou 1)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths,labels):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        image = iio.imread(self.image_paths[idx])\n",
    "        image = torch.tensor(np.array(image,dtype = float)/255, dtype=torch.float).permute(2,1,0)\n",
    "        label = self.labels[idx]\n",
    "        return {\"image\": image, \"label\" : label}\n",
    "\n",
    "    def __len__(self):  # return count of sample we have\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition de quelques hyper-paramètres et paramètres d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On fixe la valeurs de plusieurs paramètres sur lesquels on reviendra plus tard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'n_epoch' : 10,\n",
    "    'val_size' : 0.15,\n",
    "    'batch_size' : 30,\n",
    "    'optimizer' : \"SGD\",\n",
    "    'lr' : 0.005,\n",
    "    'momentum' : 0.9,\n",
    "    'model type': \"convnet\",\n",
    "    'descriptif': \"Entrainement avec un convnet\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création des `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Das un premier temps on va découper notre jeu d'apprentissage (les 366 images labellisées) en : \n",
    "- un jeu de d'entraînement sur lequel on entraînera notre modèle de classification\n",
    "- un jeu de validation ne participant pas à l'entrainement du modèle mais permettant d'en évaluer les performances\n",
    "\n",
    "La fonction `train_test_split` de la librairie `scikit-learn` permet de faire ce découpage. Le paramètre `val_size` du dictionnaire de configuration donne le taux d'images du jeu de données initial que l'on veut inclure dans le jeu de validation. Le paramètre `stratify` de la fonction `train_test_split` permet de préciser sur quel critère on veut stratifier cette sélection aléatoire (on choisit la variable de labels dans le but d'avoir un échantillon représentatif des labels de l'ensemble des images).\n",
    "\n",
    "Lors de l'entraînement, on va procéder par itérations sur des petits paquets d'images (appelés *batchs*). Pour chaque batch, on calcule batchs l'erreur de prédiction et son gradient, puis on modifie les paramètres du modèle en conséquence (par descente de gradient).\n",
    "\n",
    "L'extraction de paquets d'images est facilitée par la création d'objets `Dataloader`. Le paramètre `batch_size`permet de préciser la taille des batchs désirée. Le paramètre `shuffle` égal à `True` signifie que les images seront toutes remélangées une fois un tour total de l'ensemble des images réalisé. On appelle un tour complet une *epoch*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =  config['batch_size']\n",
    "all_dataset = CustomDataset(train_images_paths,train_images_labels)\n",
    "\n",
    "# On découpe le data set en train et validation (20 % de validation) en stratifiant par les labels\n",
    "train_indices, valid_indices = train_test_split(\n",
    "    list(range(len(train_images_labels))),\n",
    "    test_size=config['val_size'],\n",
    "    stratify=train_images_labels\n",
    ")\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(all_dataset, train_indices)\n",
    "valid_dataset = torch.utils.data.Subset(all_dataset, valid_indices)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=80, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Définition des modèles de classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les réseaux de neurones convolutionnels sont en général très efficaces pour des tâches de classification d'images. On définit ici une classe `Net` (héritant de la classe `nn.Module` de `Pytorch`) qui implémente un réseau de neurones convolutifs.\n",
    "\n",
    "La méthode `__init__` permet de définir l'ensemble des couches utilisées dans la méthode de propagation `forward`. Cette méthode définit la suite des opérations appliquées aux images `x` passant par dans réseau `Net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 4)\n",
    "        self.conv4 = nn.Conv2d(32, 16, 3)\n",
    "        self.conv5 = nn.Conv2d(16, 24, 3)\n",
    "        self.fc1 = nn.Linear(24 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, NB_CLASSES)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = torch.flatten(x, 1)  # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a défini un modèle de réseau de neurones convolutifs. Cette structure est très utilisée lorsqu'on traite des problèmes sur les images. De manière assez générale en Machine Learning, on fonctionne souvent en 2 étapes : \n",
    "1) extraction de caractéristiques de l'entrée (extraction des composantes principales d'une ACP par exemple)\n",
    "2) un classifieur est appliqué aux caractéristiques extraites, ce qui permet de prédire une classe\n",
    "\n",
    "Les réseaux de neurones convolutifs fonctionnent sur le même principe, si ce n'est que l'extraction des caractéristiques est complètement internalisée dans le modèle. On apprend au modèle à extraire ces caractéristiques lui-même. L'extraction de caractéristique est permise par l'usage d'opérations de *convolution* et de *pooling* permettant de réduire la dimension de l'image.\n",
    "\n",
    "Dans un premier temps (à l'intérieur du constructeur), on a défini les différents objets utiles à la construction du réseau.\n",
    "\n",
    "Les couches `Conv2d` sont des couches *convolutives* appliquables aux images. Appliquer une convolution revient à appliquer un filtre sur l'image. Un filtre est une matrice réelle généralement de petite dimension. Appliquer ce filtre au pixel $(i,j)$ revient à effectuer la moyenne pondérée par les coefficients du filtre des pixels au voisinage de $(i,j)$, le voisinage étant défini par la dimension du filtre.\n",
    "\n",
    "Les couches `Conv2D` prennent notamment en paramètres : \n",
    "- le nombre de canaux en entrée : la première convolution prend 3 canaux en entrée car c'est elle qui sera appliquée à notre image de départ qui a trois canaux (image en couleurs)\n",
    "- le nombre de canaux en sortie : la première couche convolutive aura 6 canaux de sortie. Ce qui veut dire que 6 filtres seront générée par cette couche donnant naissance à 6 images en sortie résultantes de l'application des 6 filtres sur l'image 3 x 256 x 256 initiale\n",
    "- la taille des filtres appliqués\n",
    "\n",
    "Plein d'autres paramètres existent, que l'on peut consulter sur la documentation `Pytorch` (voir `stride` et `padding` par exemple). Les coefficients contenus dans les filtres font partie des paramètres du modèle, ce sont eux qui seront entraînés, *i.e* modifiés à chaque itération de l'entraînement.\n",
    "\n",
    "L'opération *maxpool* est également une opération de réduction de la dimension, mais elle ne contient pas de paramètres. Le *maxpool* applique un filtre à une image en entrée.  Appliquer ce filtre au pixel $(i,j)$ revient à rechercher le coefficient maximum au voisinage de $(i,j)$, le voisinage étant défini par la dimension du filtre. \n",
    "\n",
    "L'extraction de caractéristiques de l'image est en fait réalisé par l'enchaînement de couches convolutives. Une couche convolutive entière est composée d'une couche `Conv2D`, d'une couche `Maxpool2D` et de l'application d'une fonction non linéaire (non paramétrée, ici la fonction ReLU). L'entraînement modifiera les paramètres de cet enchaînement de couches (en l'occurrence les paramètres des filtres de convolution) de telle sorte que l'extraction de caractéristiques soit la plus pertinente possible au regard du problème posé.\n",
    "\n",
    "Après passage des 5 couches convolutives sur l'image initiale, 24 * 4 * 4 = 384 caractéristiques on été extraites. \n",
    "Les couches `Linear` permettent de construire le classifieur permettant par la suite de trancher entre le label 0, ou 1 pour l'image considérée. On notera par ailleurs que la sortie du réseau à la même dimension que le nombre de classes. Cette sortie de dimension 2 représente en fait le score attribué à telle ou telle classe par le modèle permettant de décider si l'image appartient à la classe 0 ou la classe 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On crée un modèle de la classe `Net`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut voir la structure de notre réseau grâce à la ligne suivante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(16, 32, kernel_size=(4, 4), stride=(1, 1))\n",
       "  (conv4): Conv2d(32, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv5): Conv2d(16, 24, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=384, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction ci-dessous permet d'obtenir le nombre de paramètres de notre modèle.\n",
    "Notre *petit* modèle contient en fait 75374 paramètres, ce qui dépasse l'ordre de grandeur usuel du nombre de paramètres rencontré en Machine Learning *classique*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75734"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_n_params(model):\n",
    "    pp = 0\n",
    "    for p in list(model.parameters()):\n",
    "        nn = 1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn * s\n",
    "        pp += nn\n",
    "    return pp\n",
    "\n",
    "get_n_params(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation des images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous une fonction permettant d'afficher une image d'oiseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_image(image):\n",
    "    img = image.permute(2,1,0)\n",
    "    img = np.array(img)\n",
    "    plt.imshow(np.array(img))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous on utilise la fonction précédente pour afficher des images du jeu d'entraînement aléatoirement en itérant sur `DataLoader` d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "No such file: '/home/onyxia/work/cours-nouvelles-donnees-site/applications/images/oiseaux/valid/ABBOTTS BABBLER/3.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m t = \u001b[38;5;28miter\u001b[39m(train_loader)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m dico = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m images = dico[\u001b[33m\"\u001b[39m\u001b[33mimage\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m      4\u001b[39m labels = dico[\u001b[33m\"\u001b[39m\u001b[33mlabel\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mCustomDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.is_tensor(idx):\n\u001b[32m      8\u001b[39m     idx = idx.tolist()\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m image = \u001b[43miio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mimage_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m image = torch.tensor(np.array(image,dtype = \u001b[38;5;28mfloat\u001b[39m)/\u001b[32m255\u001b[39m, dtype=torch.float).permute(\u001b[32m2\u001b[39m,\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m)\n\u001b[32m     12\u001b[39m label = \u001b[38;5;28mself\u001b[39m.labels[idx]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/imageio/v3.py:53\u001b[39m, in \u001b[36mimread\u001b[39m\u001b[34m(uri, index, plugin, extension, format_hint, **kwargs)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     51\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m] = index\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mimopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mplugin_kwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m img_file:\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m np.asarray(img_file.read(**call_kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/imageio/core/imopen.py:113\u001b[39m, in \u001b[36mimopen\u001b[39m\u001b[34m(uri, io_mode, plugin, extension, format_hint, legacy_mode, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m     request.format_hint = format_hint\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m113\u001b[39m     request = \u001b[43mRequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mio_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mformat_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextension\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextension\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m source = \u001b[33m\"\u001b[39m\u001b[33m<bytes>\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(uri, \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m uri\n\u001b[32m    117\u001b[39m \u001b[38;5;66;03m# fast-path based on plugin\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# (except in legacy mode)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/imageio/core/request.py:250\u001b[39m, in \u001b[36mRequest.__init__\u001b[39m\u001b[34m(self, uri, mode, extension, format_hint, **kwargs)\u001b[39m\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid Request.Mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Parse what was given\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_parse_uri\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# Set extension\u001b[39;00m\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m extension \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/python/lib/python3.13/site-packages/imageio/core/request.py:410\u001b[39m, in \u001b[36mRequest._parse_uri\u001b[39m\u001b[34m(self, uri)\u001b[39m\n\u001b[32m    407\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_read_request:\n\u001b[32m    408\u001b[39m     \u001b[38;5;66;03m# Reading: check that the file exists (but is allowed a dir)\u001b[39;00m\n\u001b[32m    409\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(fn):\n\u001b[32m--> \u001b[39m\u001b[32m410\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mNo such file: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m % fn)\n\u001b[32m    411\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    412\u001b[39m     \u001b[38;5;66;03m# Writing: check that the directory to write to does exist\u001b[39;00m\n\u001b[32m    413\u001b[39m     dn = os.path.dirname(fn)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: No such file: '/home/onyxia/work/cours-nouvelles-donnees-site/applications/images/oiseaux/valid/ABBOTTS BABBLER/3.jpg'"
     ]
    }
   ],
   "source": [
    "t = iter(train_loader)\n",
    "dico = next(t)\n",
    "images = dico[\"image\"]\n",
    "labels = dico[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    print(labels[i])\n",
    "    print(dic_id_to_label[int(labels[i])])\n",
    "    display_image(images[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrainement du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme dit précédemment, entraîner le modèle revient à modifier, itération par itération ses paramètres afin que ce dernier soit le plus pertinent possible dans la tâche de classification qu'il doit réaliser. Une itération consiste en 4 étapes :\n",
    "\n",
    "1) Récupération d'un batch d'images\n",
    "2) Calcul de l'erreur de classification commise par le modèle sur le batch (l'erreur est une fonction de ses paramètres)\n",
    "3) Calcul du gradient de l'erreur\n",
    "4) Descente du gradient, *i.e* on bouge les paramètres dans la direction opposée du gradient calculé précédemment pour abaisser l'erreur commise par le modèle. Le taux d'apprentissage permet de définir l'amplitude du mouvement dans cette direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La structure des réseaux de neurones est telle que les calculs peuvent être effectués par des GPUS pour réduire grandement le temps de calcul. Comparer la différence de temps d'execution en initiant la variable device à \"cuda:0\" (ce qui veut dire que les calculs seront lancés sur GPU). Pour cela il faut que le service utilisé mette des GPUs à disposition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): device = torch.device(\"cuda:0\" )\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"cpu\"\n",
    "# device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lancement de l'entraînement**\n",
    "\n",
    "On définit un processus d'optimisation `Adam` et une fonction de perte `CrossEntropyLoss` pour pouvoir procéder à l'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.SGD(net.parameters(), lr=config['lr'], momentum=config['momentum'])\n",
    "optimizer = optim.Adam(net.parameters(), lr=config['lr'])\n",
    "net = net.to(device)\n",
    "\n",
    "entropy = nn.CrossEntropyLoss()\n",
    "\n",
    "liste_loss = []\n",
    "liste_acc_val = []\n",
    "for epoch in range(config['n_epoch']):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    t = tqdm(train_loader, desc=\"epoch %i\" % (epoch+1), position = 0, leave=True)\n",
    "    epoch_loop = enumerate(t)\n",
    "\n",
    "    for i, data in epoch_loop:\n",
    "        taille_batch = data['image'].shape[0]\n",
    "        images = data['image']\n",
    "        labels  =  data['label']\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        pred = net(images)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss = entropy(pred,labels)\n",
    "        loss.backward()  # Calculer le gradient\n",
    "        optimizer.step()  # Avancer dans le sens du gradient calculé\n",
    "\n",
    "        del images, labels, pred\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        if (i+1) % 10 == 0:\n",
    "            # Enregistrement de la loss sur le train, sur le validation\n",
    "            liste_loss.append(running_loss)\n",
    "\n",
    "            # Validation\n",
    "            ech_val = iter(valid_loader)\n",
    "            dico = next(ech_val)\n",
    "            images = dico[\"image\"]\n",
    "            labels = dico[\"label\"]\n",
    "\n",
    "            pred = torch.argmax(net(images.to(device)), dim=1)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            acc_val = round(\n",
    "                100 * int(torch.sum(pred == labels).cpu()) / int(np.array(labels.size())),\n",
    "                2\n",
    "            )\n",
    "            liste_acc_val.append(acc_val)\n",
    "            t.set_description(\"epoch %i, 'mean loss: %.6f', 'acc.val: %.2f'\" % (epoch+1, running_loss/10, acc_val))\n",
    "            t.refresh()\n",
    "            running_loss =0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Attention** : selon la configuration il faut parfois beaucoup d'epochs pour voir la perte diminuer, ne désespérez pas ! \n",
    "Si la perte ne descend pas, une option est de tester un autre optimiseur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si l'entraînement avance trop lentement, réouvrez un autre service et essayez de mettre en place la partie **Un autre jeu d'images pour s'amuser**, voir ci-dessous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation du réseau entraîné"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lors de l'entraînement, les moyennes de la perte sur le jeu d'entraînement et de la précision obtenue sur le jeu de validation sont enregistrées tous les 5 batchs dans :\n",
    "- `liste_loss` \n",
    "- `liste_acc_val` \n",
    "\n",
    "On peut les représenter graphiquement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('epoch')\n",
    "ax1.set_ylabel('valiadation accuracy', color=color)\n",
    "ax1.plot(liste_acc_val, color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx()   # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('entropy loss', color=color)  # we already handled the x-label with ax1\n",
    "ax2.plot(liste_loss, color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que rapidement, la précision obtenue sur le jeu de validation se stabilise et même dimininue (sur-apprentissage). On pourrait arrêter l'entraînement à ce moment là."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut calculer la matrice de confusion pour le jeu de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = iter(valid_loader)\n",
    "dico = next(t)\n",
    "\n",
    "images = dico[\"image\"]\n",
    "labels = dico[\"label\"]\n",
    "\n",
    "pred = np.array(torch.argmax(net(images.to(device)),dim =1).cpu())\n",
    "labels = np.array(labels)\n",
    "\n",
    "confusion_matrix(pred, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate que (remplir les **??**) : \n",
    "- La majorité des images du jeu de validation sont correctement classées par notre algorithme\n",
    "- **??** images du jeu de validation sont classées parmi les 0 par notre algorithme à tort\n",
    "- **??** images du jeu de validation sont classées parmi les 1 par notre algorithme à tort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Que faire pour continuer ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jouer sur les différents hyperparamètres (dans `config`). Quels sont les impacts sur les résultats du :\n",
    "- Nombre d'élements par batch ?\n",
    "- Nombre d'epochs ?\n",
    "- Learning rate ?\n",
    "- Momentum ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que se passe-t-il si on augmente le nombre de classes d'oiseau à prédire ? (`NB_CLASSES`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Un autre jeu d'images pour s'amuser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on va charger un autre jeu d'images, de chats et de chiens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.get('projet-funathon/diffusion/Sujet9_deep_learning_donnees_satellites/train.zip', 'chat_chien.zip')\n",
    "if not os.path.exists('chat_chien'):\n",
    "    os.mkdir('chat_chien')\n",
    "shutil.unpack_archive('chat_chien.zip', extract_dir='chat_chien')\n",
    "os.remove('chat_chien.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic_label_to_id  = {\"cat\": 0, \"dog\": 1}\n",
    "train_images_file = os.listdir(\"chat_chien/train\")\n",
    "\n",
    "train_images_paths = [\"chat_chien/train/\" + path for path in train_images_file]\n",
    "train_images_labels = [dic_label_to_id[st[0:3]] for st in train_images_file]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La liste des chemins de fichiers pointant vers les images et la liste des labels ayant été définies, vous pouvez essayer de reproduire les différentes étapes introduites précédemment :\n",
    "1) Création de la classe dataset\n",
    "2) Création des loaders (essayer d'afficher les images par la suite)\n",
    "3) Création du modèle\n",
    "4) Entraînement du modèle\n",
    "5) Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Documentation Pytorch](https://pytorch.org/docs/stable/index.html)  \n",
    "- [Un autre exemple de classification avec `Pytorch` (ici le data set KMNIST)](https://pyimagesearch.com/2021/07/19/pytorch-training-your-first-convolutional-neural-network-cnn/)  \n",
    "- [Utiliser des réseaux pré-entrainés](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
