[
  {
    "objectID": "applications/application1.html",
    "href": "applications/application1.html",
    "title": "Application 1 - Données spatiales",
    "section": "",
    "text": "En premier lieu, ce TD utilise une source administrative nommée DVF (« Demandes de Valeurs Foncières »).\nL’analyse de ces données sera complétée des données Filosofi produites par l’Insee :\nEnfin, nous proposons trois contours géographiques ad hoc :\nL’objectif de ce TD est d’illustrer la manière dont peuvent être traitées des données spatiales de manière flexible avec duckdb.",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Application 1 - Données spatiales"
    ]
  },
  {
    "objectID": "applications/application1.html#import-des-données",
    "href": "applications/application1.html#import-des-données",
    "title": "Application 1 - Données spatiales",
    "section": "Import des données",
    "text": "Import des données\nL’import des contours en  se fait assez naturellement grâce à sf.\n\ntriangle &lt;- sf::st_read(\"data/triangle.geojson\", quiet=TRUE)\nmalakoff &lt;- sf::st_read(\"data/malakoff.geojson\", quiet=TRUE)\nmontrouge &lt;- sf::st_read(\"data/montrouge.geojson\", quiet=TRUE)\n\nEn premier lieu, on peut visualiser la ville de Malakoff :\n\nmapview(malakoff) + mapview(triangle, col.regions = \"#ffff00\")\n\n\n\n\n\nEt ensuite les contours de Montrouge :\n\nmapview(montrouge)",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Application 1 - Données spatiales"
    ]
  },
  {
    "objectID": "applications/application1.html#préparation-de-duckdb",
    "href": "applications/application1.html#préparation-de-duckdb",
    "title": "Application 1 - Données spatiales",
    "section": "Préparation de DuckDB",
    "text": "Préparation de DuckDB\nDuckDB est un moteur de base de données analytique en mémoire, optimisé pour les requêtes SQL sur des données volumineuses, particulièrement adapté aux fichiers plats comme Parquet ou CSV, et intégrable dans des langages comme Python, R ou SQL.\nEn principe, duckdb fonctionne à la manière d’une base de données. Autrement dit, on définit une base de données et effectue des requêtes (SQL ou verbes tidyverse) dessus. Pour créer une base de données, il suffit de faire un read_parquet avec le chemin du fichier.\nLa base de données se crée tout simplement de la manière suivante :\n\ncon &lt;- dbConnect(duckdb::duckdb())\ndbExecute(con, \"INSTALL spatial;\")\ndbExecute(con, \"LOAD spatial;\")\n\nNous verrons ultérieurement pourquoi nous avons besoin de cette extension spatiale.\nCette connexion duckdb peut être utilisée de plusieurs manières. En premier lieu, par le biais d’une requête SQL. dbGetQuery permet d’avoir le résultat sous forme de dataframe puisque la requête est déléguée à l’utilitaire duckdb qui est embarqué dans les fichiers de la librairie :\n\nout &lt;- dbGetQuery(\n  con,\n  glue(  \n    'SELECT * EXCLUDE (geometry) FROM read_parquet(\"data/dvf.parquet\") LIMIT 5'\n  )\n)\nout\n\nLa chaîne d’exécution ressemble ainsi à celle-ci :\n\n\n\n\n\nMême si DuckDB simplifie l’utilisation du SQL en proposant de nombreux verbes auxquels on est familier en R ou Python, SQL n’est néanmoins pas toujours le langage le plus pratique pour chaîner des opérations nombreuses. Pour ce type de besoin, le tidyverse offre une grammaire riche et cohérente. Il est tout à fait possible d’interfacer une base duckdb au tidyverse. On pourra donc utiliser nos verbes préférés (mutate, filter, etc.) sur un objet duckdb : une phase préliminaire de traduction en SQL sera automatiquement mise en oeuvre :\n\n\n\n\n\n\ntable_logement &lt;- tbl(con, glue('read_parquet(\"data/dvf.parquet\")'))\ntable_logement %&gt;% head(5)",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Application 1 - Données spatiales"
    ]
  },
  {
    "objectID": "applications/application1.html#premières-requêtes-sql-description-des-données-dvf",
    "href": "applications/application1.html#premières-requêtes-sql-description-des-données-dvf",
    "title": "Application 1 - Données spatiales",
    "section": "Premières requêtes SQL : description des données DVF",
    "text": "Premières requêtes SQL : description des données DVF\nTout d’abord, il convient de se familiariser avec les données. Les requêtes proposées pour l’exercice 1 permettent d’obtenir des informations primordiale de manière très rapide et sans nécessité de charger l’ensemble des données dans la mémoire vive.\n\n\n\n\n\n\nExercice 1\n\n\n\nCet exercice nous fera rentrer progressivement dans les données à partir de quelques requêtes basiques.\n\nLire les 10 premières lignes des données par l’approche SQL et par l’approche tidyverse.\nAfficher les noms des colonnes selon les deux approches.\nRegarder les valeurs uniques de la colonne nature_mutation selon les deux approches.\nCalculer les bornes min et max des prix des transactions selon ces deux approches.\n\n\n\nA la question 1, vous devriez avoir :\n\n\n\n  \n\n\n\nA la question 2, la liste des colonnes donnera plutôt\n\n\n [1] \"id_mutation\"                  \"date_mutation\"               \n [3] \"numero_disposition\"           \"nature_mutation\"             \n [5] \"valeur_fonciere\"              \"adresse_numero\"              \n [7] \"adresse_suffixe\"              \"adresse_nom_voie\"            \n [9] \"adresse_code_voie\"            \"code_postal\"                 \n[11] \"code_commune\"                 \"nom_commune\"                 \n[13] \"code_departement\"             \"ancien_code_commune\"         \n[15] \"ancien_nom_commune\"           \"id_parcelle\"                 \n[17] \"ancien_id_parcelle\"           \"numero_volume\"               \n[19] \"lot1_numero\"                  \"lot1_surface_carrez\"         \n[21] \"lot2_numero\"                  \"lot2_surface_carrez\"         \n[23] \"lot3_numero\"                  \"lot3_surface_carrez\"         \n[25] \"lot4_numero\"                  \"lot4_surface_carrez\"         \n[27] \"lot5_numero\"                  \"lot5_surface_carrez\"         \n[29] \"nombre_lots\"                  \"code_type_local\"             \n[31] \"type_local\"                   \"surface_reelle_bati\"         \n[33] \"nombre_pieces_principales\"    \"code_nature_culture\"         \n[35] \"nature_culture\"               \"code_nature_culture_speciale\"\n[37] \"nature_culture_speciale\"      \"surface_terrain\"             \n[39] \"longitude\"                    \"latitude\"                    \n[41] \"geometry\"                     \"__index_level_0__\"           \n\n\nQue contient le champ nature_mutation ? (il a été filtré au ventes classiques pour simplifié cette application ; les vraies données sont plus riches).\n\n\n\n  \n\n\n\nA la question 4, vous devriez obtenir des statistiques similaires à celles-ci :\n\n\n\n  \n\n\n\nNous venons de voir comment faire quelques requêtes basiques sur un geoparquet avec duckdb et l’équivalence entre les approches SQL et tidyverse. La dernière question était déjà une introduction au calcul à la volée de statistiques descriptives, ajoutons quelques statistiques avec ce nouvel exercice.\n\n\n\n\n\n\nExercice 2 : statistiques descriptives sur la dimension attributaire\n\n\n\nNe garder que les seules transactions effectuées à Montrouge ou Malakoff et faire une médiane par communes des montants des transactions\nFaire ceci avec SQL et dplyr[^chatGPT]\n\n\n[^chatGPT] : Vous avez le droit d’utiliser chatGPT ou Claude ou vous IA assistante préférée ! Mais ne prenez pas pour argent comptant ce qu’elle vous propose.\nAvec l’approche SQL vous devriez obtenir\n\n\n  code_commune mediane_valeur_fonciere\n1        92049                  394500\n2        92046                  375000\n\n\nOn peut se rassurer, on obtient la même chose l’approche dplyr :\n\n\n\n  \n\n\n\nOn peut en conclure que les biens vendus à Montrouge (dans notre base) ont une médiane un peu plus élevée qu’à Malakoff.",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Application 1 - Données spatiales"
    ]
  },
  {
    "objectID": "applications/ape.html",
    "href": "applications/ape.html",
    "title": "Application 1: classification automatique de textes",
    "section": "",
    "text": "Cette application illustrera certains apports des outils du NLP pour la codification automatique des déclarations d’activité dans la nomenclature des activités françaises. On pourra coder dans un notebook au sein de l’environnement SSP Cloud suivant:\nCe tutoriel n’a pas vocation à introduire aux principaux concepts du NLP (tokenisation, sac de mot, embedding, etc.) mais à être une introduction pratique à la thématique de la classification textuelle. Pour découvrir les concepts centraux du NLP, se référer au cours de Python pour la data science de l’ENSAE.\nDans une démarche exploratoire, le plus simple est de commencer par compter les mots de manière indépendante (approche sac de mot). Par exemple, de manière naturelle, nous avons beaucoup plus de déclarations liées à la boulangerie que liées à la data science:\nfilter_train_data(train, \"data science\").head(5)\nfilter_train_data(train, \"boulanger\").head(5)\n\nNombre d'occurrences de la séquence 'data science': 54\nNombre d'occurrences de la séquence 'boulanger': 1928\n\n\n\n\n\n\n\n\n\nnace\ntext\nCode\nLibellé\n\n\n\n\n90\n1071C\nBOULANGERIE PATISSERIE VIENNOISERIE\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n107\n1071C\nBOULANGERIE PATISSERIE FABRICATION\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n153\n1071C\nBOULANGERIE PATISSERIE GLACES CONFISERIES BOIS...\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n314\n1071C\nBOULANGERIE PATISSERIE VIENNOISERIE CONFISE...\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n487\n1071C\nBOULANGERIE PATISSERIE ACHAT VENTE ET MAINT...\n1071C\nBoulangerie et boulangerie-pâtisserie\nLes wordclouds peuvent servir à rapidement visualiser la structure d’un corpus. On voit ici que notre corpus est très bruité car nous n’avons pas nettoyé celui-ci:\nPour commencer à se faire une idée sur les spécificités des catégories, on peut représenter le corpus de certaines d’entre elles ? Arrivez-vous à inférer la catégorie de la NAF en question ? Si oui, vous utilisez sans doute des heuristiques proches de celles que nous allons mettre en oeuvre dans notre algorithme de classification.\nNéanmoins, à ce stade, les données sont encore très bruitées. La première étape classique est de retirer les stop words et éventuellement des termes spécifiques à notre corpus. Par exemple, pour des données de caisse, on retirera les bruits, les abréviations, etc. qui peuvent bruiter notre corpus.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1: classification automatique de textes"
    ]
  },
  {
    "objectID": "applications/ape.html#exploration-du-jeu-de-données",
    "href": "applications/ape.html#exploration-du-jeu-de-données",
    "title": "Application 1: classification automatique de textes",
    "section": "Exploration du jeu de données",
    "text": "Exploration du jeu de données\nCe tutoriel se propose d’illustrer la problématique de la classification automatique par le biais de l’algorithme d’apprentissage supervisé FastText, développé par Meta, à partir des données issues des déclarations Sirene.\nL’idée de ce tutoriel est de classer des déclarations d’entreprise dans une nomenclature générique des activités productives. Celle-ci permet de produire de nombreuses statistiques économiques sectorielles sur le tissu productif français. L’Insee ayant vocation à produire des statistiques agrégées sur de nombreuses questions, cette approche constitue l’un des principaux cas d’application du NLP pour l’Institut dans des domaines aussi divers que la classification dans une nomenclature d’activités (NAF), une nomenclature de professions (PCS), de produits (COICOP), de lieux géographiques, etc.\nComment passe-t-on d’une déclaration en langage naturel, qui fait sens pour un entrepreneur, à une représentation plus générique, et forcément plus simpliste, de l’activité d’une entreprise, qui fait sens à une institution statistique et espérons, pour le débat public ? Grâce à des algorithmes de classement ad hoc qui permettent d’extraire une information à partir de libellés textuels. Historiquement l’Insee utilisait des règles déterministes de classement à partir d’un algorithme nommé Sicore, une IA symbolique qui classait à partir de règles métiers préconfigurée.\nAvec l’avènement du machine learning, la possibilité d’entraîner un algorithme apprenant par induction plutôt que de manière déductive, est apparue être une voie d’investissement intéressante pour l’Insee. L’objet de ce TD est d’illustrer la démarche adoptée avec cette approche à partir d’un modèle un peu plus simple que celui mis en oeuvre à l’Insee mais reprenant les principales caractéristiques de celui-ci.\nLe code pour lire les données est directement fourni:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom wordcloud import WordCloud\n\nDATA_PATH = \"https://minio.lab.sspcloud.fr/projet-formation/diffusion/mlops/data/firm_activity_data.parquet\"\nNAF_PATH = \"https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/naf2008_liste_n5.xls\"\nnaf = pd.read_excel(NAF_PATH, skiprows = 2)\nnaf['Code'] = naf['Code'].str.replace(\".\",\"\")\ntrain = pd.read_parquet(DATA_PATH)\ntrain = train.merge(naf, left_on = \"nace\", right_on = \"Code\")\ntrain.head(5)\n\n\n\n\n\n\n\n\nnace\ntext\nCode\nLibellé\n\n\n\n\n0\n8220Z\nMISSIONS PONCTUELLES A L AIDE D UNE PLATEFORME\n8220Z\nActivités de centres d'appels\n\n\n1\n8553Z\nINSPECTEUR AUTOMOBILE\n8553Z\nEnseignement de la conduite\n\n\n2\n5520Z\nLA LOCATION TOURISTIQUE DE LOGEMENTS INSOLITES...\n5520Z\nHébergement touristique et autre hébergement d...\n\n\n3\n4791A\nCOMMERCE DE TOUT ARTICLES ET PRODUITS MARCHAND...\n4791A\nVente à distance sur catalogue général\n\n\n4\n9499Z\nREGROUPEMENT RETRAITE\n9499Z\nAutres organisations fonctionnant par adhésion...\n\n\n\n\n\n\n\nLe premier exercice a vocation à illustrer la manière classique de rentrer dans un corpus de données textuelles. La démarche n’est pas particulièrement originale mais permet d’illustrer les enjeux du nettoyage de texte.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1: classification automatique de textes"
    ]
  },
  {
    "objectID": "applications/ape.html#exercice-1",
    "href": "applications/ape.html#exercice-1",
    "title": "Application 1: classification automatique de textes",
    "section": "Exercice 1",
    "text": "Exercice 1\n\nLancer le code ci-dessous pour préparer votre environnement de travail :\n\nimport spacy\n!python -m spacy download fr_core_news_sm\nimport nltk\nnltk.download('punkt_tab')\nnltk.download('stopwords')\n\nCréer une fonction pour compter le nombre de textes contenant une séquence de caractères donnée dans le corpus. La tester avec “data science” et “boulanger”.\nFaire une fonction pour afficher le wordcloud de notre corpus dans son ensemble et de certaines catégories pour comprendre la nature de notre corpus.\nRetirer les stopwords à partir de la liste des mots disponibles dans SpaCy.\n\n\n\nAide\n\nfrom nltk.tokenize import word_tokenize\nimport spacy\n\nnlp = spacy.load(\"fr_core_news_sm\")\nstop_words = #liste de stopwords\n\n# Function to remove stopwords\ndef remove_stopwords(text):\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n    return ' '.join(filtered_text)\n\ndef remove_single_letters(text):\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if len(word) &gt; 1]\n    return ' '.join(filtered_text)\n\n# Apply the function to the 'text' column\ntrain['text_clean'] = (train['text']\n    .apply(remove_stopwords)\n    .apply(remove_single_letters)\n)\n\n\nRefaire quelques uns des nuages de mots et étudier la différence avant nettoyage.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1: classification automatique de textes"
    ]
  },
  {
    "objectID": "applications/ape.html#premier-algorithme-dapprentissage-supervisé",
    "href": "applications/ape.html#premier-algorithme-dapprentissage-supervisé",
    "title": "Application 1: classification automatique de textes",
    "section": "Premier algorithme d’apprentissage supervisé",
    "text": "Premier algorithme d’apprentissage supervisé\nNous avons nettoyé nos données. Cela devrait améliorer la pertinence de nos modèles en réduisant le ratio signal/bruit. Nous allons généraliser notre nettoyage de texte en appliquant un peu plus d’étapes que précédemment. Nous allons notamment raciniser nos mots.\nPour cela, récupérer les fichiers suivants:\n\nconstants.py\nprocessor.py\nutils.py\n\net mettre ceux-ci dans le même dossier que votre notebook Jupyter.\nLe code de nettoyage est directement fourni:\n\nfrom processor import Preprocessor\npreprocessor = Preprocessor()\n\n# Preprocess data before training and testing\nTEXT_FEATURE = \"text\"\nY = \"nace\"\n\ndf = preprocessor.clean_text(train, TEXT_FEATURE).drop('text_clean', axis = \"columns\")\ndf.head(2)\n\n\n\n\n\n\n\n\nnace\ntext\nCode\nLibellé\n\n\n\n\n0\n8220Z\nmission ponctuel aid plateform\n8220Z\nActivités de centres d'appels\n\n\n1\n8553Z\ninspecteur automobil\n8553Z\nEnseignement de la conduite\n\n\n\n\n\n\n\nNous allons commencer à entraîner un modèle dont le plongement de mot est de faible dimension. Voici les paramètres qui seront utiles pour le prochain exercice.\n\nimport pathlib\n\nparams = {\n    \"dim\": 25,\n    \"label_prefix\": \"__label__\"\n}\n\ndata_path = pathlib.Path(\"./data\")\ndata_path.mkdir(parents=True, exist_ok=True)\n\ndef write_training_data(df, params, training_data_path=None):\n    warnings.filterwarnings(\"ignore\", \"Setuptools is replacing distutils.\")\n    if training_data_path is None:\n        training_data_path = get_root_path() / \"data/training_data.txt\"\n\n    with open(training_data_path, \"w\", encoding=\"utf-8\") as file:\n        for _, item in df.iterrows():\n            formatted_item = f\"{params['label_prefix']}{item[Y]} {item[TEXT_FEATURE]}\"\n            file.write(f\"{formatted_item}\\n\")\n    return training_data_path.as_posix()\n\n\n\nDécouper notre échantillon complet en train et test.\nFastText effectue son entraînement à partir d’objets stockés dans un .txt. Utiliser la fonction write_training_data de la manière suivante pour l’écrire.\n\n# Write training data in a .txt file (fasttext-specific)\ntraining_data_path = write_training_data(df_train, params, pathlib.Path(str(data_path.absolute()) + \"/training_data.txt\"))\n\nAvec l’aide de la documentation de la librairie FastText, entraîner votre modèle de classification.\nSauvegarder le modèle sous forme de binaire, cela pourra éventuellement servir ultérieurement.\nRenvoyer les trois catégories les plus probables pour les nouveaux libellés suivants:\n\nlist_libs = [\"vendeur d'huitres\", \"boulanger\"]\n\nSur l’ensemble du jeu de test, renvoyer la meilleure prédiction pour chaque descriptif d’activités. Evaluer la performance globale et la performance classe par classe, par exemple en calculant le rappel (pour les classes de plus de 200 cas).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1: classification automatique de textes"
    ]
  },
  {
    "objectID": "applications/ape.html#pour-aller-plus-loin-introduction-au-mlops",
    "href": "applications/ape.html#pour-aller-plus-loin-introduction-au-mlops",
    "title": "Application 1: classification automatique de textes",
    "section": "Pour aller plus loin, introduction au MLOps",
    "text": "Pour aller plus loin, introduction au MLOps\nOn utilise dans cette application un modèle de Machine Learning (ML) pour prédire l’activité des entreprises à partir de texte descriptifs. Les méthodes de ML sont quasiment indispensables pour traiter du texte, mais utiliser des modèles de ML pour servir des cas d’usage réels demande de respecter un certain nombre de bonnes pratiques pour que tout se passe convenablement, en particulier:\n\nTracking propre des expérimentations\nVersioning des modèles, en même temps que des données et du code correspondants\nMise à disposition efficace du modèle aux utilisateurs\nMonitoring de l’activité du modèle servi\nRéentraînement du modèle\n\nUne introduction à ces bonnes pratiques, auxquelles on fait régulièrement référence à travers le terme MLOps, est donné dans cette formation (dépôt associé).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1: classification automatique de textes"
    ]
  },
  {
    "objectID": "textes_exemples.html",
    "href": "textes_exemples.html",
    "title": "Application 2",
    "section": "",
    "text": "L’objectif de ce TP est d’explorer les aspects suivants du traitement du langage naturel:\n\nPreprocessing\nElasticSearch: indexation et requêtage\n\nIl est disponible sur cette page en version web et peut être ouvert sur l’environnement SSPCloud où ElasticSearch est disponible en cliquant sur le bouton suivant:",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 2"
    ]
  },
  {
    "objectID": "administratives.html",
    "href": "administratives.html",
    "title": "Données administratives",
    "section": "",
    "text": "La baisse généralisée au niveau européen des taux de réponse1 (Luiten, Hox, and Leeuw 2020; Beck et al. 2022), qui accroît les coûts de collecte et rend plus difficile celle-ci sur certaines sous-populations, notamment les plus jeunes, nécessite de trouver des solutions pour répondre à la demande toujours accrue de statistique officielle.\nComme développé dans l’introduction, les données administratives sont des données de gestion produites par l’administration. Le processus de production statistique, où la collecte de donnée est construite de manière à mesurer le plus objectivement possible un phénomène cible, diffère du processus de production administratif. Pour cette dernière, la donnée est produite de sorte à faciliter la gestion. L’exploitation de celle-ci à des fins de production de statistique ou de recherche n’est pas le moteur de leur construction. L’exploitation de cette donnée est une affaire d’opportunité. Cette perte de contrôle du processus de production, qui fait que l’exploitant de la donnée se retrouve en aval de son processus de production, a tout de même des bénéfices : l’exhaustivité sur une population cible et la plus haute fréquence de ces données. Ceci explique qu’elles deviennent de plus en plus importantes dans la production de statistique officielle.\nCe chapitre revient sur le contexte d’utilisation des données administratives, leurs différences avec d’autres sources de données et les apports de celles-ci à la production de savoir statistique.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#nature-des-données",
    "href": "administratives.html#nature-des-données",
    "title": "Données administratives",
    "section": "Nature des données",
    "text": "Nature des données\nLes données statistiques traditionnelles (sondage ou recensement) sont produites pour informer. Cette finalité guide la conception de celles-ci, que ce soit au niveau du design, des concepts mesurés ou des retraitements post-collecte. La logique des données administratives est toute autre. Il s’agit de bases dont la finalité de construction est la gestion, c’est-à-dire l’enregistrement d’événements pour déclencher des actions (remboursement, paiement, etc.).\nCet aspect transactionnel de la donnée adminstrative change ainsi le processus de production. Ces bases sont susceptibles d’être mises à jour à plusieurs échéances. D’abord, leur structure n’est pas figée dans le temps. Selon les événements à enregistrer, la structure du fichier de données évoluera. Par exemple, un nouveau crédit d’impôt amènera à l’ajout d’une catégorie dans les déclarations fiscales ce qui se traduira par un changement du fichier de gestion. A ce premier facteur d’évolution peut s’ajouter des changements à plus brève échéance. La collecte de données administratives est un processus vivant. Les données sont généralement modifiables au cours d’un exercice de gestion voire au-delà. La donnée n’est stabilisée qu’après plusieurs cycles de gestion et sa continuité, au niveau de l’unité statistique, ne va pas de soi. Par exemple, une entreprise changeant d’identifiant SIREN pour une raison liée à un changement administratif (par exemple une fusion) ne sera identifiable dans différents millésimes de données administratives que si on est en mesure de relier les différents identifiants sous lequel elle apparaît.\nLes données administratives peuvent provenir de plusieurs origines. Elles sont en premier lieu issues de processus de gestion interne à l’administration concernée. Par exemple, pour être en mesure de gérer les remboursements liés au système de protection sociale français, l’assurance maladie collecte et enregistre de nombreuses informations sur les actes médicaux. Cette collecte est automatisée grâce à la carte vitale et au système d’information de l’assurance maladie ou passe par des déclarations papiers normalisées.\nUne seconde source d’origine des données administratives sont les déclarations administratives2 (Rivière 2018). Par exemple, les déclarations fiscales des ménages sont annuelles, avec un calendrier déterminé à l’avance (qui dépend du format, papier ou internet). Ce calendrier inclut d’ailleurs des possibilités allongées de retour sur la donnée fournie. L’obligation de certaines déclarations administratives se traduit par un pouvoir coercitif, pouvant prendre diverses formes, comme celle d’engager des poursuites. Ceci réduit le risque de non-déclaration ou de déclaration faussée mais ne l’annihile pas non plus. Selon la nature de la donnée, ces poursuites peuvent être pénales et les amendes non négligeables. L’existence de ces moyens coercitifs permet d’anticiper une information exhaustive sur la sous-population concernée par la donnée et honnête3.\nSi les données administratives sont devenues centrales dans le champ de la production statistique, c’est certes de par leur nature exhaustive mais aussi du fait de leur disponibilité à faible coût marginal. Les données administratives étant collectées et centralisées dans un système informatique à des fins de gestion, leur mise à disposition pour d’autres usages, s’il soulève certains enjeux sur lesquels nous reviendrons comme les questions de confidentialité, est marginalement peu coûteux. L’utilisation de ces données est ainsi une affaire d’opportunité: comme ces données sont disponibles et, sous un certain cadre juridique et technique, peuvent être ré-utilisables à d’autres fins, si elles fournissent une information de qualité, il est utile pour la production statistique de les exploiter.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#quelle-différence-avec-les-autres-sources-de-données-numériques",
    "href": "administratives.html#quelle-différence-avec-les-autres-sources-de-données-numériques",
    "title": "Données administratives",
    "section": "Quelle différence avec les autres sources de données numériques ?",
    "text": "Quelle différence avec les autres sources de données numériques ?\nCette propriété des données administratives qu’est le coût marginal faible rapproche celles-ci des traces numériques. Les entreprises du numérique ont pu centrer leur modèle économique autour de la collecte et de la valorisation de données justement parce que la collecte de nouvelles informations est d’un coût marginal nul. Il en va de même avec les données de gestion: la collecte d’une information supplémentaire sur une unité ou d’une unité supplémentaire n’est pas coûteuse. Dans le monde de la donnée numérique, il est certes nécessaire d’engager des investissements pour être en mesure de collecter des données de manière massive ou mettre à l’échelle un processus de collecte devenu plus ambitieux que le plan initial mais la donnée marginale ne coûte pas très cher puisque, comme nous allons le voir, la collecte de celle-ci est reportée sur un tiers.\nDès lors, la distinction entre données administratives et données numériques, telles qu’on peut formaliser le buzzword “big-data”, apparaît floue. La distinction correspond en premier lieu à l’origine des données. La donnée administrative est une donnée produite par la sphère administrative. Dans sa nature, son processus de production ne diffère pas de celui de la donnée privée. Dans les deux cas, un acteur effectue une activité (par exemple déclarer quelque chose) et cette activité va être transformée en information plus ou moins normalisée pour intégrer un système d’information et être stockée dans les serveurs d’un acteur centralisateur. Dans les deux cas, la personne dont la donnée a été collectée pourra éventuellement corriger l’information et/ou produire de nouvelles activités.\nLa différence entre données administratives et données privée est ainsi plutôt une différence de degré que de nature. Les données administratives sont généralement collectées à plus faible fréquence. Par exemple, le rythme de collecte de nombreuses données est annuel pour correspondre aux rythmes des campagnes fiscales. Mais certaines sources sont à des rythmes plus fréquents. Par exemple la DSN, sur laquelle nous reviendrons, est collectée à un rythme mensuel. Certaines données sont mêmes enregistrées à des rythmes qui n’ont pas grand chose à envier avec les traces numériques du big data. Par exemple, les systèmes d’information SIVIC et SIDEP, respectivement celui de suivi des entrées à l’hôpital des personnes malades du Covid et celui des tests, étaient mis à jour quotidiennement. De même, le système d’information de l’assurance maladie est mis à jour en continu en fonction des nouveaux événements qui appellent un remboursement. Bien qu’on n’associe pas forcément les données administratives avec une collecte en temps réel, il ne s’agit ainsi pas d’un critère les discriminant vis à vis des traces numériques.\nLa différence principale, peut-être, entre les données administratives et les données privées est que pour les premières, le champ est connu par le fait que celles-ci sont issues d’une collecte d’une population bien ciblée. Comme indiqué précédemment, comme la collecte de données administrative est souvent assortie de prérogatives légales, la population cible est généralement bien identifiée. Dans le monde de la donnée privée, comme c’est l’activité qui génère la donnée, le champ dépend de la base d’utilisateurs. Selon le type de données, celle-ci peut être plus ou moins large. Même parmi les données privées où les populations sont les plus larges, la couverture de la population n’est pas parfaite. Par exemple, les smartphones sont largement partagés dans la population. Néanmoins, cette technologie a un moindre taux de pénétration dans certaines population, notamment les plus agées. De plus, les opérateurs ont des parts de marché potentiellement hétérogènes (en fonction de critères d’âge ou territoriaux). Pour les opérateurs, il est difficile d’évaluer le champ de leur clientèle puisque cette information nécessite une enquête, et ainsi souffre de taux de réponse imparfaits ou de réponses incorrectes. Le champ est donc incertain puisqu’il n’est pas possible pour les producteurs de données privées d’apparier de manière automatique ces données avec les données administratives. Même s’il n’est pas toujours possible d’apparier des données administratives entre elles pour des raisons légales, le fait de fournir des informations communes dans différentes sources (état civil voire NIR) à un même acteur (l’Etat), facilite l’association entre les sources lorsque celle-ci est autorisée.\nLes 5V du big-data, initialement listés dans un rapport de MacKinsey, ne sont pas l’apanage des données privées. Il y a peut-être une différence de degré avec le big-data mais certainement pas de nature:\n\nVolume: certaines données administratives représentent des volumes conséquents. La DSN représente ainsi plus d’1To de données par an ;\nVélocité: certaines données, notamment celles de l’assurance maladie, sont à haute fréquence ;\nVariété: l’Etat collecte et exploite des données de natures très différentes ;\nVéracité: les données collectées par l’Etat ne sont pas à l’abri d’erreurs mais ces dernières, qu’elles soient volontaires ou non, pouvant être couteuses, les données sont normalement de meilleure qualité que celles auto-déclarées sans contrôle ex-post ;\nValeur: les données collectées par l’Etat sont d’une grande valeur même si elles ne sont pas monétisées. La valorisation par l’Etat n’est bien-sûr pas individuelle mais la collecte de données qui sont ensuite agrégées permet de créer une statistique publique, qui est un bien public, sans valeur de marché mais avec une valeur sociale.\n\nFinalement, il y a peu de différence entre les données administratives et certaines données privées disponibles sous forme structurée. Par exemple, les données générées par les paiements par cartes bancaires (données du GIE CB)  ne sont pas d’une nature très différente de données administratives. Comme celles-ci, il s’agit de données structurées issues d’un organisme centralisateur (le GIE CB) et mises à disposition consolidées pour la statistique publique.\n\nUne donnée plus sensible\nL’aspect exhaustif, sur un certain champ d’unités et d’informations de gestion, des données administratives peut les rendre, au niveau individuel, assez sensibles. La question de la confidentialité et de la sensibilité des données fournie à l’administration n’est pas nouvelle, il s’agit de la raison d’être du secret statistique défini dans l’une des lois les plus importantes de la statistique publique, à savoir la loi de 1951. Les informations fournies dans le cadre de certaines enquêtes peuvent être sensibles (informations sur le revenu ou le patrimoine, la santé, l’appartenance à certains groupes sociaux…). Cependant, l’aspect non exhaustif des enquêtes rend plus difficile la réidentification après la phase d’anonymisation. Avec les données administratives, l’information fournie peut parfois être moins précise mais le caractère exhaustif de celles-ci fait qu’en combinant plusieurs sources de données la réindentification est facilitée.\nLa question de la confidentialité est donc, au même titre que pour les données privées, devenu un enjeu dans le domaine des données administratives. Il est à noter que par rapport aux données privées cette question ne se pose pas au même niveau. Au niveau de la collecte de données, c’est-à-dire de la transformation d’une activité en donnée de gestion, là où l’utilisateur d’un service numérique bénéficie d’une relative liberté sur le choix des données collectées du fait du RGPD, ce n’est pas le cas pour l’utilisateur d’un service géré par l’Etat. Ce privilège de l’Etat s’appuie sur des décrets qui définissent des missions de service public. Cependant, au niveau des traitements mis en oeuvre, du stockage puis de la diffusion de la donnée, des conditions restrictives s’appliquent aussi à l’Etat. Exemple: SNDS.\n\n\n\n\n\n\nCadre légal\n\n\n\nCet encadré résume des éléments juridiques listés par Isnard (2018).\nLes membres du service statistique public (SSP) bénéficient d’une disposition importante qui facilite énormément le travail du statisticien. Ce sont les seuls organismes à pouvoir mettre en œuvre l’article 7bis de la loi de 1951. Cet article leur permet de se faire communiquer, à des fins d’élaboration de statistiques publiques, tout fichier de gestion d’une administration ou d’une personne privée gérant un service public, dès lors que le Conseil national de l’information statistique a été consulté et que la demande émane du ministre chargé de l’économie (en pratique du directeur général de l’Insee). Cette mesure, insérée dans la loi du 7 juin 1951 par la loi du 26 décembre 1986, a permis une exploitation large des données administratives et ainsi un allègement de la charge de réponse aux enquêtes.\nL’utilisation de déclarations ou de sources administratives à des fins statistiques est préconisée par le code de bonnes pratiques de la statistique européenne dans le but d’alléger la charge statistique des déclarants. En France, ceci est rendu possible par la loi de 1951 relative à l’obligation, à la coordination et au secret en matière de statistique et a été réaffirmé récemment par la loi pour une République numérique (2016).",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#processus-de-production",
    "href": "administratives.html#processus-de-production",
    "title": "Données administratives",
    "section": "Processus de production",
    "text": "Processus de production\nLe processus de production de la donnée administrative est différent de celui de la donnée traditionnelle. La différence principale est la place centrale d’une autorité gestionnaire, qui centralise la donnée, dans le modèle de production des données administratives (Rivière 2018). Cet acteur doit être distingué de l’administration qui exploite le flux, que ce soit à des fins de gestion ou d’exploitation statistique.\nLa Table 1 donne quelques exemples de plateformes centralisatrices. Ces dernières ne se contentent pas de centraliser ou mettre à disposition la donnée, elles ont aussi en charge la normalisation de celle-ci à partir de systèmes d’informations divers. La normalisation est un enjeu majeur car elle seule permet l’exploitation des données: la collecte étant en général réalisée automatiquement via des auto-déclarations, les plateformes centralisatrices récupèrent des informations aux contenus hétérogènes.\n\n\n\nTable 1: Exemples d’autorités centralisatrices (Rivière 2018)\n\n\n\n\n\nDonnée\nAutorité centralisatrice\n\n\n\n\nDSN\nGip-MDS\n\n\nDonnées hospitalières\nATIH-10\n\n\nSI gestion des eaux\nSANDRE12\n\n\n\n\n\n\nLa Figure 1 résume la place du GIP-MDS dans le processus de production de la DSN:\n\n\n\n\n\n\nFigure 1: Schéma de la place du GIP-MDS dans la production de la DSN. Source: Humbert-Bottin (2018).",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#usage-de-la-donnée-administrative",
    "href": "administratives.html#usage-de-la-donnée-administrative",
    "title": "Données administratives",
    "section": "Usage de la donnée administrative",
    "text": "Usage de la donnée administrative\nL’usage de ces données est de deux nature: l’usage à des fins de gestion (la finalité pour laquelle elles sont construites) et l’usage à des fins d’analyse (la finalité fortuite). Ces peut aller au-delà de l’administration concernée. Par exemple, la déclaration sociale nominative n’est pas utilisée exclusivement par le Ministère du Travail mais aussi par la DGFIP, les institutions de prévoyance, les organismes de retraite, l’Acoss, pour leurs propres usages de gestion ; les données de SIRENE servent de référence, de preuve pour les entreprises, elles sont utilisées par les chambres de commerce et d’industrie ou par les greffes des tribunaux de commerce (Rivière 2018).\n\nUn usage accru pour apparier des sources\nCertaines sources administratives ont un rôle particulier dans le processus de production statistique car elle permettent d’identifier des unités statistiques dans plusieurs sources. Le Répertoire national d’identification des personnes physiques (RNIPP), le répertoire Sirene pour les entreprises ou encore XXX pour les logements, sont des sources qui permettent de relier des unités statistiques entre plusieurs sources. On parle d’appariements pour désigner ce type d’opérations où plusieurs sources de données sont associées grâce à une information commune. Cela peut se faire sur la base d’une information exacte, en général un identifiant unique fourni par un des référentiels, ou de manière floue à partir d’informations non uniques mais qui, combinées, peuvent aider à identifier une unité (nom, raison sociale d’une entreprise, adresse, etc.).\nCes répertoires administratifs sont ainsi des sources devenues centrales dans le processus de production statistique. Ils permettent d’enrichir d’autres sources administratives, ou des enquêtes, d’informations administratives. Ces dernières peuvent ainsi permettre d’alléger certains questionnaires d’enquêtes ou de concentrer ceux-ci sur des informations qui ne sont pas disponibles dans les sources administratives.\n\n\n\n\n\n\nLe CSNS\n\n\n\nUn enjeu fort existe autour de la production d’un code statistique non signifiant (CSNS) pour les besoins de mise en œuvre de traitements à finalité de statistique publique impliquant le numéro de sécurité sociale (NIR) ou des traits d’identité, en particulier les appariements au sein du Service statistique publique. La version finale est prévue pour la fin de l’année 2022.\n\n\n\n\nUn changement de la place de l’analyste de la donnée\nCette situation change la place du statisticien dans le processus de production de la statistique officielle. Il convient de transformer en aval les données pour répondre aux besoins de l’analyse statistique. Cela implique un contrôle qualité ex-post, éventuellement un travail de reconstitution et de consolidation.\nCette situation change également la place des chercheurs dans le processus de production de la donnée. Comme le statisticien, le chercheur n’est plus associé à l’amont de la production de données. Cependant celui-ci est, généralement, encore plus en aval que le statisticien public. Il reçoit les données généralement consolidées, anonymisées et éventuellement appariées entre différentes sources. A cet égard, les données administratives scandinaves sont parmi les données les plus utilisées par les chercheurs sur le marché du travail car elles constituent une source depuis longtemps centralisée et mise à disposition de manière anonymisée.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#en-conclusion-quels-avantages-et-inconvénients",
    "href": "administratives.html#en-conclusion-quels-avantages-et-inconvénients",
    "title": "Données administratives",
    "section": "En conclusion, quels avantages et inconvénients ?",
    "text": "En conclusion, quels avantages et inconvénients ?\nLa production et l’usage de données administratives se sont généralisés. La numérisation croissante de l’économie est amené à confirmer cette tendance. L’utilisation par la statistique publique de données privées, sous leur forme structurée, n’est qu’un prolongement de cette dynamique. Ces dernières permettent d’enrichir l’information dont dispose l’administration avec des informations collectées dans le cadre d’activités économiques détachées de l’administration.\nLes avantages des données administratives sont multiples. En premier lieu, la collecte automatisée de celle-ci, associée à un pouvoir public coercitif, permet d’atteindre sur un champ d’unités statistiques bien définies (usuellement par le biais d’un décret), une forme d’exhaustivité. Cette dernière permet de construire des statistiques plus fines. Si aujourd’hui il est possible pour des chercheurs de zoomer sur le très haut de la distribution de revenu (voir les travaux de Piketty), c’est parce que l’aspect exhaustif des données permet d’avoir des groupements suffisamment nombreux pour assurer la confidentialité de ces groupes.\nUne fois payé le coût d’investissement pour automatiser la production statistique à partir de données de gestion, les données administratives ouvrent la voie à la production à plus haute fréquence de statistiques officielles. La production annuelle ou infra-annuelle de statistiques n’est possible qu’avec un nombre restreint d’enquêtes - dans la plupart des cas, les résultats d’enquêtes sont connus avec du retard. La publication quotidienne par le service statistique du Ministère de la Santé (la DREES) et Santé Publique France d’indicateurs sur la pandémie est un bon exemple de l’intérêt de ces données. Ces dernières ont permis un suivi très fin par la puissance publique mais aussi par la société civile des évolutions de l’épidémie.\nUn autre avantage des données administratives est que les informations qui sont disponibles dans celles-ci sont certes diverses (nous reviendrons sur cela dans le prochain chapitre à travers quelques exemples) mais elles sont, sur certains champs, très fiables. Elles souffrent normalement moins de biais de réponses même si elles n’en sont pas exemptées (les déclarations erronées à l’administration fiscale existent, qu’il s’agisse d’un comportement volontaire ou non).\nCes données soulèvent de nouveaux défis pour la statistique publique. En premier lieu, elles amènent à redéfinir le rôle du métier dans le processus de production de la donnée. Ceci est vrai dans le monde de la donnée administrative mais aussi dans le domaine des données privées. Comme l’utilisateur de données ne contrôle pas le champ ou la définition du concept mesuré, c’est le concentrateur, cet acteur dont l’activité est spécialisée autour de la collecte et de la gestion du flux, qui intervient à cet étape. Il peut ainsi être amené à faire évoluer le champ, la définition du phénomène mesuré ou encore le formulaire sans que l’analyste de données n’ait son mot à dire. Pour reprendre l’exemple des données quotidiennes, l’apparition de variants à plusieurs reprises a amené à des évolutions, parfois sans préavis, du type de donnée collectée, enregistrée. Les données déjà collectées n’ayant pas vocation à intégrer ces informations qui n’avaient pas de sens au moment de la collecte, c’est à l’analyste de données de faire des choix méthodologiques pour reconstruire une série cohérente. Le statisticien, parce qu’il intervient plus en aval, change donc de rôle. Les données administratives n’étant pas construites pour mesurer un phénomène qui a du sens pour le statisticien public (ou l’analyste de la donnée privée), c’est à lui de reconstruire à partir de l’information de gestion la réalité statistique derrière (Salgado and Oancea 2020). Le travail de l’analyse de données va au donc au delà de la simple reconstruction de variable, ou du contrôle qualité, il est également nécessaire de réfléchir au concept mesuré pour ne pas construire d’“artefact”, au sens de Bourdieu. Cette problématique se pose, de la même manière, à la recherche et à l’exploitation de données privées.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#footnotes",
    "href": "administratives.html#footnotes",
    "title": "Données administratives",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPar exemple, le taux de réponse a baissé pour l’enquête en face-à-face Cadre de vie et sécurité de 72 % à 66 % entre 2012 et 2021. En ce qui concerne SRCV (Statistiques sur les ressources et les conditions de vie), le taux de réponse est passé de 85 % à 80 % entre 2010 et 2019. Des événements ponctuels comme la crise du Covid-19 peuvent de plus avoir des effets très forts sur le taux de réponse. Par exemple, en 2020, à la date du 23 avril, le taux de réponse à l’enquête sur la production industrielle en mars, qui sert d’indicateur avancé de l’activité économique, était inférieur d’environ 20 points de pourcentage à ce qui est observé lors d’un mois habituel (voir blog de l’Insee).↩︎\nObligation est faite à un certain nombre d’entités (individus, entreprises, organismes publics) de fournir des informations respectant une certaine forme, selon certaines modalités (internet, papier) et temporalités.↩︎\nCertaines enquêtes, reconnues d’utilité publique, comme l’enquête emploi, le recensement ou encore l’enquête ressources et conditions de vie (SRCV), sont obligatoires. Bien que cela permette d’avoir des taux de réponse élevés, cela n’assure pas un taux de 100%. Comme cela a été évoqué précédemment, le taux de réponse de SRCV est par exemple passé de 85 % à 80 % entre 2010 et 2019. Pour plus d’informations sur les enquêtes obligatoires, voir la description du CNIS et la liste des enquêtes concernées parmi les enquêtes auprès des particuliers.↩︎",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Données émergentes (ENSAI)",
    "section": "",
    "text": "Ce site web centralise les contenus du cours de Master de l’ENSAI sur les données émergentes\n\n\n\n\n\n\nImportant\n\n\n\nLe contenu de ce site est partiellement redondant avec le contenu des slides. Ces dernières sont plus récentes, elles sont donc à privilégier."
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "L’histoire de la statistique est une suite d’évolutions de la discipline où les données émergentes un jour deviennent le lendemain traditionnelles. Le XIXe siècle, qui est celui où la statistique s’est constituée en temps que discipline autonome et s’est dotée d’une partie des concepts qui en font aujourd’hui les fondements, est ainsi une période où de nombreuses données ont émergé et ont pu entraîner des révolutions scientifiques. Parmi celles-ci, la construction de la loi normale, qui constitue aujourd’hui l’objet central de la statistique, correspond au besoin de construire de nouveaux concepts et outils afin de structurer dans une théorie commune un ensemble de nouvelles données. La manière dont Gauss a collecté et synthétisé un ensemble d’observations astronomiques a ainsi permis de construire la méthode des moindres carrés et le concept de loi normale, appréhendé à partir des erreurs d’observations.\nL’accès à des recensements par des universitaires à la fin du XIXe siècle a été un élément moteur de la constitution de la sociologie en temps que discipline autonome. Les registres de décès ont ainsi permis à Durkheim de participer aux débats sociologiques sur le suicide et de proposer une interprétation sociologique de ses causes à rebours des approches psychologisantes qui étaient fréquentes à l’époque. Avant Durkheim, l’usage novateur des monographies a permis de dessiner les prémisses de la sociologie en temps que discipline autonome. Les avancées de la statistique au cours du XXe siècle sont intimement liées à la génération des enquêtes ou des sondages.\nLes notions d’échantillonnage, de représentativité, ou encore de marges d’erreur, qui sont au coeur de la statistique moderne, ont permis de rendre traditionnel ce nouveau mode de collecte. Ces enquêtes sont aujourd’hui encore très utilisées dans la production statistique moderne ou dans les études économiques et sociologiques.\nLa prolifération de traces numériques, parce qu’elle a créé de nouvelles opportunités pour la puissance publique ou pour des acteurs privés de valoriser des données, est un moteur d’évolution de la statistique. L’émergence du concept de data-science, qu’on le considère comme un ensemble de pratiques ou uniquement comme un buzzword, est intimement lié à la multiplication des traces numériques. Les nouvelles disciplines ou méthodes qui se sont développées récemment sont intrinsèquement liées aux données émergentes. La vitesse à laquelle se développent les innovations dans le domaine de la data-science est d’une ampleur inédite du fait de la multiplicité des données collectées et des acteurs impliqués. IBM estimait en effet que 2.5 quintillions d’octets de données étaient générés chaque jour il y a environ 10 ans. Dans un ouvrage sur l’histoire de la statistique, Hacking (1990) parle déjà en 1990 du début d’une “avalanche de chiffres”.",
    "crumbs": [
      "Home",
      "Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#les-données-émergentes-dans-le-temps-long",
    "href": "introduction.html#les-données-émergentes-dans-le-temps-long",
    "title": "Introduction",
    "section": "",
    "text": "L’histoire de la statistique est une suite d’évolutions de la discipline où les données émergentes un jour deviennent le lendemain traditionnelles. Le XIXe siècle, qui est celui où la statistique s’est constituée en temps que discipline autonome et s’est dotée d’une partie des concepts qui en font aujourd’hui les fondements, est ainsi une période où de nombreuses données ont émergé et ont pu entraîner des révolutions scientifiques. Parmi celles-ci, la construction de la loi normale, qui constitue aujourd’hui l’objet central de la statistique, correspond au besoin de construire de nouveaux concepts et outils afin de structurer dans une théorie commune un ensemble de nouvelles données. La manière dont Gauss a collecté et synthétisé un ensemble d’observations astronomiques a ainsi permis de construire la méthode des moindres carrés et le concept de loi normale, appréhendé à partir des erreurs d’observations.\nL’accès à des recensements par des universitaires à la fin du XIXe siècle a été un élément moteur de la constitution de la sociologie en temps que discipline autonome. Les registres de décès ont ainsi permis à Durkheim de participer aux débats sociologiques sur le suicide et de proposer une interprétation sociologique de ses causes à rebours des approches psychologisantes qui étaient fréquentes à l’époque. Avant Durkheim, l’usage novateur des monographies a permis de dessiner les prémisses de la sociologie en temps que discipline autonome. Les avancées de la statistique au cours du XXe siècle sont intimement liées à la génération des enquêtes ou des sondages.\nLes notions d’échantillonnage, de représentativité, ou encore de marges d’erreur, qui sont au coeur de la statistique moderne, ont permis de rendre traditionnel ce nouveau mode de collecte. Ces enquêtes sont aujourd’hui encore très utilisées dans la production statistique moderne ou dans les études économiques et sociologiques.\nLa prolifération de traces numériques, parce qu’elle a créé de nouvelles opportunités pour la puissance publique ou pour des acteurs privés de valoriser des données, est un moteur d’évolution de la statistique. L’émergence du concept de data-science, qu’on le considère comme un ensemble de pratiques ou uniquement comme un buzzword, est intimement lié à la multiplication des traces numériques. Les nouvelles disciplines ou méthodes qui se sont développées récemment sont intrinsèquement liées aux données émergentes. La vitesse à laquelle se développent les innovations dans le domaine de la data-science est d’une ampleur inédite du fait de la multiplicité des données collectées et des acteurs impliqués. IBM estimait en effet que 2.5 quintillions d’octets de données étaient générés chaque jour il y a environ 10 ans. Dans un ouvrage sur l’histoire de la statistique, Hacking (1990) parle déjà en 1990 du début d’une “avalanche de chiffres”.",
    "crumbs": [
      "Home",
      "Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#la-production-renouvelée-de-données-de-la-puissance-publique",
    "href": "introduction.html#la-production-renouvelée-de-données-de-la-puissance-publique",
    "title": "Introduction",
    "section": "La production renouvelée de données de la puissance publique",
    "text": "La production renouvelée de données de la puissance publique\nLa puissance publique est une productrice historique de données. Les registres administratifs ou comptables sont une source de données très appréciée des historiens. Si elles n’atteignent pas les volumétries actuelles, ces sources sont néanmoins les ancêtres de nos données administratives actuelles. Les recensements de population sont également une des productions historiques de données. Le comptage de la population et des impôts fait partie intégrante du processus de constitution de la puissance publique centralisatrice (Desrosières 2010). Curieusement, la tablette Kish de l’empire sumérien (environ 3500 av. J.-C.), l’un des plus anciens exemples d’écriture humaine, semble être un document administratif destiné à des fins statistiques.\nLa statistique publique, si elle est aujourd’hui entendue beaucoup plus largement que par le passé, et qu’elle dispose d’une indépendance vis-à-vis d’autres branches de l’Etat, c’est parce qu’elle est un élément essentiel pour pour permettre le bon fonctionnement de l’économie et de la démocratie. Le slogan de l’Insee, “mesurer pour comprendre”, correspond bien à cette idée. Les statistiques officielles essaient d’objectiver les phénomènes socio-économiques par la collecte de données et la construction de concepts cohérents avec le phénomène mesuré.\nLes enquêtes sont historiquement une source privilégiée puisque la conception de celles-ci, en amont de la collecte et des retraitements post-collecte, est justement effectuée en fonction des réutilisations futures. Les questions sont ainsi conçues pour s’approcher au plus près des phénomènes qu’on désire quantifier et l’échantillonnage puis les redressements post-collecte permettront de contrôler la population sur laquelle portent les statistiques construites. L’inconvénient est que cette production nécessite des moyens et un temps conséquents (en amont de la collecte, lors de celle-ci puis à l’issue de celle-ci). De plus, les enquêtes ne sont pas à l’abri d’erreurs dans la collecte, qu’il s’agisse d’omissions ou réponses erronnées, qu’elles soient volontaires ou non. A ces problèmes s’ajoute la baisse historique des taux de réponse (Rivière 2018).\nL’Etat n’accumule pas uniquement de la connaissance sur sa population par le biais d’enquête. Les registres des impôts, de l’assurance maladie, etc. sont des sources de gestion par lesquelles chaque individu communique un certain nombre d’informations sur lui. On parle de données administratives pour regrouper cet ensemble de sources qui sont produites par la puissance publique et dont la collecte répond à des enjeux de gestion mais pas à des besoins de statistique publique. La définition qu’en donnait Desrosières (2004), résume bien ceci: “une source administrative est issue d’une institution dont la finalité n’est pas de produire une telle information, mais dont les activités de gestion impliquent la tenue, selon des règles générales, de fichiers ou de registres individuels, dont l’agrégation n’est qu’un sous-produit”. Les besoins de la statistique publique ne sont donc pas à la source de la collecte mais on peut utiliser celle-ci comme opportunité pour enrichir la connaissance de phénomènes socio-économiques (Connelly et al., Einav et al.). Certaines informations disponibles dans ces données sont très génériques et communes à de nombreuses bases de gestion (l’état civil notamment), ce qui peut faciliter l’association entre elles, alors que d’autres sont propres à chaque source. Outre la possibilité de disposer d’informations sur une population plus importante, la différence principale entre ces sources de données, historiquement collectées par papier et de plus en plus par collecte numérique, et les enquêtes est que les premières ne sont pas conçues initialement à des fins de statistique donc le statisticien n’en contrôle pas la conceptualisation et la collecte. Néanmoins, ces sources peuvent fournir des informations très précieuses à la statistique publique. Si on est en mesure de relier celles-ci à une enquête, il devient possible d’enrichir ou de corriger certaines informations collectées si les concepts présents dans l’enquête correspondent à ceux de la source administrative.\nLes données administratives deviennent ainsi de plus en plus fréquemment mobilisées dans la production officielle de statistiques ou dans les études économiques. La numérisation de l’économie et des démarches administratives, parce qu’elle a facilité la constitution de bases et l’association entre celles-ci, a accéléré le mouvement de constitution de grands répertoires administratifs. Parmi les principaux exploités par la statistique publique : la DSN, Fidéli, le SNDS… La construction de ces sources, car celles-ci nécessitent pour leur usage à des fins statistiques une reconstruction, implique également un changement des institutions collectant la donnée. Ce n’est plus l’Insee qui collecte directement la donnée (que ce soit à son compte ou pour le compte d’autres institutions comme les services statistiques ministériels) mais des ministères. Ces derniers peuvent, ou non, exploiter ces données à leur propre compte mais aussi mettre à disposition la donnée brute ou une version retravaillée de celle-ci. Par exemple, la Direction Générale des Finances Publiques (DGFiP) est, par son rôle de collecte des impôts, un acteur central dans la constitution de bases sur les revenus qui permettent de produire de nombreuses statistiques socio-économiques. De même, la Caisse Nationale d’Assurance Maladie (CNAM) est, par son rôle de gestionnaire du système français de sécurité sociale, un élément central dans la constitution du Système national des données de santé (SNDS).\nLa multiplication de traces numériques collectées non plus seulement par les acteurs publics mais aussi par des acteurs privés a permis de produire de nouvelles sources de données, à une fréquence ou à une échelle inédite. A ce premier facteur qu’est l’intensification de la production de statistique, s’ajoute la demande croissante de la population et des décideurs publics pour des statistiques plus détaillées et disponibles plus rapidement. Cela a ainsi amené à une intensification de la disponibilité de statistiques, dont la production n’est plus le monopole de la puissance publique. Afin de pouvoir produire ces statistiques, tout en satisfaisant aux critères usuels de qualité sur lesquels nous reviendrons, la statistique publique se doit d’innover dans la collecte traditionnelle, l’utilisation de nouvelles statistiques et concepts ou dans les processus de valorisation de données auquel elle accédait déjà. Parmi ces trois facteurs, nous allons principalement nous concentrer sur le deuxième, c’est-à-dire la valorisation de nouvelles sources de données, qu’il s’agisse de données produites par l’administration ou de données privées. Le premier point - l’innovation dans les méthodes de collecte traditionnelles - renvoie, entre autres, à la question du multimode. Enfin, en ce qui concerne le troisième élément - la rénovation des processus de production - il y a des éléments connexes à notre problématique (certaines méthodes sont intrinsèquement liées à de nouvelles sources) mais aussi certains qui le dépassent. Nous n’allons donc pas nous concentrer sur ceux-ci bien qu’il se peut que nous évoquions à plusieurs reprises ces enjeux.",
    "crumbs": [
      "Home",
      "Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "textes.html",
    "href": "textes.html",
    "title": "Données textuelles et non structurées",
    "section": "",
    "text": "Les données textuelles sont aujourd’hui parmi les types de données les plus prometteurs pour la statistique publique et l’un des champs les plus actifs de la recherche en data science. Pour cause, de plus en plus de services existent sur le web qui conduisent à la collecte de données textuelles. En outre, des nouvelles méthodes pour collecter et traiter ces traces numériques particulières ont été développées dans les dernières années.\nUne partie des méthodes d’analyse qui appartiennent à la palette des compétences des data scientists spécialistes du traitement de données textuelles sont en réalité assez anciennes. Par exemple, la distance de Levensthein a été proposée pour la première fois en 1965, l’ancêtre des réseaux de neurone actuels est le perceptron qui date de 1957, etc.1 Néanmoins, le fait que certaines entreprises du net basent leur business model sur le traitement et la valorisation de la donnée textuelle, notamment Google, Facebook et Twitter, a amené à renouveler le domaine.\nLa statistique publique s’appuie également sur la collecte et le traitement de données textuelles. Les collectes de données officielles ne demandent pas exclusivement d’informations sous le forme de texte. Les premières informations demandées sont généralement un état civil, une adresse, etc. C’est ensuite, en fonction du thème de l’enquête, que d’autres informations textuelles seront collectées: un nom d’entreprise, un titre de profession, etc. Les données administratives elles-aussi comportent souvent des informations textuelles. Ces données défient l’analyse statistique car cette dernière, qui vise à détecter des grandes structures à partir d’observations multiples, doit s’adapter à la différence des données textuelles: le langage est un champ où certaines des notions usuelles de la statistique (distance, similarité notamment) doivent être revues.\nCe chapitre propose un panorama très incomplet de l’apport des données non structurées, principalement textuelles, pour la statistique et l’analyse de données. Nous évoquerons plusieurs sources ou méthodes de collecte. Nous ferons quelques détours par des exemples pour aller plus loin.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#présentation",
    "href": "textes.html#présentation",
    "title": "Données textuelles et non structurées",
    "section": "Présentation",
    "text": "Présentation\nLe webscraping est une méthode de collecte de données qui repose sur le moissonnage d’objets de grande dimension (des pages web) afin d’en extraire des informations ponctuelles (du texte, des nombres…). Elle désigne les techniques d’extraction du contenu des sites Internet. C’est une pratique très utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n’existant pas forcément sous la forme de fichiers exportables.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#enjeux-pour-la-statistique-publique",
    "href": "textes.html#enjeux-pour-la-statistique-publique",
    "title": "Données textuelles et non structurées",
    "section": "Enjeux pour la statistique publique",
    "text": "Enjeux pour la statistique publique\nLe webscraping présente un certain nombre d’enjeux en termes de légalité, qui ne seront pas enseignés dans ce cours. En particulier, la Commission nationale de l’informatique et des libertés (CNIL) a publié en 2020 de nouvelles directives sur le webscraping reprécisant qu’aucune donnée ne peut être réutilisée à l’insu de la personne à laquelle elle appartient.\nLe webscraping est un domaine où la reproductibilité est compliquée à mettre en oeuvre. Une page web évolue régulièrement et d’une page web à l’autre, la structure peut être très différente ce qui rend certains codes difficilement généralisables. Par conséquent, la meilleure manière d’avoir un programme fonctionnel est de comprendre la structure d’une page web et dissocier les éléments exportables à d’autres cas d’usages des requêtes ad hoc.\nUn code qui fonctionne aujourd’hui peut ainsi très bien ne plus fonctionner au bout de quelques semaines. Il apparaît préférable de privilégier les API qui sont un accès en apparence plus compliqué mais en fait plus fiable à moyen terme. Cette difficulté à construire une extraction de données pérenne par webscraping une illustration du principe “there is no free lunch”. La donnée est au cœur du business model de nombreux acteurs, il est donc logique qu’ils essaient de restreindre la moisson de leurs données.\nLes APIs sont un mode d’accès de plus en plus généralisé à des données. Cela permet un lien direct entre fournisseurs et utilisateurs de données, un peu sous la forme d’un contrat. Si les données sont ouvertes avec restrictions, on utilise des clés d’authentification. Avec les API, on structure sa demande de données sous forme de requête paramétrée (source désirée, nombre de lignes, champs…) et le fournisseur de données y répond, généralement sous la forme d’un résultat au format JSON. Python et JavaScript sont deux outils très populaires pour récupérer de la donnée selon cette méthode. Pour plus de détails, vous pouvez explorer le chapitre sur les API dans le cours de Python de l’ENSAE.\nOn n’est pas à l’abri de mauvaises surprises avec les APIs (indisponibilité, limite atteinte de requêtes…) mais cela permet un lien plus direct avec la dernière donnée publiée par un producteur. L’avantage de l’API est qu’il s’agit d’un service du fournisseur de données, qui en tant que service va amener un producteur à essayer de répondre à une demande dans la mesure du possible. Le webscraping étant un mode d’accès à la donnée plus opportuniste, où le réel objectif du producteur de données n’est pas de fournir de la donnée mais une page web, il n’y a aucune garantie de service ou de continuité.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#exemples-dans-la-statistique-publique",
    "href": "textes.html#exemples-dans-la-statistique-publique",
    "title": "Données textuelles et non structurées",
    "section": "Exemples dans la statistique publique",
    "text": "Exemples dans la statistique publique",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#implémentations",
    "href": "textes.html#implémentations",
    "title": "Données textuelles et non structurées",
    "section": "Implémentations",
    "text": "Implémentations\nPython est le langage le plus utilisé par les scrappers. BeautifulSoup sera suffisant quand vous voudrez travailler sur des pages HTML statiques. Dès que les informations que vous recherchez sont générées via l’exécution de scripts Javascript, il vous faudra passer par des outils comme Selenium. De même, si vous ne connaissez pas l’URL, il faudra passer par un framework comme Scrapy, qui passe facilement d’une page à une autre (“crawl”). Scrapy est plus complexe à manipuler que BeautifulSoup : si vous voulez plus de détails, rendez-vous sur la page du tutoriel Scrapy. Pour plus de détails, voir le TP sur le webscraping en 2e année de l’ENSAE.\nLes utilisateurs de R privilégieront httr and rvest qui sont les packages les plus utilisés. Il est intéressant d’accorder de l’attention à polite. Ce package vise à récupérer des données en suivant les recommandations de bonnes pratiques sur le sujet, notamment de respecter les instructions dans robots.txt (“The three pillars of a polite session are seeking permission, taking slowly and never asking twice”).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#du-bag-of-words-aux-modèles-de-langage",
    "href": "textes.html#du-bag-of-words-aux-modèles-de-langage",
    "title": "Données textuelles et non structurées",
    "section": "Du bag of words aux modèles de langage",
    "text": "Du bag of words aux modèles de langage\nL’objectif du Natural Langage Processing (NLP) est de transformer une information de très haute dimension (une langue est un objet éminemment complexe) en information à dimension plus limitée qui peut être exploitée par un ordinateur.\nLa première approche pour entrer dans l’analyse d’un texte est généralement l’approche bag of words ou topic modeling. Dans la première, il s’agit de formaliser un texte sous forme d’un ensemble de mots où on va piocher plus ou moins fréquemment dans un sac de mots possibles. Dans la seconde, il s’agit de modéliser le processus de choix de mots en deux étapes (modèle de mélange): d’abord un choix de thème puis, au sein de ce thème, un choix de mots plus ou moins fréquents selon le thème.\nDans ces deux approches, l’objet central est la matrice document-terme. Elle formalise les fréquences d’occurrence de mots dans des textes ou des thèmes. Néanmoins, il s’agit d’une matrice très creuse: même un texte au vocabulaire très riche n’explore qu’une petite partie du dictionnaire des mots possibles.\nL’idée derrière les embeddings est de proposer une information plus condensée qui permet néanmoins de capturer les grandes structures d’un texte. Il s’agit par exemple de résumer l’ensemble d’un corpus en un nombre relativement restreint de dimensions. Ces dimensions ne sont pas prédéterminées mais plutôt inférées par un modèle qui essaie de trouver la meilleure partition des dimensions pour rapprocher les termes équivalents. Chacune de ces dimensions va représenter un facteur latent, c’est à dire une variable inobservée, de la même manière que les composantes principales produites par une ACP. Techniquement, au lieu de représenter les documents par des vecteurs sparse de très grande dimension (la taille du vocabulaire) comme on l’a fait jusqu’à présent, on va les représenter par des vecteurs denses (continus) de dimension réduite (en général, autour de 100-300).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#intérêt-des-modèles-de-langage",
    "href": "textes.html#intérêt-des-modèles-de-langage",
    "title": "Données textuelles et non structurées",
    "section": "Intérêt des modèles de langage",
    "text": "Intérêt des modèles de langage\nPar exemple, un humain sait qu’un document contenant le mot “Roi” et un autre document contenant le mot “Reine” ont beaucoup de chance d’aborder des sujets semblables.\n\n\n\n\n\n\nFigure 1: Schéma illustratif de word2vec.\n\n\n\nPourtant, une vectorisation de type comptage ou TF-IDF ne permet pas de saisir cette similarité : le calcul d’une mesure de similarité (norme euclidienne ou similarité cosinus) entre les deux vecteurs donnera une valeur très faible, puisque les mots utilisés sont différents.\nA l’inverse, un modèle word2vec (voir Figure 1) bien entraîné va capter qu’il existe un facteur latent de type “royauté”, et la similarité entre les vecteurs associés aux deux mots sera forte.\nLa magie va même plus loin : le modèle captera aussi qu’il existe un facteur latent de type “genre”, et va permettre de construire un espace sémantique dans lequel les relations arithmétiques entre vecteurs ont du sens ; par exemple (voir Figure 2) :\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\nChaque mot est représenté par un vecteur de taille fixe (comprenant \\(n\\) nombres), de façon à ce que deux mots dont le sens est proche possèdent des représentations numériques proches. Ainsi les mots « chat » et « chaton » devraient avoir des vecteurs de plongement assez similaires, eux-mêmes également assez proches de celui du mot « chien » et plus éloignés de la représentation du mot « maison ».\nComment ces modèles sont-ils entraînés ? Via une tâche de prédiction résolue par un réseau de neurones simple. L’idée fondamentale est que la signification d’un mot se comprend en regardant les mots qui apparaissent fréquemment dans son voisinage. Pour un mot donné, on va donc essayer de prédire les mots qui apparaissent dans une fenêtre autour du mot cible.\nEn répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié, on obtient finalement des embeddings pour chaque mot du vocabulaire, qui présentent les propriétés discutées précédemment.\n\n\n\n\n\n\nFigure 2: Illustration des word embeddings.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#les-modèles-de-langage-aujourdhui",
    "href": "textes.html#les-modèles-de-langage-aujourdhui",
    "title": "Données textuelles et non structurées",
    "section": "Les modèles de langage aujourd’hui",
    "text": "Les modèles de langage aujourd’hui\nLa méthode de construction d’un plongement lexical présentée ci-dessus est celle de l’algorithme Word2Vec. Il s’agit d’un modèle open-source développé par une équipe de Google en 2013. Word2Vec a été le pionnier en termes de modèles de plongement lexical.\nLe modèle GloVe constitue un autre exemple (Pennington, Socher, and Manning 2014). Développé en 2014 à Stanford, ce modèle ne repose pas sur des réseaux de neurones mais sur la construction d’une grande matrice de co-occurrences de mots. Pour chaque mot, il s’agit de calculer les fréquences d’apparition des autres mots dans une fenêtre de taille fixe autour de lui. La matrice de co-occurrences obtenue est ensuite factorisée par une décomposition en valeurs singulières. Il est également possible de produire des plongements de mots à partir du modèle de langage BERT, développé par Google en 2019, dont il existe des déclinaisons dans différentes langues, notamment en Français (les modèles CamemBERT ou FlauBERT).\nEnfin, le modèle FastText, développé en 2016 par une équipe de Facebook, fonctionne de façon similaire à Word2Vec mais se distingue particulièrement sur deux points :\n\nEn plus des mots eux-mêmes, le modèle apprend des représentations pour les n-grams de caractères (sous-séquences de caractères de taille \\(n\\), par exemple « tar », « art » et « rte » sont les trigrammes du mot « tarte »), ce qui le rend notamment robuste aux variations d’orthographe ;\nLe modèle a été optimisé pour que son entraînement soit particulièrement rapide.\n\nLe modèle GPT-3 (acronyme de Generative Pre-trained Transformer 3) a aujourd’hui le vent en poupe. Celui-ci a été développé par la société OpenAI et rendu public en 2020 (Brown et al. 2020). GPT-3 est le plus gros modèle de langage jamais entraîné avec 175 milliards de paramètres. Il sert de brique de base à plusieurs applications utilisant l’analyse textuelle pour synthétiser, à partir d’une instruction, des éléments importants et proposer un texte cohérent. Github Copilot l’utilise pour transformer une instruction en proposition de code, à partir d’un grand corpus de code open source. Algolia l’utilise pour transformer une instruction en mots clés de recherche afin d’améliorer la pertinence des résultats.\nEn ce moment, le champ du prompt engineering est en effervescence. Les modèles de langage comme GPT-3 permettent en effet d’extraire les éléments qui permettent de mieux discriminer les thèmes d’un texte.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#utilisation-dans-un-processus-de-créatoin-de-contenu-créatif",
    "href": "textes.html#utilisation-dans-un-processus-de-créatoin-de-contenu-créatif",
    "title": "Données textuelles et non structurées",
    "section": "Utilisation dans un processus de créatoin de contenu créatif",
    "text": "Utilisation dans un processus de créatoin de contenu créatif\nLa publication par l’organisation Open AI de son modèle de génération de contenu créatif Dall-E-2 (un jeu de mot mélangeant Dali et Wall-E) a créé un bruit inédit dans le monde de la data-science. Un compte Twitter (Weird Dall-E Mini Generations) propose de nombreuses générations de contenu drôles ou incongrues. Le bloggeur tech Casey Newton a pu parler d’une révolution créative dans le monde de l’IA.\nLa Figure 3 montre un exemple d’image générée par DALL-E-2.\n\n\n\n\n\n\nFigure 3: “A Shiba Inu dog wearing a beret and black turtleneck”\n\n\n\nLes modèles générateurs d’image DallE et Stable Diffusion peuvent, schématiquement, être décomposés en deux niveaux de réseaux de neurones:\n\nle contenu de la phrase est analysé par un modèle de langage comme GPT-3 ;\nles éléments importants de la phrase (recontextualisés) sont ensuite transformés en image à partir de modèles entraînés à reconnaître des images.\n\n\n\n\n\n\nStable Diffusion est une version plus accessible que DALL-E pour les utilisateurs de Python.\n\n\n\n\n\nSi vous êtes intéressés par ce type de modèle, vous pouvez tester les exemples du cours de Python de l’ENSAE. Vous pouvez tester “Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene” pour générer une image comme celle-ci dessous ou chercher à obtenir l’image de votre choix:",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#catégorisation",
    "href": "textes.html#catégorisation",
    "title": "Données textuelles et non structurées",
    "section": "Catégorisation",
    "text": "Catégorisation\nA l’Insee, plusieurs modèles de classification de libellés textuels dans des nomenclatures reposent sur l’algorithme de plongement lexical FastText. Les derniers mis en oeuvre sont les suivants:\n\ncatégorisation des professions dans la nomenclature des PCS ;\ncatégorisation des entreprises dans la nomenclature d’activité APE ;\ncatégorisation des produits dans la nomenclature des COICOP.\n\nLes deux premiers devraient servir prochainement à la production de statistiques officielles. Le troisième est une expérimentation encore en cours.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#appariements",
    "href": "textes.html#appariements",
    "title": "Données textuelles et non structurées",
    "section": "Appariements",
    "text": "Appariements\n\n\n\nSource: Galiana and Suarez Castillo (2022).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#footnotes",
    "href": "textes.html#footnotes",
    "title": "Données textuelles et non structurées",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour remonter plus loin dans la ligne du temps des données textuelles, on peut penser au Soundex, un algorithme d’indexation des textes dans les annuaires dont l’objectif était de permettre de classer à la suite des noms qui ne déviaient que par une différence typographique et non sonore.↩︎",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "administratives_exemples.html",
    "href": "administratives_exemples.html",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "4 exemples de nature différente:\n\nla DSN: base de gestion transmise à l’Insee et la DARES pour la production ;\nSirene: répertoire géré par l’Insee, utilisé par d’autres acteurs ;\nFidéli: agrégation et mise en cohérence de plusieurs sources ;\nSNDS: mise en cohérence de données de gestion hospitalières et de l’assurance maladie, enjeu encore plus fort de confidentialité ;\n\n\n\n\n\n\n\nLes autres répertoires de la statistique publique\n\n\n\n\nFilosofi (Fichier localisé social et fiscal): répertoire de synthèse des sources fiscales ;\nLa Base permanente des équipements (BPE): répertoire d’équipements et services.\n\n\n\n\n\n\n\n\n\n\n\nLes DADS et la DSN\n\n\n\nDescriptions sur le site de l’Insee de la Déclaration annuelle de données sociales (DADS) et de la Déclaration sociale nominative (DSN).\n\n\nLa Déclaration sociale nominative est aujourd’hui le mode d’échanges de données sociales des entreprises vers l’administration, et concerne toutes les entreprises du secteur privé. Elle résulte d’un projet de simplification administrative qui s’est étalé sur près de dix ans : la collecte des données est adossée au processus générateur de la collecte des cotisations sociales, c’est-à-dire au processus de paie (Humbert-Bottin 2018). En plus de réduire la charge imposée aux entreprise, la DSN garantit une bien meilleure qualité et l’exhaustivité de l’information recueillie.\n\n\nLes déclarations sociales font partie des tâches administratives historiquement imposées aux entreprises françaises. La déclaration sociale nominative (DSN), née à la fin des années 2000, a été instituée par la loi de simplification du 22 mars 2012, dite loi Warsman. Elle est obligatoire pour toutes les entreprises depuis début 2017.\nLes déclarations sociales reposaient auparavant sur des formulaires Cerfa dont le contenu était fixé par les textes fondant la collecte des données utiles aux organismes de protection sociale et à l’administration pour l’exercice de leurs missions. Non seulement les déclarants étaient amenés à fournir plusieurs fois la même information, mais ils devaient surtout fournir une information qui n’était pas naturellement produite par leur système de gestion, ce qui était source d’incohérences et d’erreurs dans les déclarations. La DSN met en œuvre une logique fondamentalement différente : elle s’approche au plus près du fait générateur des rémunérations et cotisations sociales dans le domaine de la protection sociale, la paie. Elle repose sur un modèle unique de cette dernière et un échange de données primaires de gestion entre l’émetteur, qui fait la paie, et tous les organismes et administrations qui ont besoin de ces données sociales pour recouvrer des cotisations et servir des droits. Elle opère donc un déplacement de la charge de traitement des données de l’amont (l’entreprise déclarante) vers l’aval.\nLa DSN se fait au niveau de chaque établissement avec un principe clé : chaque salarié doit apparaître dans la déclaration. Cette dernière se fait de manière mensuelle et reflète la paie du mois \\(M-1\\), avec certaines possibilités de correction.\n\n\n\n\n\n\nFigure 1: Schéma explicatif des changements apportés par la DSN. Source : Humbert-Bottin (2018).\n\n\n\n\n\n\nLa DSN présente de nombreux avantages. Elle constitue une source unique et cohérente entre administrations. Avec la DSN, on est sûr que les employeurs et les salariés sont identifiés de la même façon quel que soit l’organisme destinataire de l’information (Renne 2018).\nElle a aussi permis une forte réduction des charges pour les entreprisess (“dites le nous bien une seule fois”). Par exemple, depuis janvier 2018, les entreprises n’ont plus obligation de fournir leur effectif salarié de fin de période, celui-ci pouvant être recalculé directement par les organismes destinataires à partir des informations individuelles transmises sur les salariés (Renne 2018).\nLa fréquence mensuelle de transmission des données permet un meilleur suivi des changements infra-annuels. Auparavant, les entreprises transmettaient des données multiples à diverses échéances et à différents organismes, globalisées par établissement.\nLa DSN n’a pas vocation à servir un besoin spécifique, mais au contraire à couvrir différents usages. Les systèmes d’informations des administrations utilisatrices (Insee, DARES, Pole Emploi, etc.) reçoivent une liste spécifique de données, fixée par arrêté selon leurs missions et se sont synchronisés au fur et à mesure de l’élargissement du périmètre. Depuis 2019, la DSN est le support du prélèvement à la source pour les salariés.\n\n\n\nPlusieurs challenges se posent au moment d’utiliser les données issues de la DSN à des fins statistiques. Tout d’abord, les données sont complexes, ce qui implique un certain coût d’entrée. Elles sont aussi volumineuses (environ 1To par an, sans la fonction publique) et leur traitement requiert ainsi des ressources informatiques conséquentes et des outils adaptés. On constate bien un transfert d’une partie de la charge des entreprises vers les systèmes d’information en aval.\nAutres challenges liés à l’exploitation statistique:\n\nparvenir à relier les concepts administratifs à des réalités économiques ;\néviter les “artefacts” au sens de Bourdieu.\n\n\n\n\n\nLe Système national d’identification et du répertoire des entreprises et de leurs établissements (Sirene) est un répertoire administré par l’Insee qui centralise de l’information sur chacun des 32 millions d’établissements (dont 13 millions d’établissements actifs) existant en France. En particulier, il attribue un numéro SIREN aux entreprises, organismes et associations ainsi qu’un numéro SIRET aux établissements de ces entités.\nL’utilité du numéro SIRET est multiple. S’il constitue avant tout la preuve juridique de l’existence d’un établissement, il permet également d’effectuer un certain nombre de démarches commerciales et administratives.\nAinsi, il sert à :\n\nÉmettre des factures, mais aussi des documents commerciaux. En effet, il est obligatoire de faire apparaître le numéro sur chacun de ces documents. En outre, si l’entreprise à un site internet, le numéro doit apparaître dans les mentions légales ;\nObtenir des informations officielles sur les sociétés. Grâce au SIRET, tout prestataire ou client peut vérifier la fiabilité des données que l’entreprise lui fournit, via une recherche sur internet notamment ;\nProuver l’existence légale de la compagnie. Ce numéro permet en effet de l’identifier auprès de ses clients, prestataires, co-contractants et par l’administration fiscale ;\nProduire des statistiques à partir de la base Sirene et du numéro SIRET. En effet, ces deux éléments donnent accès à des informations capitales que l’INSEE peut réutiliser et analyser.\n\nPour la statistique publique, Sirene met à disposition des utilisateurs un code APE (pour activité principale exercée) choisi dans la Nomenclature d’activité française (NAF) pour chaque établissement (APET) et pour chaque entreprise (APEN), ainsi que sa localisation, sa catégorie juridique, son effectif salarié et l’historique des mouvements (création, cessation, etc.). Le répertoire SIRENE est aussi la base de référence pour toutes les études et enquêtes statistiques sur les entreprises.\n\n\n\nLe Fichier démographique sur les logements et les individus (Fidéli) est une base annuelle exhaustive de données statistiques sur les logements et de leurs occupants. Fidéli est en réalité un assemblage raisonné de données administratives conçu pour répondre à des finalités en matière de statistiques démographiques.\nCet appariement met en regard:\n\ndes données d’origine fiscale: fichier de la taxe d’habitation, fichier des propriétés bâties, fichiers d’imposition des personnes et fichier des déclarations de revenus. Ces données sont de nature démographique pour les personnes et la structure des ménages, ainsi que sur les revenus perçus au sein des foyers;\ndes données contextuelles pour décrire les adresses: coordonnées, appartenance à des mailles géographiques (IRIS, quartiers de la ville), etc. ;\ndes informations sur les agrégats de revenus déclarés et les montants de prestations sociales reçues.\n\nFidéli fournit des possibilités d’études poussées sur des sujets extrêmement variés et à des échelles géographiques fines. Des exemples de projets de recherche récents :\n\nDynamiques de l’organisation du territoire et des inégalités spatiales en milieux urbains pollués ;\nCaractérisation spatiale de la vulnérabilité sociale à la hausse des températures en milieu urbain ;\nEvaluation de l’impact de la majoration de la taxe d’habitation sur les résidences secondaires…\n\n\n\n\nLe Système national des données de santé (SNDS) est un entrepôt de données médico-administratives pseudonymisées couvrant l’ensemble de la population française et contenant l’ensemble des soins présentés au remboursement. Le SNDS peut être vu comme un appariement des grandes bases médico-administratives nationales, notamment :\n\nles données de l’assurance maladie (base SNIIRAM) ;\nles données des hôpitaux (base PMSI) ;\nles causes médicales de décès (base du CépiDC de l’Inserm).\n\nLe SNDS est un dispositif quasiment sans équivalent en Europe ou dans le monde. Il contient un flux annuel de 1,2 milliards de feuilles de soins, 11 millions de séjours hospitaliers et 500 millions d’actes (plus de 3000 variables) qui représentes 450 To de données.\nUne des grandes forces du SNDS est qu’il fait le lien entre médecine de ville et médecine hospitalière, ce qui permet de travailler sur les parcours de soin complets des patients pour des études, recherches ou évaluations présentant un caractère d’intérêt public. Les finalités autorisées pour les traitements sont :\n\nl’information sur la santé et l’offre de soins ;\nl’évaluation des politiques de santé ;\nl’évaluation des dépenses de santé ;\nl’information des professionnels de santé sur leur activité ;\nla veille et la sécurité sanitaires ;\nla recherche, les études, l’évaluation et l’innovation en santé.\n\n\n\n\n\n\n\nMise à disposition des données\n\n\n\nCréé par la Loi du 24 juillet 2019 relative à l’organisation et la transformation du système de santé, le Health Data Hub est un groupement d’intérêt public qui associe 56 parties prenantes, en grande majorité issues de la puissance publique (CNAM, CNRS, Haute Autorité de santé, France Assos Santé, etc.). Le Health Data Hub est en charge de mettre en œuvre les grandes orientations stratégiques relatives au Système National des Données de Santé fixées par l’Etat.\nL’offre du Health Data Hub s’articule autour de 4 enjeux stratégiques:\n\nmettre en valeur le patrimoine des données de santé, en appuyant leur collecte, leur standardisation et leur documentation, en fournissant un hébergement à l’état de l’art sécurisé et un accompagnement dans la mise en conformité RGPD ;\nfaciliter l’usage des données, en proposant un catalogue de données documentées, ainsi qu’une plateforme d’analyse et des outils à l’état de l’art ;\nprotéger les données et les citoyens, en garantissant un très haut niveau de sécurité à travers une démarche éthique de protection des données et de transparence ;\ninnover avec l’ensemble des acteurs, en développant des partenariats académiques et industriels, et en appuyant la dynamique de développement d’outils open source et de l’open data.\n\n\n\n\n\n\n\n\n\nConfidentialité et données de santé\n\n\n\nPour protéger l’identité des patients et garantir la confidentialité des données, chaque patient est repéré dans l’ensemble du SNDS par un pseudonyme, obtenu par l’application au NIR d’un procédé cryptographique irréversible appelé FOIN. Les données du SNDS sont conservées pour une durée totale de 20 ans, puis archivées pour une durée de 10 ans.\nL’accès aux données du SNDS et leur analyse ne peut se faire que dans un cadre d’hébergement très restrictif respectant le référentiel de sécurité du SNDS, afin de garantir la traçabilité des accès et des traitements, la confidentialité des données et leur intégrité.\n\n\n\n\nL’EDP-Santé est un enrichissement des données de l’échantillon démographique permanent (EDP) avec des informations issues du SNDS sur les années 2008-2022. Ce traitement a fait l’objet d’une autorisation de la CNIL et s’inscrit dans le cadre du règlement général sur la protection des données (RGPD), ainsi que la loi relative à l’informatique, aux fichiers et aux libertés (n° 78-17 du 6 janvier 1978 modifiée). Constitué dans le cadre de la stratégie nationale de santé 2018-2022, les données ne sont exploitables que par les personnes habilitées au sein de la DREES et sont conservées pour une période de 5 ans.\nL’EDP-Santé contient :\n\nles données issues de l’EDP concernent l’état civil, la situation familiale, la vie professionnelle (diplôme, situation professionnelle, données relatives à l’activité salariée) et des informations d’ordre économique (revenus, situation fiscale) ;\nles données issues du SNDS sur les recours aux soins et les données issues des certificats de décès.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#la-dsn",
    "href": "administratives_exemples.html#la-dsn",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Les DADS et la DSN\n\n\n\nDescriptions sur le site de l’Insee de la Déclaration annuelle de données sociales (DADS) et de la Déclaration sociale nominative (DSN).\n\n\nLa Déclaration sociale nominative est aujourd’hui le mode d’échanges de données sociales des entreprises vers l’administration, et concerne toutes les entreprises du secteur privé. Elle résulte d’un projet de simplification administrative qui s’est étalé sur près de dix ans : la collecte des données est adossée au processus générateur de la collecte des cotisations sociales, c’est-à-dire au processus de paie (Humbert-Bottin 2018). En plus de réduire la charge imposée aux entreprise, la DSN garantit une bien meilleure qualité et l’exhaustivité de l’information recueillie.\n\n\nLes déclarations sociales font partie des tâches administratives historiquement imposées aux entreprises françaises. La déclaration sociale nominative (DSN), née à la fin des années 2000, a été instituée par la loi de simplification du 22 mars 2012, dite loi Warsman. Elle est obligatoire pour toutes les entreprises depuis début 2017.\nLes déclarations sociales reposaient auparavant sur des formulaires Cerfa dont le contenu était fixé par les textes fondant la collecte des données utiles aux organismes de protection sociale et à l’administration pour l’exercice de leurs missions. Non seulement les déclarants étaient amenés à fournir plusieurs fois la même information, mais ils devaient surtout fournir une information qui n’était pas naturellement produite par leur système de gestion, ce qui était source d’incohérences et d’erreurs dans les déclarations. La DSN met en œuvre une logique fondamentalement différente : elle s’approche au plus près du fait générateur des rémunérations et cotisations sociales dans le domaine de la protection sociale, la paie. Elle repose sur un modèle unique de cette dernière et un échange de données primaires de gestion entre l’émetteur, qui fait la paie, et tous les organismes et administrations qui ont besoin de ces données sociales pour recouvrer des cotisations et servir des droits. Elle opère donc un déplacement de la charge de traitement des données de l’amont (l’entreprise déclarante) vers l’aval.\nLa DSN se fait au niveau de chaque établissement avec un principe clé : chaque salarié doit apparaître dans la déclaration. Cette dernière se fait de manière mensuelle et reflète la paie du mois \\(M-1\\), avec certaines possibilités de correction.\n\n\n\n\n\n\nFigure 1: Schéma explicatif des changements apportés par la DSN. Source : Humbert-Bottin (2018).\n\n\n\n\n\n\nLa DSN présente de nombreux avantages. Elle constitue une source unique et cohérente entre administrations. Avec la DSN, on est sûr que les employeurs et les salariés sont identifiés de la même façon quel que soit l’organisme destinataire de l’information (Renne 2018).\nElle a aussi permis une forte réduction des charges pour les entreprisess (“dites le nous bien une seule fois”). Par exemple, depuis janvier 2018, les entreprises n’ont plus obligation de fournir leur effectif salarié de fin de période, celui-ci pouvant être recalculé directement par les organismes destinataires à partir des informations individuelles transmises sur les salariés (Renne 2018).\nLa fréquence mensuelle de transmission des données permet un meilleur suivi des changements infra-annuels. Auparavant, les entreprises transmettaient des données multiples à diverses échéances et à différents organismes, globalisées par établissement.\nLa DSN n’a pas vocation à servir un besoin spécifique, mais au contraire à couvrir différents usages. Les systèmes d’informations des administrations utilisatrices (Insee, DARES, Pole Emploi, etc.) reçoivent une liste spécifique de données, fixée par arrêté selon leurs missions et se sont synchronisés au fur et à mesure de l’élargissement du périmètre. Depuis 2019, la DSN est le support du prélèvement à la source pour les salariés.\n\n\n\nPlusieurs challenges se posent au moment d’utiliser les données issues de la DSN à des fins statistiques. Tout d’abord, les données sont complexes, ce qui implique un certain coût d’entrée. Elles sont aussi volumineuses (environ 1To par an, sans la fonction publique) et leur traitement requiert ainsi des ressources informatiques conséquentes et des outils adaptés. On constate bien un transfert d’une partie de la charge des entreprises vers les systèmes d’information en aval.\nAutres challenges liés à l’exploitation statistique:\n\nparvenir à relier les concepts administratifs à des réalités économiques ;\néviter les “artefacts” au sens de Bourdieu.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#sirene",
    "href": "administratives_exemples.html#sirene",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Le Système national d’identification et du répertoire des entreprises et de leurs établissements (Sirene) est un répertoire administré par l’Insee qui centralise de l’information sur chacun des 32 millions d’établissements (dont 13 millions d’établissements actifs) existant en France. En particulier, il attribue un numéro SIREN aux entreprises, organismes et associations ainsi qu’un numéro SIRET aux établissements de ces entités.\nL’utilité du numéro SIRET est multiple. S’il constitue avant tout la preuve juridique de l’existence d’un établissement, il permet également d’effectuer un certain nombre de démarches commerciales et administratives.\nAinsi, il sert à :\n\nÉmettre des factures, mais aussi des documents commerciaux. En effet, il est obligatoire de faire apparaître le numéro sur chacun de ces documents. En outre, si l’entreprise à un site internet, le numéro doit apparaître dans les mentions légales ;\nObtenir des informations officielles sur les sociétés. Grâce au SIRET, tout prestataire ou client peut vérifier la fiabilité des données que l’entreprise lui fournit, via une recherche sur internet notamment ;\nProuver l’existence légale de la compagnie. Ce numéro permet en effet de l’identifier auprès de ses clients, prestataires, co-contractants et par l’administration fiscale ;\nProduire des statistiques à partir de la base Sirene et du numéro SIRET. En effet, ces deux éléments donnent accès à des informations capitales que l’INSEE peut réutiliser et analyser.\n\nPour la statistique publique, Sirene met à disposition des utilisateurs un code APE (pour activité principale exercée) choisi dans la Nomenclature d’activité française (NAF) pour chaque établissement (APET) et pour chaque entreprise (APEN), ainsi que sa localisation, sa catégorie juridique, son effectif salarié et l’historique des mouvements (création, cessation, etc.). Le répertoire SIRENE est aussi la base de référence pour toutes les études et enquêtes statistiques sur les entreprises.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#fidéli",
    "href": "administratives_exemples.html#fidéli",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Le Fichier démographique sur les logements et les individus (Fidéli) est une base annuelle exhaustive de données statistiques sur les logements et de leurs occupants. Fidéli est en réalité un assemblage raisonné de données administratives conçu pour répondre à des finalités en matière de statistiques démographiques.\nCet appariement met en regard:\n\ndes données d’origine fiscale: fichier de la taxe d’habitation, fichier des propriétés bâties, fichiers d’imposition des personnes et fichier des déclarations de revenus. Ces données sont de nature démographique pour les personnes et la structure des ménages, ainsi que sur les revenus perçus au sein des foyers;\ndes données contextuelles pour décrire les adresses: coordonnées, appartenance à des mailles géographiques (IRIS, quartiers de la ville), etc. ;\ndes informations sur les agrégats de revenus déclarés et les montants de prestations sociales reçues.\n\nFidéli fournit des possibilités d’études poussées sur des sujets extrêmement variés et à des échelles géographiques fines. Des exemples de projets de recherche récents :\n\nDynamiques de l’organisation du territoire et des inégalités spatiales en milieux urbains pollués ;\nCaractérisation spatiale de la vulnérabilité sociale à la hausse des températures en milieu urbain ;\nEvaluation de l’impact de la majoration de la taxe d’habitation sur les résidences secondaires…",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#snds",
    "href": "administratives_exemples.html#snds",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Le Système national des données de santé (SNDS) est un entrepôt de données médico-administratives pseudonymisées couvrant l’ensemble de la population française et contenant l’ensemble des soins présentés au remboursement. Le SNDS peut être vu comme un appariement des grandes bases médico-administratives nationales, notamment :\n\nles données de l’assurance maladie (base SNIIRAM) ;\nles données des hôpitaux (base PMSI) ;\nles causes médicales de décès (base du CépiDC de l’Inserm).\n\nLe SNDS est un dispositif quasiment sans équivalent en Europe ou dans le monde. Il contient un flux annuel de 1,2 milliards de feuilles de soins, 11 millions de séjours hospitaliers et 500 millions d’actes (plus de 3000 variables) qui représentes 450 To de données.\nUne des grandes forces du SNDS est qu’il fait le lien entre médecine de ville et médecine hospitalière, ce qui permet de travailler sur les parcours de soin complets des patients pour des études, recherches ou évaluations présentant un caractère d’intérêt public. Les finalités autorisées pour les traitements sont :\n\nl’information sur la santé et l’offre de soins ;\nl’évaluation des politiques de santé ;\nl’évaluation des dépenses de santé ;\nl’information des professionnels de santé sur leur activité ;\nla veille et la sécurité sanitaires ;\nla recherche, les études, l’évaluation et l’innovation en santé.\n\n\n\n\n\n\n\nMise à disposition des données\n\n\n\nCréé par la Loi du 24 juillet 2019 relative à l’organisation et la transformation du système de santé, le Health Data Hub est un groupement d’intérêt public qui associe 56 parties prenantes, en grande majorité issues de la puissance publique (CNAM, CNRS, Haute Autorité de santé, France Assos Santé, etc.). Le Health Data Hub est en charge de mettre en œuvre les grandes orientations stratégiques relatives au Système National des Données de Santé fixées par l’Etat.\nL’offre du Health Data Hub s’articule autour de 4 enjeux stratégiques:\n\nmettre en valeur le patrimoine des données de santé, en appuyant leur collecte, leur standardisation et leur documentation, en fournissant un hébergement à l’état de l’art sécurisé et un accompagnement dans la mise en conformité RGPD ;\nfaciliter l’usage des données, en proposant un catalogue de données documentées, ainsi qu’une plateforme d’analyse et des outils à l’état de l’art ;\nprotéger les données et les citoyens, en garantissant un très haut niveau de sécurité à travers une démarche éthique de protection des données et de transparence ;\ninnover avec l’ensemble des acteurs, en développant des partenariats académiques et industriels, et en appuyant la dynamique de développement d’outils open source et de l’open data.\n\n\n\n\n\n\n\n\n\nConfidentialité et données de santé\n\n\n\nPour protéger l’identité des patients et garantir la confidentialité des données, chaque patient est repéré dans l’ensemble du SNDS par un pseudonyme, obtenu par l’application au NIR d’un procédé cryptographique irréversible appelé FOIN. Les données du SNDS sont conservées pour une durée totale de 20 ans, puis archivées pour une durée de 10 ans.\nL’accès aux données du SNDS et leur analyse ne peut se faire que dans un cadre d’hébergement très restrictif respectant le référentiel de sécurité du SNDS, afin de garantir la traçabilité des accès et des traitements, la confidentialité des données et leur intégrité.\n\n\n\n\nL’EDP-Santé est un enrichissement des données de l’échantillon démographique permanent (EDP) avec des informations issues du SNDS sur les années 2008-2022. Ce traitement a fait l’objet d’une autorisation de la CNIL et s’inscrit dans le cadre du règlement général sur la protection des données (RGPD), ainsi que la loi relative à l’informatique, aux fichiers et aux libertés (n° 78-17 du 6 janvier 1978 modifiée). Constitué dans le cadre de la stratégie nationale de santé 2018-2022, les données ne sont exploitables que par les personnes habilitées au sein de la DREES et sont conservées pour une période de 5 ans.\nL’EDP-Santé contient :\n\nles données issues de l’EDP concernent l’état civil, la situation familiale, la vie professionnelle (diplôme, situation professionnelle, données relatives à l’activité salariée) et des informations d’ordre économique (revenus, situation fiscale) ;\nles données issues du SNDS sur les recours aux soins et les données issues des certificats de décès.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "geolocalized_data.html",
    "href": "geolocalized_data.html",
    "title": "Données géolocalisées",
    "section": "",
    "text": "Disposer de données géolocalisées pour produire de la statistique publique est un besoin qui se fait de plus en plus fort. Pour cause, un intérêt croissant est accordé aux caractéristiques spatiales des phénomènes que la statistique publique a pour rôle de décrire. Le comité d’experts des Nations Unies sur la gestion de l’information géospatiale mondiale (UN-GGIM) a d’ailleurs reconnu l’importance cruciale d’intégrer les informations géospatiales aux statistiques et aux données socio-économiques et le développement d’une infrastructure statistique géospatiale.\nLa production et la diffusion accrue de données géolocalisées dépasse le cadre de la statistique publique. La généralisation de traces numériques géolocalisées (données mobile, GPS, localisation d’adresses IP…) a entraîné une multiplication des acteurs valorisant des données spatiales. Certains acteurs de l’écosystème de la donnée sont spécialisés dans la collecte ou la valorisation de sources géolocalisées collectées par d’autres.\nUn premier apport fondamental des données géolocalisées est qu’elles permettent de calculer des indicateurs avec une granularité spatiale plus fine que les découpages administratifs ou historiques classiques. Cette approche permet d’éclairer des phénomènes socio-économiques locaux comme les problématiques de mixité (Galiana, Sémécurbe, et al. 2020). L’Insee met à disposition en open-data des données très fines sur une grande variété de facteur. Les sites officiels geoportail et statistiques-locales.insee.fr ou encore les sites faits par des tiers comme celui d’Etienne Côme ou hubblo permettent d’explorer la richesse des sources fines mises à disposition. Pour désigner les sources les plus fines, on parle de données carroyées, publiées sur des carreaux pouvant aller de 200 mètres à plusieurs kilomètres de côté (voir Figure 1). Une telle granularité permet de capter certains phénomènes démographiques ou socio-économiques qui ne sont pas détectables au niveau de l’IRIS ou de la commune1.\n\n\n\n\n\n\nFigure 1: Carte des densités de population sur des carreaux de largeur d’un kilomètre à Lyon et ses alentours en 2017 (calculées à partir de Filosofi). Source : géoportail.",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Données géolocalisées"
    ]
  },
  {
    "objectID": "geolocalized_data.html#footnotes",
    "href": "geolocalized_data.html#footnotes",
    "title": "Données géolocalisées",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe projet gridviz porté par Eurostat vise à proposer un outil facilitant la construction de mosaiques agrégées à partir de données spatiales.↩︎",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Données géolocalisées"
    ]
  },
  {
    "objectID": "series_temporelles.html",
    "href": "series_temporelles.html",
    "title": "Nowcasting",
    "section": "",
    "text": "La statistique publique publiant des statistiques à intervalle régulier, une longue tradition de production et d’exploitation de séries temporelles précède les innovations récentes. La constance de la méthodologie, ou les harmonisations faites a posteriori - par exemple la technique de la rétropolation en comptabilité nationale - assure en théorie une forme de comparabilité et permet de considérer les productions statistiques comme des séries temporelles. Parmi les productions statistiques dont la formalisation a été la plus précoce, la comptabilité nationale tient une bonne place. Le Système de Comptabilité Nationale (SCN) est ainsi un cadre international harmonisé qui permet la construction de séries temporelles depuis l’après-guerre. La France est l’un des pays pour lesquels il est possible de remonter le plus loin dans le passé avec des séries harmonisées depuis XXXX.\nEn raison de délais imposés par la collecte et le traitement des données, certains indicateurs supposés donner des informations sur la situation actuelle sont publiés avec du retard et ne peuvent pas jouer leur rôle dans la prise de décision publique. C’est pourquoi la statistique publique participe aussi à la réalisation de prévisions à court terme de valeurs d’indicateurs macro-économiques. Si la production de séries statistiques récurrentes fait partie des missions de tous les instituts statistiques, l’Insee a également des missions plus spécifiques dans le domaine des séries temporelles. La construction d’indicateurs conjoncturels prospectifs, au service du débat public et de la prise de décision politique, en fait partie. A l’Insee cette mission prospective est assurée par le département de la conjoncture qui construit des indicateurs et des analyses prospectifs sur l’activité économiques des prochains trimestres. Ce département mobilise historiquement des données dont la remontée est plus rapide que celles utilisées pour la construction des agrégats macroéconomiques de la comptabilité nationale. Cependant, la collecte accrue de traces numériques a permis l’accès à des données à haute fréquence pouvant être mobilisées pour disposer de signaux sur la situation macroéconomique actuelle ou très récente.\nDans ce cadre, l’Insee, QuantCube1, Paris School of Economics2, CANDRIAM et la Société Générale ont créé une Chaire de recherche Mesures de l’économie, nowcasting ‐ au‐delà du PIB en 2021. Cette Chaire a pour objectif de travailler sur l’amélioration des prévisions économiques, en particulier grâce à la mobilisation de nouvelles sources de données. Parmi ces nouvelles sources, on trouve les actualités (Bortoli, Combes, and Renault 2018), les médias sociaux, les données satellitaires, les réseaux professionnels et les avis de consommateurs, ainsi que les données sur le commerce international, la consommation d’électricité et le transport routier (Fornaro 2020), le transport maritime, l’immobilier, l’hôtellerie et les télécommunications. Un autre objectif de la Chaire est de travailler sur la mesure de nouveaux indicateurs économiques, de bien-être ou de développement durable (au‐delà du PIB), ici encore en utilisant ces données nouvelles.\nCôté technique, des modèles autorégressifs de type bridge models ou mixed-data sampling (Schumacher 2016), des dynamic factor models (Stock and Watson 2010) ou plus récemment des modèles de Deep Learning de type LSTM (Hopp 2021) sont souvent utilisés pour combiner des indicateurs de type soft comme le climat des affaires ou le sentiment des consommateurs avec des indicateurs hard comme la production industrielle, le commerce de détail, les prix de l’immobilier, etc. à différentes fréquences. La littérature fait particulièrement état de l’utilisation de deux sources de données massive permettant d’obtenir des indicateurs soft (Richardson 2018), même si bien d’autres sources ont été expertisées :\n‑ les statistiques de recherches sur Internet basées sur la fréquence de recherche de mots‑clés ou de sujets spécifiques ; ‑ les médias sociaux sur Internet (Twitter).",
    "crumbs": [
      "Home",
      "Nowcasting",
      "Nowcasting"
    ]
  },
  {
    "objectID": "series_temporelles.html#footnotes",
    "href": "series_temporelles.html#footnotes",
    "title": "Nowcasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStart‐up proposant des prévisions macroéconomiques fondées sur le Big Data et l’intelligence artificielle↩︎\nsociété internationale de gestion d’actifs↩︎\nDans le domaine de la monétique, le Groupement des cartes bancaires est un groupement d’intérêt économique privé qui réunit la plupart des établissements financiers français dans le but d’assurer l’interbancarité des cartes de paiement.↩︎",
    "crumbs": [
      "Home",
      "Nowcasting",
      "Nowcasting"
    ]
  },
  {
    "objectID": "nowcasting_exemples.html",
    "href": "nowcasting_exemples.html",
    "title": "Application",
    "section": "",
    "text": "L’objectif de ce TP est de montrer comment récupérer des données à partir de l’API de Twitter et de montrer un début d’exemple d’exploitation de ces données.\nLe TP peut être lancé sur le SSP Cloud en cliquant sur le bouton suivant:\n\nUn second bouton si l’installation de Pytorch est trop longue:",
    "crumbs": [
      "Home",
      "Nowcasting",
      "Application"
    ]
  },
  {
    "objectID": "geolocalized_data_exemples.html",
    "href": "geolocalized_data_exemples.html",
    "title": "Application",
    "section": "",
    "text": "L’objectif de ce TP est de donner un exemple introductif d’utilisation de données géolocalisées dans le cadre de la production de statistique publique. Plus précisément, il propose de manipuler les données du Registre Parcellaire Graphique (RPG), une base de données géographiques administrative servant de référence à l’instruction des aides de la politique agricole commune (PAC). Ces données sont mises en regard:\n\ndes données du Drias de simulation de l’évolution climatique pour le siècle en cours sur la France;\ndes données ERA5 agro-météorologiques de surface quotidiens pour la période allant de 1979 à aujourd’hui.\n\nLe TP composé de plusieurs applications décrites ci-dessous et disponibles sur ce site.\nUn environnement de travail peut être lancé sur le SSP Cloud en cliquant sur le bouton suivant:\n\n\nApplication 0 (optionnelle) : Création d’une base de données PostgreSQL\nLes données du RPG étant volumineuses (on a vu que c’était souvent le cas pour les données administratives), il faut pouvoir les requêter depuis une base de données. Les données sont disponibles dans une base de données PostgreSQL (avec l’extension pour données spatiales PostGIS) prête à l’emploi. Néanmoins, cette application explique comment procéder pour créer une telle base de données sur la plateforme SSP Cloud.\n\n\nApplication 1 : Première manipulation du RPG\nL’objectif de cette première étape est d’effectuer des premières requêtes géographiques permettant d’examiner les cultures à proximité d’un point géographique donné, et de comparer la composition observée avec les compositions départementale, régionale, etc. On propose également de mettre au point une interface de type tableau de bord permettant d’obtenir ces informations interactivement.\n\n\nApplication 2 : Exposition des cultures au déficit de précipitations\nL’objectif de cette application est de mettre en regard cultures et prévisions climatiques localement, pour identifier des cultures particulièrement mises en danger par le changement climatique en France.\n\n\nApplication 3 : Evolution des cultures, lien avec le climat passé\nAprès avoir regardé vers l’avenir, il est temps de jeter un coup d’oeil dans le rétroviseur, et de regarder comment l’évolution des températures au cours des 40 dernières années a pu influencer certaines cultures en France. On estimera l’évolution des dates potentielles de récolte du maïs grain dans les différents bassins de productions français depuis 1980.",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Application"
    ]
  },
  {
    "objectID": "images.html",
    "href": "images.html",
    "title": "Images",
    "section": "",
    "text": "Les images sont des données qui sont utilisées depuis longtemps de manière automatique. Une image pour un ordinateur est représentée par un tableau en 2 ou 3 dimensions (images en nuances de gris et images en couleur respectivement). En 2 dimensions, l’image a ainsi une longueur \\(L\\) et une largeur \\(W\\) : elle est constituée de \\(L \\times W\\) pixels, chacun associé à une valeur entière comprise entre 0 et 255 (ou parfois à une valeur décimale comprise entre 0 et 1), comme illustré en Figure 1.\n\n\n\n\n\n\nFigure 1: Représentation du logo de Python en nuances de gris avec une faible résolution. La valeur de chaque pixel (entier allant de 0 pour un pixel complètement noir à 255 pour un pixel complètement blanc) figure à l’emplacement de ce dernier.\n\n\n\nUne image en couleur est constituée de 3 canaux (RGB pour Red, Green et Blue). Chacun des \\(L \\times W\\) pixels de l’image est ainsi associé à 3 valeurs entières comprises entre 0 et 225 (ou à 3 valeurs décimales comprises entre 0 et 1), comme illustré en Figure 2.\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 2: Représentation du logo de Python en couleurs. L’image du haut correspond à la superposition des trois canaux représentés sur la rangée inférieure.\n\n\n\nLe domaine de la vision par ordinateur (computer vision) a vu le jour dans les années 1960 avec le développement des premiers algorithmes cherchant à extraire de l’information d’images. Par exemple, Sobel and Feldman (1973) introduit la méthode suivante pour faire de la détection de contours sur une image \\(A\\).\nOn calcule\n\\[\nG_x = \\begin{bmatrix}\n+1 & 0 & -1\\\\\n+2 & 0 & -2\\\\\n+1 & 0 & -1\n\\end{bmatrix} \\star A \\quad \\text{et} \\quad G_y = \\begin{bmatrix}\n+1 & +2 & +1\\\\\n0 & 0 & 0\\\\\n-1 & -2 & -1\n\\end{bmatrix} \\star A\n\\]\noù \\(\\star\\) est l’opérateur de convolution 2-dimensionnel en traitement du signal (illustré en Figure 4).\nAlors l’image \\(G = \\sqrt{G_x^2 + G_y^2}\\) fournit une représentation des contours de l’image \\(A\\). Une illustration de l’application de cette méthode est donnée en Figure 3.\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 3: L’image de droite est obtenue par application sur l’image de gauche de la méthode de détection de contours introduite par Sobel and Feldman (1973). Source : Wikipedia.\n\n\n\n\n\n\n\n\n\nFigure 4: Illustration de l’opérateur de convolution 2-dimensionnel \\(\\star\\). Le noyau (matrice en bleu sur le dessin) est multiplié par -1 et glisse sur la matrice de gauche. Une multiplication élément par élément est faite sur chaque sous-matrice de la taille du noyau. Pour chacune de ces multiplication, les coefficients sont ensuite sommés pour donner une valeur de sortie unique. Par exemple ici, la valeur du pixel en vert correspond au calcul \\(3 = 1*(-1) + 1*1 + 1*2 + 1*1\\).",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#réseaux-de-neurone-convolutifs",
    "href": "images.html#réseaux-de-neurone-convolutifs",
    "title": "Images",
    "section": "Réseaux de neurone convolutifs",
    "text": "Réseaux de neurone convolutifs\nUne architecture de modèles a joué un rôle particulièrement important dans cette révolution : les réseaux de neurones convolutifs (voir LeCun et al. 1989 pour un des articles fondateurs). Ces réseaux de neurones sont constitués d’un enchaînement de couches convolutives, chacune composée de trois étapes :\n\nUne étape de convolution utilisant l’opérateur \\(\\star\\) décrit ci-dessus qui transforme un tenseur 3-dimensionnel de taille \\((H, W, C)\\) en entrée en un tenseur de taille \\((H', W', C')\\) ou \\(H'\\), \\(W'\\) et \\(C'\\) dépendent de la taille du noyau de convolution choisi ;\nUne étape de détection où une fonction non-linéaire est appliquée au tenseur obtenu en sortie de l’étape de convolution ;\nUne étape de pooling où chaque canal du tenseur en entrée voit sa hauteur et largeur réduite à l’aide une fonction qui remplace chaque valeur par une statistique impliquant les valeurs des pixels voisins (fréquemment, la valeur maximale dans un voisinage rectangulaire : c’est l’opération de max pooling).\n\nLa succession de ces opérations est résumée dans la Figure 5\n\n\n\n\n\n\nFigure 5: Illustration d’une succession de séquences d’un réseau convolutionnel. Emprunté à https://www.analyticsvidhya.com/blog/2022/01/convolutional-neural-network-an-overview/\n\n\n\nLes tenseurs obtenus en sortie des couches convolutives sont appelés activation maps ou feature maps. Chaque feature map peut s’interpréter comme une carte qui indique les endroits où on peut trouver une feature particulière (par exemple un bord, une texture, une partie d’un objet, etc.) au sein de l’image. Les features pertinentes (c’est-à-dire les coefficients des filtres de convolution utilisés) sont apprises par le réseau de neurones au cours de la phase d’entraînement. On peut voir ces features comme des structures latentes qui combinées ensemble génèrent un objet sur l’image finale.\nLes réseaux de neurones convolutifs présentent plusieurs caractéristiques essentielles pour des tâches de vision par ordinateur, qui expliquent en partie leur succès : une invariance (relative) à la translation, la rotation et à l’échelle. Ces caractéristiques permettent aux modèles d’abstraire l’identité d’un objet de détails spécifiques aux images données en entrée tels que la position et l’orientation de cet objet par rapport à la caméra.",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#segmentation-sémantique",
    "href": "images.html#segmentation-sémantique",
    "title": "Images",
    "section": "Segmentation sémantique",
    "text": "Segmentation sémantique\nLa segmentation sémantique est une tâche de vision par ordinateur qui consiste à associer une étiquette ou une catégorie à chaque pixel d’une image (illustration en Figure 7). Plusieurs architectures de réseaux de neurones convolutifs entraînées sur des gros jeux d’entraînement obtiennent des performances très élevées sur des jeux de données d’évaluation de référence, comme l’architecture DeepLabV3 (Chen et al. 2017). Les principaux frameworks de Deep Learning fournissent des implémentations de modèles de segmentation sémantique (avec ou sans coefficients pré-entraînés) : c’est le cas du package Python torchvision par exemple qui propose une implémentation des modèles DeepLabV3, FCN et LRASPP.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Segmentation sémantique effectuée sur une photo de chat (partie gauche de la Figure). Sur le masque de segmentation (partie droite de la Figure), les pixels verts sont associés à la classe chat tandis que les pixels roses sont associés à la classe arrière-plan. Source : Hugging Face.\n\n\n\n\n\n\n\n\n\nFigure 7: Un autre exemple de segmentation sémantique, issu de ce blog",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#utilisation-de-données-dobservation-satellitaire",
    "href": "images.html#utilisation-de-données-dobservation-satellitaire",
    "title": "Images",
    "section": "Utilisation de données d’observation satellitaire",
    "text": "Utilisation de données d’observation satellitaire\n\nNature de la donnée\nDans le domaine des données d’Earth Observation, qui regroupent en fait différentes sources de données (radars, orthophotographies…), les données photographiques issues de satellites ont une place de choix. Celles-ci permettent d’observer les territoires, que ce soit leur topologie ou leur usage et potentiellement d’en tirer des enseignements à diffuser sous la forme de statistiques publiques. Par exemple, l’utilisation de données satellitaires peut permettre d’améliorer la granularité spatiale et temporelles de statistiques publiées aujourd’hui sur la production agricole (part du territoire cultivé, nature des cultures…).\nDe manière générale, ces données ont beaucoup de potentiel lorsqu’elles sont utilisées en combinaison avec d’autres sources de données lorsqu’il s’agit de pallier des insuffisances ou des manques concernant les données traditionnellement utilisées pour la statistique publique. Par exemple, Steele et al. (2017) combinent données de satellites et données de téléphonie mobile pour estimer des taux de pauvreté. En France, les départements et régions d’outre-mer sont particulièrement concernés. Les données satellites permettraient d’y combler des imperfections des données administratives. Par exemple, les parcelles cadastrales y sont parfois mal identifiées ou rarement mises à jour. Les données satellitaires peuvent être utilisées pour fiabiliser cette information.\n\n\n\n\n\n\nNote\n\n\n\nLes données d’Earth Observation présentent des difficultés d’utilisation non-négligeables dans un contexte de production statistique :\n\nIl faut au moment de la production de la statistique désirée s’assurer que l’on parvient à des résultats statistiquement robustes ;\nProduire des statistiques de manière récurrente à partir d’une source de données demande d’avoir du recul sur le fonctionnement de la chaîne de traitement en production. Comme les données d’Earth Observation ne sont aujourd’hui utilisées que par peu d’instituts statistiques, il est difficile d’avoir un tel recul sans soi-même avoir une chaîne de traitement qui tourne depuis plusieurs années ;\nPour de nombreuses applications, on souhaite utiliser des images avec une résolution élevée mais aussi exploiter la haute fréquence temporelle de passage de certains satellites. Dans un tel cadre les données d’Earth Observation ont souvent un volume très important. Entraîner des modèles pertinents (les modèles de Deep Learning state-of-the-art sont complexes) demande d’avoir des ressources informatiques adaptées à disposition ;\nSelon les besoins, la résolution disponible peut ne pas correspondre aux besoins de la statistique.\n\n\n\n\n\nFournisseurs de données\nDes acteurs publient des données satellitaires en open data :\n\nLa NASA à travers son programme historique Landsat. Les dernières générations des satellites Landsat recueillent des images dans une dizaine de bandes spectrales (bandes visibles mais aussi bandes infrarouges) avec une résolution spatiale de 30 mètres (pour les bandes visibles) ;\nL’Agence spatiale européenne (ESA) a lancé le programme Sentinel-2 en 2015. Les images des satellites Sentinel-2 sont aussi disponibles en open data, sur 12 bandes avec une résolution spatiale de 10 mètres, plus fine que celle des images de Landsat. La périodicité de la couverture des satellites Sentinel-2 est relativement faible : ces derniers repassent au-dessus des mêmes zones tous les cinq jours.\n\nDes entreprises privées collectent aussi des images avec leurs propres satellites, parfois avec des meilleures résolutions que les images disponibles en libre accès, ce qui peut être nécessaire en fonction du cas d’usage envisagé. De manière générale, il y a toutefois un arbitrage à faire entre le détail local des mesures (résolution radiométrique, nombre de bandes spectrales) et la résolution spatiale des images. La richesse des images issues de satellites réside plutôt dans la première dimensions, alors que les orthophotographies par exemple sont à privilégier si on désire une plus haute résolution spatiale.\n\n\nPipeline\nLe traitement d’images de satellites se divise de manière classique en trois parties (Direction de la recherche et de l’innovation 2018) :\n\nd’abord vient le pré-traitement des données, qui inclut le stockage, le data managment, le contrôle de la qualité des données, l’inclusion d’autres sources et l’identification d’outils appropriés pour l’analyse.\nCe pré-traitement est suivi par une phase d’analyse, où l’on définit les indicateurs à calculer, les données à utiliser et où l’on applique la méthode analytique choisie.\nEnfin, au cours de la phase d’évaluation, on collecte et on interprète les résultats de l’analyse.\n\nDes méthodes historiques existent pour analyser des images de satellites (pour in fine produire des statistiques). Par exemple, l’utilisation de modèle physiques pour prédire la valeur d’une variable d’intérêt à partir de l’observation empirique de certaine bandes, ou encore de méthodes d’analyse d’images traditionnelles où des informations spatiales, relatives à des motifs, à des textures, etc. sert à segmenter l’image sous supervision humaine (OBIA). Récemment, le Machine Learning (et en particulier le Deep Learning) a fourni des outils d’analyse puissants facilement applicables aux images satellites.\n\n\nCas d’usage\nLes cas d’usage potentiels d’utilisation de ces données pour la statistique publique touchent de nombreux thèmes, qui incluent :\n\nLa supervision des forêts, de l’agriculture, des masses d’eau ;\nL’urbanisation et les infrastructures ;\nLa pollution environnementale et la qualité de l’air atmosphérique ;\n\nEn particulier, l’analyse d’images satellite peut permettre de calculer des indicateurs comme la proportion de surface agricole en agriculture intensive ou en agriculture durable, le pourcentage de masses d’eau présentant une bonne qualité de l’eau ambiante, la couverture forestière dans le cadre d’une gestion forestière durable, la perte nette permanente de forêts, etc.\nPlusieurs cas d’usage précis ont été ciblés aujourd’hui pour la statistique publique en France et donnent ou vont donner lieu à des travaux expérimentaux.\n\nUn des cas d’usage identifiés depuis un moment déjà est l’utilisation d’images satellites pour calculer les statistiques sur l’occupation et l’usage des sols sur le territoire français. Aujourd’hui, ces statistiques sont tirées de l’enquête Teruti conduite par le Bureau des statistiques structurelles environnementales et forestières du SSP (Ministère de l’Agriculture).\nUn échantillon de points est observé sur le terrain sur un cycle de 3 ans permettant d’estimer l’occupation des sols avec une précision qui reste satisfaisante à l’échelon départemental. L’échantillonnage des points se fait à partir de sources multiples, dont des données satellitaires (satellite SPOT) et des orthophotographies de l’IGN. En outre, une phase de validation des résultats de l’enquête est réalisée à partir d’une couche d’exploitation du sol issue de données de Sentinel-2 et réalisée de manière automatique est le Centre d’Etudes Spatiales sur la BIOsphere à Toulouse.\nDes travaux sont actuellement en cours pour encore davantage améliorer la phase d’échantillonnage à l’aide d’images satellitaires. En outre, une méthode automatique donnant des couches d’exploitation des sols avec une précision suffisante pour les besoins de la statistique publique pourrait permettre de diffuser des statistiques plus régulièrement qu’avec l’enquête Teruti et avec une granularité territoriale plus fine.\nLes parcelles cadastrales sont parfois mal identifiées dans les départements et région d’outre-mer, en particulier en Guyane et à Mayotte. Or ces parcelles sont utilisées pour des tirages d’échantillon par l’Insee, pour le recensement de la population par exemple.\nIci encore, des modèles de segmentation retournant des couches d’exploitation et d’usage des sols peuvent être utilisés pour consolider l’information disponible sur les parcelles cadastrales. Dans le cadre d’une expérimentation, un modèle de segmentation U-Net (Ronneberger, Fischer, and Brox 2015) pré-entraîné sur le jeu de données ImageNet a été fine-tuné sur un sous-échantillon du jeu annoté S2GLC (Sentinel-2 Global Land Cover). Ce modèle prend en entrée une image satellite et renvoie une prédiction pixel par pixel de la catégorie de terrain (en 10 classes), comme illustré en Figure 8. S’il est assez précis sur la catégorie surfaces artificielles et construction, ses prédictions pourraient servir à consolider les données cadastrales.\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 8: À gauche : image satellite issue de Sentinel-2. Au milieu : segmentation prédite par le modèle U-Net. À droite : vraie segmentation de l’image.\n\n\n\n\nL’enquête sur la structure des exploitations agricoles (Bureau des statistiques structurelles environnementales et forestières du SSP) dont la prochaine édition aura lieu en 2023 pose des questions sur les vergers. Il n’existe aujourd’hui pas de source administrative permettant de consolider les résultats de l’enquête sur cette thématique.Ainsi, un projet d’expérimentation utilisant des orthophotographies pour dénombrer le nombre d’arbres et la surface associée est envisagé. La librairie DeepForest propose des modèles pré-entraînés pour faire de la détection d’arbres (voir Figure 9) et pourra servir de point de départ pour cette expérimentation.\n\n\n\n\n\n\n\nFigure 9: Détection d’arbres sur une orthophotographie à l’aide de la librairie DeepForest.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlusieurs questions méthodologiques essentielles se posent lorsqu’on exploite des données satellitaires grâce à des méthodes de Deep Learning :\n\nArchitectures des modèles\nUtilisation des différentes bandes\nPré-traitements sur les images : détection et suppression de nuages, amélioration de la résolution ;\nTransférabilité des modèles : est-ce qu’un modèle entraîné sur des images provenant d’un satellite fonctionnera correctement avec des images provenant d’un autre satellite ? Ou avec un réentraînement minimal ?\n\nUn enjeu majeur est l’obtention de données annotées (même si le pré-entraînement de modèles sur des jeux de données énormes réduit le besoin de données annotées pour la tâche considérée). Pour des tâches de prédiction de l’utilisation du sol, on peut par exemple mobiliser la base de données géographiques CORINE Land Cover, un inventaire biophysique qui fournit une photographie complète de l’occupation des sols, à des fréquences régulières.\nElle est issue de l’interprétation visuelle d’images satellitaires, avec des données complémentaires d’appui. Les classes d’occupation correspondent à une nomenclature comportant 44 postes.",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#la-reconnaissance-optique-de-caractères",
    "href": "images.html#la-reconnaissance-optique-de-caractères",
    "title": "Images",
    "section": "La reconnaissance optique de caractères",
    "text": "La reconnaissance optique de caractères\nL’administration française a été historiquement une grande productrice de fichiers sous format papier. Même si la numérisation des sources de collectes administratives réduit le volume de production papier, ce dernier mode de collecte est encore d’usage. Afin de réduire le temps de numérisation, il est donc utile de mettre en oeuvre des routines automatisées. Dans la même veine, l’administration a longtemps mis en oeuvre des publications (tableaux ou graphiques) sous format papier. Être en mesure de valoriser ce patrimoine de connaissance est un enjeu pour la recherche.\nLa reconnaissance optique de caractères (souvent abrégée par OCR pour Optical character recognition) désigne la tâche de conversion de texte manuscrit ou imprimé en texte encodé par un ordinateur. C’est une tâche essentielle pour exploiter des documents disponibles sous la forme d’images numériques.\nDévelopper son propre moteur d’OCR est une tâche très complexe mais heureusement des moteurs open source existent. Tesseract est un logiciel pour la reconnaissance de caractères open source depuis 2015. Tesseract offre plusieurs moteurs depuis sa version 4 : en plus du moteur historique, un moteur basé sur le Deep Learning (réseaux de neurones LSTM) est aujourd’hui disponible.\n\nApplication : extraction d’informations de documents scannés photographiés\nDes documents scannés ou photographies peuvent souvent constituer une source d’information précieuse pour la production de statistiques publiques.\nPar exemple, la Direction des Statistiques d’Entreprises (DSE) à l’Insee effectue de manière périodique un profilage des groupes de sociétés. Pour la statistique publique la notion d’entreprise est souvent associée à une définition purement juridique, c’est-à-dire à la notion d’unité légale inscrite au répertoire Sirene. Toutefois, aujourd’hui certaines unités légales sont détenues par d’autres et peuvent ainsi perdre une partie de leur autonomie. Le profilage consiste à identifier au sein des groupes les entreprises au sens économique, puis à collecter et calculer des statistiques sur ces nouveaux contours.\nLa plupart des catégories de sociétés ont l’obligation de déposer annuellement leurs comptes sociaux au Registre du commerce et des sociétés (RCS), afin d’en garantir la transparence. Les documents à déposer incluent les comptes annuels (bilan actif et passif, compte de résultats et annexes), le rapport de gestion pour les sociétés cotées, les documents portant sur l’affectation du résultat, etc. Dans le cas où une société possède des filiales ou participations au moins à hauteur de 10% du capital, elle doit inclure dans ses comptes sociaux un tableau des filiales et participations (voir Figure 10) offrant une vision financière synthétique des différentes filiales et participations détenues. Ce tableau est très utile pour consolider le profilage d’un groupe, car il centralise des informations qui sont difficiles à obtenir par ailleurs.\n\n\n\n\n\n\nFigure 10: Exemple d’un tableau des filiales et participations figurant dans les comptes sociaux d’une société.\n\n\n\nAujourd’hui, les profileurs de la DSE utilisent les comptes sociaux de manière manuelle. Ils récupèrent les comptes sociaux, souvent sous la forme de documents scannés, depuis une interface de programmation mise à disposition par l’Institut National de la Propriété Industrielle (INPI) et pour chaque groupe qui les intéresse, cherchent eux-mêmes l’emplacement du tableau des filiales et participations dans le document puis récupèrent les informations pertinentes pour la consolidation. La reconnaissance optique de caractères peut permettre de traiter automatiquement (au moins en partie) les comptes sociaux, ce qui permettrait à la fois de dégager du temps aux profileurs pour des activités à plus forte valeur ajoutée, mais aussi de consolider plus de comptes.\nUne chaîne de traitement complète envisagée pour l’extraction d’un tableaux filiales et participations est décrite ci-dessous :\n\nOn récupère l’exemplaire des comptes sociaux d’intérêt via un appel à l’API de l’INPI ;\nUn document est en général constitué de plusieurs pages. Pour identifier la page sur laquelle se trouve le tableau des filiales et participations, tout le texte de chaque page du document est extrait à l’aide d’un moteur de reconnaissance de caractères. Puis un modèle de forêt aléatoire qui a été entraîné sur des observations annotées à la main prend en entrée la totalité des mots présents sur chaque page, pour renvoyer en sortie une probabilité que le tableau des filiales et participations y soit présent. Pour un document donné, on retient la page avec la probabilité de sortie la plus élevée si cette dernière dépasse un certain seuil fixé empiriquement.\nL’extraction à proprement parler du tableau se fait ensuite en plusieurs étapes :\n\nD’abord l’image est pré-traitée : elle est remise droite dans le cas où le document a été scanné de travers, les couleurs sont inversées si on repère une zone de l’image où du texte blanc figure sur une zone sombre, etc. ;\nOn applique ensuite le modèle de segmentation TableNet (Paliwal et al. 2020) à l’image, qui retourne deux masques : le premier masque indique l’emplacement des tableaux au sein de l’image, et le deuxième indique l’emplacement des colonnes au sein de l’image (voir Figure 11). Ce modèle a été entraîné à partir du jeu de données annotées Marmot disponible en libre accès sur Internet et optionnellement à partir de données supplémentaires des comptes sociaux annotées à la main ;\nLes masques sont post-traités dans l’étape suivante où des artefacts sont retirés, la table et les colonnes sont remplis lorsque des trous apparaissent sur les masques, etc. ;\nLe contenu de chaque colonne est extrait (chaque caractère accompagné de sa position sur l’image) grâce à un moteur de reconnaissance optique de caractères (par exemple Tesseract) ;\nLes colonnes sont alignées pour reconstituer la table aussi bien que possible ;\nOn identifie les colonnes de la table utile pour la consolidation des comptes grâce à l’utilisation d’expressions régulières et d’une distance textuelle ;\nLe tableau avec les noms de colonnes nettoyés est enfin exporté (par exemple en format csv).\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 11: Exemple de masques bruts obtenus en sortie de TableNet. À gauche, le masque indiquant l’emplacement de la table. À droite, le masque indiquant l’emplacement des colonnes.\n\n\n\n\n\nExtraction d’information de tickets de caisse\nL’enquête Budget des Familles, réalisée par la Direction des statistiques démographiques et sociales (DSDS) de l’Insee, repose traditionnellement sur la collecte de tickets de caisse dont les champs sont manuellement repris et numérisés par les enquêteurs1. Toutefois, il existe aujourd’hui des méthodes pour automatiser cette extraction en utilisant des moteurs de reconnaissance optique de caractères.\nUne première idée envisageable est d’utiliser un moteur d’OCR pour récupérer ligne par ligne le texte figurant sur un ticket de caisse puis d’extraire l’information sous forme structurée avec une approche basée sur des règles. Les tickets de caisse se ressemblant en général beaucoup, cette approche fonctionne convenablement sur cette tâche quelque soit le ticket, mais elle présente tout de même des défauts de généralisabilité. Une approche Deep Learning end-to-end est préférable, même si elle nécessite des données annotées. De telles méthodes ont été testées dans le cadre de compétitions (notamment sur les jeux de données SROIE 2019 et Cord) et ont donné de bons résultats.\n\n\nReferences\n\n\nChen, Liang-Chieh, George Papandreou, Florian Schroff, and Hartwig Adam. 2017. “Rethinking Atrous Convolution for Semantic Image Segmentation.” CoRR abs/1706.05587. http://arxiv.org/abs/1706.05587.\n\n\nDirection de la recherche et de l’innovation, Commissariat général au développement durable –. 2018. Plan d’applications Satellitaires 2018 - Des Solutions Spatiales Pour Connaître Le Territoire.\n\n\nLeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. “Backpropagation Applied to Handwritten Zip Code Recognition.” Neural Computation 1 (4): 541–51. https://doi.org/10.1162/neco.1989.1.4.541.\n\n\nPaliwal, Shubham, Vishwanath D, Rohit Rahul, Monika Sharma, and Lovekesh Vig. 2020. “TableNet: Deep Learning Model for End-to-End Table Detection and Tabular Data Extraction from Scanned Document Images.” CoRR abs/2001.01469. http://arxiv.org/abs/2001.01469.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nSobel, Irwin, and Gary Feldman. 1973. “A 3×3 Isotropic Gradient Operator for Image Processing.” Pattern Classification and Scene Analysis, January, 271–72.\n\n\nSteele, Jessica E., Pål Roe Sundsøy, Carla Pezzulo, Victor A. Alegana, Tomas J. Bird, Joshua Blumenstock, Johannes Bjelland, et al. 2017. “Mapping Poverty Using Mobile Phone and Satellite Data.” Journal of The Royal Society Interface 14 (127): 20160690. https://doi.org/10.1098/rsif.2016.0690.\n\n\nVoulodimos, Athanasios, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis. 2018. “Deep Learning for Computer Vision: A Brief Review.” Computational Intelligence and Neuroscience 2018: 1–13. https://doi.org/10.1155/2018/7068349.",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#footnotes",
    "href": "images.html#footnotes",
    "title": "Images",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlus précisément, les ménages enquêtés se voient confier un carnet de dépenses qu’ils doivent remplir pendant une certaine période. Pour certaines dépenses les carnets sont renseignés à la main par un membre du ménage. Pour d’autres, le ménage a la possibilité d’inclure dans le carnet des tickets de caisse. Jusqu’à présent les enquêteurs étaient chargés de recopier le contenu des tickets de caisse pour rendre ces données exploitables.↩︎",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images_exemples.html",
    "href": "images_exemples.html",
    "title": "Application",
    "section": "",
    "text": "L’objectif de ce TP est d’explorer l’exploitation d’images à l’aide de modèles de Deep Learning, avec une application sur les images satellitaires pour la statistique publique.\nLe TP peut être lancé sur le SSP Cloud en cliquant sur le bouton suivant:\n\nLe notebook classification_oiseau.ipynb traite un problème simple de classification d’image, à regarder dans un premier temps. Le notebook donnees_satellite.ipynb traite ensuite un problème de segmentation sémantique sur des images satellitaires, qui permet d’obtenir des cartes de couverture du territoire pouvant être utilisées pour produire des statistiques officielles.\nUn second bouton si l’installation de Pytorch est trop longue:",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Application"
    ]
  },
  {
    "objectID": "applications/data.html",
    "href": "applications/data.html",
    "title": "Exemple basique utilisation geoparquet",
    "section": "",
    "text": "import subprocess\n\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet dvf.parquet\", shell=True)\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet carreaux.parquet\", shell=True)\n\n\nfrom cartiflette import carti_download\n\n# 1. Fonds communaux\ncontours_villes_arrt = carti_download(\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    crs = 4326,\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n# 2. Départements \ndepartements = contours_villes_arrt.dissolve(\"INSEE_DEP\").reset_index()\n#buffer_75 = departements.loc[departements[\"INSEE_DEP\"] == \"75\"].to_crs(2154).buffer(distance = 2000).plot()"
  },
  {
    "objectID": "applications/data.html#la-base-dvf",
    "href": "applications/data.html#la-base-dvf",
    "title": "Exemple basique utilisation geoparquet",
    "section": "",
    "text": "import subprocess\n\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet dvf.parquet\", shell=True)\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet carreaux.parquet\", shell=True)\n\n\nfrom cartiflette import carti_download\n\n# 1. Fonds communaux\ncontours_villes_arrt = carti_download(\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    crs = 4326,\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n# 2. Départements \ndepartements = contours_villes_arrt.dissolve(\"INSEE_DEP\").reset_index()\n#buffer_75 = departements.loc[departements[\"INSEE_DEP\"] == \"75\"].to_crs(2154).buffer(distance = 2000).plot()"
  },
  {
    "objectID": "applications/data.html#exploitation",
    "href": "applications/data.html#exploitation",
    "title": "Exemple basique utilisation geoparquet",
    "section": "Exploitation",
    "text": "Exploitation\n\nimport duckdb\nduckdb.execute(\"INSTALL spatial;\")\nduckdb.execute(\"LOAD spatial;\")\n\n\nreference_lon = 2.35  # Replace with your reference longitude\nreference_lat = 48.853 # Replace with your reference latitude\n\n\ncarreaux_duckdb = duckdb.sql(f\"\"\"\nFROM read_parquet('carreaux_geoparquet.parquet')\nSELECT\n    *, ST_AsText(geometry) AS geom_text\nWHERE st_distance(\n        st_centroid(geometry),\n        ST_Transform(st_point({reference_lat}, {reference_lon}), 'EPSG:4326', 'EPSG:2154')\n    ) / 1000 &lt; 2\n\"\"\")\n\n\ncarreaux = carreaux_duckdb.to_df()\ncarreaux = gpd.GeoDataFrame(carreaux)\ncarreaux = carreaux.drop(\"geometry\", axis = \"columns\")\ncarreaux['geometry'] = gpd.GeoSeries.from_wkt(carreaux['geom_text'])\ncarreaux = carreaux.set_geometry('geometry', crs=2154).drop(\"geom_text\", axis = \"columns\")\n\n\ncarreaux['prop_men_pauv'] = 100*carreaux['men_pauv']/carreaux['men']\n\n\nimport folium\nimport branca.colormap as cm\n\ncarreaux = carreaux.to_crs(4326)\ncentroid = carreaux.geometry.unary_union.centroid\nmap_center = [centroid.y, centroid.x]\n\n# Create a linear colormap\ncolormap = cm.LinearColormap(\n    colors=['green', 'yellow', 'red'],  # Color range\n    vmin=carreaux['prop_men_pauv'].min(),  # Minimum value of the variable\n    vmax=carreaux['prop_men_pauv'].max(),  # Maximum value of the variable\n    caption='Proportion ménages pauvres'  # Legend title\n)\n\ndef style_function(feature):\n    proportion = feature['properties']['prop_men_pauv']\n    return {\n        'fillColor': colormap(proportion),\n        'color': 'black',         # Border color\n        'weight': 1,              # Border thickness\n        'fillOpacity': 0.7,       # Fill transparency\n    }\n\nm = folium.Map(location=map_center, zoom_start=14, tiles='cartodbpositron')\n\n# Add GeoJson layer to the map with tooltips\nfolium.GeoJson(\n    carreaux,\n    name='carreaux Borders',\n    style_function=style_function,\n    tooltip=folium.GeoJsonTooltip(\n        fields=['idcar_200m', 'idcar_1km', 'idcar_nat', 'prop_men_pauv'],\n        aliases=['ID Car 200m:', 'ID Car 1km:', 'ID Car Nat:', 'Proportion ménages pauvres:'],\n        localize=True\n    )\n).add_to(m)\n\n# Add the colormap legend to the map\ncolormap.add_to(m)\n\n# Optional: Add layer control if you have multiple layers\nfolium.LayerControl().add_to(m)\n\n# Display the map\nm\n\n\nduckdb.sql(\"SELECT * FROM read_parquet('dvf_geoparquet.parquet') LIMIT 2\")"
  }
]