[
  {
    "objectID": "applications/application1.html",
    "href": "applications/application1.html",
    "title": "Application 1 - Données spatiales",
    "section": "",
    "text": "Préparation de l’environnement\n\nImport des données dans votre work locale\n\nLes données « Demandes de Valeurs Foncières », ou DVF, qui recensent l’ensemble des ventes de biens fonciers réalisées au cours des dernières années, en métropole et dans les départements et territoires d’outre-mer — sauf à Mayotte et en Alsace-Moselle. Les biens concernés peuvent être bâtis (appartement et maison) ou non bâtis (parcelles et exploitations). Les données sont produites par la Direction générale des finances publiques. Elles proviennent des actes enregistrés chez les notaires et des informations contenues dans le cadastre. Cette base a été filtrée de manière à être la plus pédagogique possible pour cette formation.\nLes données spatiales carroyées à 200m, produites par l’Insee à partir du dispositif Filosofi, contentant des informations socio-économiques sur les ménages.\n3 contours géographiques :\n\nLa commune de Malakoff\nLa commune de Montrouge\nLe “Triangle d’or” de Malakoff (autrement dit, son centre-ville à peu de choses près)\n\n\n\nsystem(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet dvf.parquet\")\nsystem(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet carreaux.parquet\")\nsystem(\"mc cp s3/projet-formation/nouvelles-sources/data/triangle.geojson triangle.geojson\")\nsystem(\"mc cp s3/projet-formation/nouvelles-sources/data/malakoff.geojson malakoff.geojson\")\nsystem(\"mc cp s3/projet-formation/nouvelles-sources/data/montrouge.geojson montrouge.geojson\")\n\n\n\nChargement des libraries nécessaires :\n\n\nVariables globales pour l’applications :\n\ncog_malakoff &lt;- \"92046\"\ncog_montrouge &lt;- \"92049\"\n\n\n\nImport et visualisation des contours géographiques :\n\ntriangle &lt;- sf::st_read(\"triangle.geojson\", quiet=TRUE)\nmalakoff &lt;- sf::st_read(\"malakoff.geojson\", quiet=TRUE)\nmontrouge &lt;- sf::st_read(\"montrouge.geojson\", quiet=TRUE)\n\nmapview(malakoff) + mapview(triangle, col.regions = \"#ffff00\")\n\n\n\n\nmapview(montrouge)\n\n\n\n\n\n\n\nPréparation de DuckDB\nDuckDB est un moteur de base de données analytique en mémoire, optimisé pour les requêtes SQL sur des données volumineuses, particulièrement adapté aux fichiers plats comme Parquet ou CSV, et intégrable dans des langages comme Python, R ou SQL.\nOn commence par :\n\nCréer une connexion DuckDB\nInstaller et charger les extensions spatiales (car DVF et les données carroyées sont des données spatiales).\n\n\n\n[1] 0\n\n\n[1] 0\n\n\n\n\n\nPartie 1 : Prix immobiliers à Malakoff et à Montrouge\nDans cette partie, l’objectif est d’extraire de l’informations d’une base de données volumineuse à l’aide de DuckDB. Pour le moment, le caractère spatial des données est mis de côté : on découvre et on traite les données via des requêtes attributaires classiques.\nTentons de comparer la médiane des prix des transactions immobilières à Malakoff et à Montrouge.\n\nDescription des données DVF\nTout d’abord, il convient de se familiariser avec les données. Les requêtes ci-dessous permettent d’obtenir des informations primordiale de manière très rapide et sans nécessité de charger l’ensemble des données dans la mémoire vive.\nDescription des variables :\n\ndescribe_dvf &lt;- dbGetQuery(con, \"DESCRIBE SELECT * FROM read_parquet('dvf.parquet')\")\nprint(describe_dvf)\n\n                    column_name column_type null  key default extra\n1                   id_mutation     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n2                 date_mutation     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n3            numero_disposition      BIGINT  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n4               nature_mutation     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n5               valeur_fonciere      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n6                adresse_numero      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n7               adresse_suffixe     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n8              adresse_nom_voie     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n9             adresse_code_voie     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n10                  code_postal      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n11                 code_commune     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n12                  nom_commune     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n13             code_departement     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n14          ancien_code_commune      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n15           ancien_nom_commune     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n16                  id_parcelle     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n17           ancien_id_parcelle     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n18                numero_volume     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n19                  lot1_numero     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n20          lot1_surface_carrez      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n21                  lot2_numero     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n22          lot2_surface_carrez      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n23                  lot3_numero     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n24          lot3_surface_carrez      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n25                  lot4_numero      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n26          lot4_surface_carrez      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n27                  lot5_numero      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n28          lot5_surface_carrez      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n29                  nombre_lots      BIGINT  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n30              code_type_local      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n31                   type_local     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n32          surface_reelle_bati      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n33    nombre_pieces_principales      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n34          code_nature_culture     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n35               nature_culture     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n36 code_nature_culture_speciale     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n37      nature_culture_speciale     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n38              surface_terrain      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n39                    longitude      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n40                     latitude      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n41                     geometry    GEOMETRY  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n42            __index_level_0__      BIGINT  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n\n\nVisualisation des 10 premières lignes :\n\npreview &lt;- dbGetQuery(con, \"SELECT * FROM read_parquet('dvf.parquet') LIMIT 10\")\nprint(preview)\n\n   id_mutation date_mutation numero_disposition nature_mutation valeur_fonciere\n1       2022-1    2022-01-03                  1           Vente           55000\n2       2022-2    2022-01-03                  1           Vente          143000\n3       2022-4    2022-01-06                  1           Vente          255000\n4       2022-5    2022-01-03                  1           Vente          525000\n5       2022-7    2022-01-05                  1           Vente           64000\n6       2022-8    2022-01-03                  1           Vente          140000\n7       2022-9    2022-01-04                  1           Vente          134000\n8      2022-10    2022-01-10                  1           Vente          580000\n9      2022-11    2022-01-04                  1           Vente          138000\n10     2022-12    2022-01-05                  1           Vente          176000\n   adresse_numero adresse_suffixe       adresse_nom_voie adresse_code_voie\n1              13            &lt;NA&gt;      RUE DE LA LIBERTE              2280\n2              98            &lt;NA&gt;       RTE DE LA DOMBES              0055\n3             282            &lt;NA&gt;        RTE DE POISATON              0130\n4             217            &lt;NA&gt; PL DE LA CROIX BLANCHE              0300\n5              12            &lt;NA&gt;             BD DE BROU              0620\n6               6            &lt;NA&gt;       RUE DE BEAUGENCY              0397\n7              13            &lt;NA&gt;          RUE DE CUIRON              1070\n8              45            &lt;NA&gt;         RUE DES COMBES              0119\n9               2            &lt;NA&gt;          RUE JEAN MACE              0394\n10            464            &lt;NA&gt;            RUE DU BREU              0033\n   code_postal code_commune          nom_commune code_departement\n1         1000        01053      Bourg-en-Bresse               01\n2         1480        01398            Savigneux               01\n3         1560        01230     Mantenay-Montlin               01\n4         1390        01333 Saint-André-de-Corcy               01\n5         1000        01053      Bourg-en-Bresse               01\n6         1000        01053      Bourg-en-Bresse               01\n7         1000        01053      Bourg-en-Bresse               01\n8         1500        01007             Ambronay               01\n9         1500        01004    Ambérieu-en-Bugey               01\n10        1710        01419               Thoiry               01\n   ancien_code_commune ancien_nom_commune    id_parcelle ancien_id_parcelle\n1                   NA               &lt;NA&gt; 01053000AM0102               &lt;NA&gt;\n2                   NA               &lt;NA&gt; 01398000ZE0187               &lt;NA&gt;\n3                   NA               &lt;NA&gt; 01230000ZM0124               &lt;NA&gt;\n4                   NA               &lt;NA&gt; 01333000AN0186               &lt;NA&gt;\n5                   NA               &lt;NA&gt; 01053000BC0223               &lt;NA&gt;\n6                   NA               &lt;NA&gt; 01053000BZ0175               &lt;NA&gt;\n7                   NA               &lt;NA&gt; 01053000CS0321               &lt;NA&gt;\n8                   NA               &lt;NA&gt; 01007000ZN0521               &lt;NA&gt;\n9                   NA               &lt;NA&gt; 01004000AN0491               &lt;NA&gt;\n10                  NA               &lt;NA&gt; 01419000BN0164               &lt;NA&gt;\n   numero_volume lot1_numero lot1_surface_carrez lot2_numero\n1           &lt;NA&gt;         7.0               24.10        &lt;NA&gt;\n2           &lt;NA&gt;         1.0              123.23        &lt;NA&gt;\n3           &lt;NA&gt;        &lt;NA&gt;                  NA        &lt;NA&gt;\n4           &lt;NA&gt;        &lt;NA&gt;                  NA        &lt;NA&gt;\n5           &lt;NA&gt;         7.0               39.05        &lt;NA&gt;\n6           &lt;NA&gt;        &lt;NA&gt;                  NA        &lt;NA&gt;\n7           &lt;NA&gt;         4.0               70.02        &lt;NA&gt;\n8           &lt;NA&gt;        &lt;NA&gt;                  NA        &lt;NA&gt;\n9           &lt;NA&gt;        16.0                5.06         3.0\n10          &lt;NA&gt;        10.0               39.73        &lt;NA&gt;\n   lot2_surface_carrez lot3_numero lot3_surface_carrez lot4_numero\n1                   NA        &lt;NA&gt;                  NA          NA\n2                   NA        &lt;NA&gt;                  NA          NA\n3                   NA        &lt;NA&gt;                  NA          NA\n4                   NA        &lt;NA&gt;                  NA          NA\n5                   NA        &lt;NA&gt;                  NA          NA\n6                   NA        &lt;NA&gt;                  NA          NA\n7                   NA        &lt;NA&gt;                  NA          NA\n8                   NA        &lt;NA&gt;                  NA          NA\n9                   NA         8.0               64.92          NA\n10                  NA        &lt;NA&gt;                  NA          NA\n   lot4_surface_carrez lot5_numero lot5_surface_carrez nombre_lots\n1                   NA          NA                  NA           1\n2                   NA          NA                  NA           1\n3                   NA          NA                  NA           0\n4                   NA          NA                  NA           0\n5                   NA          NA                  NA           1\n6                   NA          NA                  NA           0\n7                   NA          NA                  NA           1\n8                   NA          NA                  NA           0\n9                   NA          NA                  NA           3\n10                  NA          NA                  NA           1\n   code_type_local  type_local surface_reelle_bati nombre_pieces_principales\n1                2 Appartement                  24                         1\n2                2 Appartement                 140                         3\n3                1      Maison                 108                         5\n4                2 Appartement                 126                         4\n5                2 Appartement                 117                         2\n6                1      Maison                 100                         4\n7                2 Appartement                  46                         1\n8                1      Maison                 204                         6\n9                2 Appartement                  67                         4\n10               2 Appartement                  39                         2\n   code_nature_culture nature_culture code_nature_culture_speciale\n1                 &lt;NA&gt;           &lt;NA&gt;                         &lt;NA&gt;\n2                 &lt;NA&gt;           &lt;NA&gt;                         &lt;NA&gt;\n3                    S           sols                         &lt;NA&gt;\n4                    S           sols                         &lt;NA&gt;\n5                 &lt;NA&gt;           &lt;NA&gt;                         &lt;NA&gt;\n6                    S           sols                         &lt;NA&gt;\n7                 &lt;NA&gt;           &lt;NA&gt;                         &lt;NA&gt;\n8                    S           sols                         &lt;NA&gt;\n9                 &lt;NA&gt;           &lt;NA&gt;                         &lt;NA&gt;\n10                &lt;NA&gt;           &lt;NA&gt;                         &lt;NA&gt;\n   nature_culture_speciale surface_terrain longitude latitude\n1                     &lt;NA&gt;              NA  5.218712 46.19805\n2                     &lt;NA&gt;              NA  4.848340 46.00063\n3                     &lt;NA&gt;             649  5.103407 46.42235\n4                     &lt;NA&gt;             628  4.951266 45.92651\n5                     &lt;NA&gt;              NA  5.229155 46.20498\n6                     &lt;NA&gt;             796  5.250556 46.20959\n7                     &lt;NA&gt;              NA  5.239453 46.19144\n8                     &lt;NA&gt;             496  5.343444 45.99513\n9                     &lt;NA&gt;              NA  5.345336 45.95979\n10                    &lt;NA&gt;              NA  5.980102 46.23238\n                                                                                                                         geometry\n1  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, c5, 65, 3c, 51, 40, 95, 2a, 41, 91, 43, 1e, d8, e9, 0e, 59, 41\n2  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, f2, 6f, 41, 8f, 33, ba, 29, 41, 97, f7, d5, 83, ca, f8, 58, 41\n3  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 57, 3b, c7, bf, a0, 4a, 2a, 41, 83, 31, 58, f2, fd, 26, 59, 41\n4  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, d4, 19, 56, 04, 01, fa, 29, 41, 52, 2a, e2, 49, f1, f0, 58, 41\n5  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, ea, eb, 83, d6, 5e, 9b, 2a, 41, bf, 9e, 89, dc, af, 0f, 59, 41\n6  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 45, 9b, b7, 9d, 24, a8, 2a, 41, d6, 1e, 4e, 46, 3b, 10, 59, 41\n7  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 04, 30, 51, 8a, e7, a1, 2a, 41, 19, b8, 04, aa, 3d, 0e, 59, 41\n8  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, aa, 52, ed, f3, 96, e5, 2a, 41, fc, f3, 29, 02, 30, f9, 58, 41\n9  00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 1a, 9c, cd, c4, a4, e7, 2a, 41, 27, 01, 7a, 89, 5c, f5, 58, 41\n10 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 00, 01, 00, 00, 00, 38, b5, 7b, de, 94, 5e, 2c, 41, 4f, f4, 98, 3f, 85, 14, 59, 41\n   __index_level_0__\n1                  0\n2                  3\n3                  5\n4                  8\n5                 12\n6                 13\n7                 18\n8                 23\n9                 29\n10                32\n\n\nQue contient le champ nature_mutation ? (il a été filtré au ventes classiques pour simplifié cette application ; les vraies données sont plus riches).\n\ndbGetQuery(con, \"SELECT DISTINCT nature_mutation FROM read_parquet('dvf.parquet')\")\n\n  nature_mutation\n1           Vente\n\n\nVérification des bornes min et max des prix des transactions :\n\ndbGetQuery(con, \"\n        SELECT\n            MIN(valeur_fonciere) AS min_valeur,\n            MAX(valeur_fonciere) AS max_valeur\n        FROM read_parquet('dvf.parquet')\n           \")\n\n  min_valeur max_valeur\n1    10000.5    9970000\n\n\nRequête de calcul des prix médians des transactions à Malakoff et à Montrouge :\n\nFiltre des seules transactions effectuées à Montrouge ou Malakoff (requête attributaire, pas de traitement spatial ici).\nCalculs groupés de la médiane par communes des montants des transactions\n\n\nquery1 &lt;- glue(\"\n    FROM read_parquet('dvf.parquet')\n    SELECT\n        code_commune,\n        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY valeur_fonciere) AS mediane_valeur_fonciere\n    WHERE code_commune IN ('{cog_malakoff}', '{cog_montrouge}')\n    GROUP BY code_commune\n\")\n\nresult1 &lt;- dbGetQuery(con, query1)\nprint(result1)\n\n  code_commune mediane_valeur_fonciere\n1        92046                  375000\n2        92049                  394500\n\n\nConclusion : les biens vendus à Montrouge (dans notre base) ont une médiane un peu plus élevée qu’à Malakoff.\n\n\n\nPartie 2 : les prix immobiliers à Malakoff, dans le centre et en-dehors.\nÀ présent, nous souhaitons avoir des informations sur les transactions effectuées dans le « fameux » Triangle d’Or de Malakoff (plus prosaïquement, dans son centre-ville commerçant).\nComme il n’est pas possible de distringuer cette zone par requêtes attributaires, nous proposons de :\n\nVia DuckDB, extraire les transactions de l’ensemble de la commune de Malakoff tout en conservant leur caractère spatial (chaque transaction correspond à un point géographique, avec ses coordonnées xy).\nUtiliser localement le package sf pour distinguer spatialement les transactions effectuées à l’intérieur ou à l’extérieur du Triangle d’Or (dont nous fournissons les contours).\nCalculer la médiane des prix dans les deux sous-zones.\n\nOn extrait les transactions de Malakoff. Pour information, dans le fichier dvf.parquet, les coordonnées spatiales sont stockées dans un format binaire spécifique (Well-Known Binary - WKB). Ce format est efficace pour le stockage et les calculs, mais n’est pas directement lisible ou interprétable par les humains.\nEn transformant ces géométries en une représentation texte lisible (Well-Known Text - WKT) avec ST_AsText, on rend les données spatiales faciles à afficher, interpréter ou manipuler dans des contextes qui ne supportent pas directement les formats binaires géospatiaux.\n\nquery2 &lt;- glue(\"\n    FROM read_parquet('dvf.parquet')\n    SELECT\n        code_commune,\n        valeur_fonciere,\n        ST_AsText(geometry) AS geom_text\n    WHERE code_commune = '{cog_malakoff}'\n\")\n\ntransactions_malakoff &lt;- dbGetQuery(con, query2)\n\ntransactions_malakoff\n\n    code_commune valeur_fonciere                                    geom_text\n1          92046        255000.0  POINT (647793.8565435864 6857548.428887248)\n2          92046       1269230.0  POINT (648887.7235760115 6858397.446977629)\n3          92046       1160000.0  POINT (648007.1430323087 6857445.769340179)\n4          92046        233000.0  POINT (648674.8575409041 6857442.155413407)\n5          92046        356000.0  POINT (648756.6646625624 6857824.388765166)\n6          92046        885000.0  POINT (648871.7319390004 6858374.125627969)\n7          92046        155300.0  POINT (647062.4229518737 6857159.093996111)\n8          92046        295285.7  POINT (648967.5965979596 6857945.732999014)\n9          92046       1000960.0  POINT (648370.9451319632 6858006.947035593)\n10         92046        159000.0   POINT (648610.829114889 6857865.487582879)\n11         92046        980000.0  POINT (648964.8823868241 6858262.998443531)\n12         92046        256000.0  POINT (648885.2926251788 6857805.239646839)\n13         92046        680000.0  POINT (647309.0279143106 6857174.861459553)\n14         92046        530800.0  POINT (648738.3752642459 6857749.048677924)\n15         92046        220000.0   POINT (648986.6027044706 6857894.30455799)\n16         92046       1220000.0   POINT (648614.816113362 6857362.515462328)\n17         92046       1078500.0   POINT (647778.7922156318 6857636.96498155)\n18         92046        803200.0  POINT (649005.1354911198 6857722.233265874)\n19         92046        390000.0   POINT (648731.947832041 6858051.891219128)\n20         92046        140335.0  POINT (648896.8056501688 6858294.955391401)\n21         92046        181950.0  POINT (649218.3073049119 6857831.224107595)\n22         92046        175000.0   POINT (648625.105372882 6858084.416623066)\n23         92046        278000.0  POINT (649164.7842524194 6858250.789705052)\n24         92046       1325000.0  POINT (649071.4650474399 6858326.443599923)\n25         92046        470000.0 POINT (648998.3192671002 6857648.5706718685)\n26         92046        275000.0  POINT (648839.8413069112 6857888.703623221)\n27         92046        282000.0   POINT (648610.829114889 6857865.487582879)\n28         92046       1343250.0 POINT (648658.8734144163 6857718.6173969535)\n29         92046        420000.0  POINT (649291.7416860285 6858318.619294695)\n30         92046       1840000.0   POINT (649286.7108355202 6858322.66635421)\n31         92046        475650.0   POINT (648637.8451939381 6857516.65041106)\n32         92046        425000.0  POINT (647976.2255411559 6857347.416424943)\n33         92046        310000.0  POINT (648889.8064628252 6857867.469362068)\n34         92046        188125.0   POINT (648920.4501951905 6858411.83640823)\n35         92046        318500.0  POINT (648676.3085346507 6858045.711760148)\n36         92046        399000.0  POINT (647807.9923122416 6857585.107331346)\n37         92046        309773.0  POINT (649218.3073049119 6857831.224107595)\n38         92046        430175.0  POINT (647976.2255411559 6857347.416424943)\n39         92046        200000.0  POINT (648141.8653281827 6857245.189777694)\n40         92046        295000.0   POINT (647389.814585229 6857231.949427107)\n41         92046        256900.0 POINT (647957.1056395257 6857499.9256156925)\n42         92046        780000.0 POINT (649355.3485889503 6858186.6297907885)\n43         92046        360000.0   POINT (649100.762058387 6858318.180194477)\n44         92046        550000.0  POINT (648733.0088398684 6857524.147836505)\n45         92046        343000.0   POINT (647542.997978782 6857320.296163094)\n46         92046        324000.0  POINT (648441.3379093816 6857888.564328602)\n47         92046        260000.0   POINT (649315.9601258913 6858283.71438975)\n48         92046        300000.0  POINT (648756.2926558497 6858197.676111581)\n49         92046        275000.0  POINT (648019.1266459919 6857463.675541215)\n50         92046        195000.0  POINT (648793.5408933193 6857767.798016141)\n51         92046        263000.0  POINT (649201.3176449259 6858087.901215131)\n52         92046        700000.0  POINT (648045.2124872584 6857752.327607776)\n53         92046        205000.0  POINT (648638.4736858582 6857571.019360576)\n54         92046        365000.0   POINT (648821.4369165015 6857417.50876028)\n55         92046       1227700.0  POINT (649162.4393878599 6857958.254259863)\n56         92046        430220.0  POINT (648852.8578339639 6858165.577332498)\n57         92046        275000.0 POINT (648823.4408574156 6857686.1389376065)\n58         92046       1147350.0 POINT (648477.0249913045 6857541.5394775085)\n59         92046        490000.0  POINT (648139.8097187788 6857672.198255488)\n60         92046        457500.0   POINT (648868.960776704 6857860.091865977)\n61         92046        800000.0   POINT (649026.6737676512 6858351.18917843)\n62         92046        663020.0   POINT (649363.384193169 6858207.130821841)\n63         92046        808000.0  POINT (648112.2224967317 6857280.926224804)\n64         92046        400000.0  POINT (648728.3029935603 6858270.756916526)\n65         92046        262500.0  POINT (648674.8575409041 6857442.155413407)\n66         92046        435000.0  POINT (648638.4736858582 6857571.019360576)\n67         92046        360000.0  POINT (648837.5791385439 6858239.768606471)\n68         92046        467800.0  POINT (648517.2004841545 6857889.669316063)\n69         92046        550000.0  POINT (649211.1484145125 6858128.846313488)\n70         92046        115000.0   POINT (649058.3784240817 6858132.29938791)\n71         92046        210000.0  POINT (648123.5878842617 6857475.972225276)\n72         92046        280000.0   POINT (648821.4369165015 6857417.50876028)\n73         92046        325000.0  POINT (648308.9248206606 6857260.377344046)\n74         92046        430000.0  POINT (648327.6317740963 6857357.061601076)\n75         92046        526500.0   POINT (648914.4205812635 6857777.51763156)\n76         92046        218000.0  POINT (648973.3773900536 6857960.137551364)\n77         92046        180000.0  POINT (649283.8308690797 6858270.429476408)\n78         92046        850000.0  POINT (649056.5778252977 6858094.508647777)\n79         92046        580000.0 POINT (648788.2219576862 6858179.8250252465)\n80         92046        289590.0  POINT (648141.8653281827 6857245.189777694)\n81         92046        228500.0  POINT (648667.1165070973 6857347.485636481)\n82         92046        375000.0  POINT (649018.8614910963 6858038.908733208)\n83         92046        347000.0  POINT (648857.1425729821 6857685.730365172)\n84         92046        206000.0   POINT (647830.5613990374 6857571.56075624)\n85         92046        420000.0  POINT (648738.3752642459 6857749.048677924)\n86         92046        736300.0  POINT (648540.7512678353 6857349.606500061)\n87         92046        277210.0  POINT (648789.2977776062 6857869.357031069)\n88         92046        280000.0   POINT (649058.3784240817 6858132.29938791)\n89         92046        187000.0  POINT (648064.4010161356 6857533.545312459)\n90         92046        204465.0  POINT (648327.6317740963 6857357.061601076)\n91         92046        810900.0  POINT (648805.9968693595 6857889.447153892)\n92         92046        450000.0  POINT (648676.3085346507 6858045.711760148)\n93         92046        240000.0  POINT (648805.0102872341 6858126.969918038)\n94         92046        570000.0    POINT (648705.3444505826 6857700.7483875)\n95         92046        530000.0  POINT (648850.0533148871 6857498.095169536)\n96         92046        200000.0  POINT (649204.1802099914 6857836.351658838)\n97         92046         59800.0   POINT (648868.960776704 6857860.091865977)\n98         92046        990000.0  POINT (647499.0195640497 6857280.886283472)\n99         92046        160000.0  POINT (647868.6701329185 6857522.180790252)\n100        92046        396500.0   POINT (648625.105372882 6858084.416623066)\n101        92046        447500.0  POINT (648638.4736858582 6857571.019360576)\n102        92046        263000.0  POINT (648244.8348832976 6857345.012603398)\n103        92046        387000.0  POINT (648850.0533148871 6857498.095169536)\n104        92046        600445.0  POINT (648199.4300481076 6857465.953746757)\n105        92046        320000.0  POINT (648627.1853143852 6857390.983019724)\n106        92046        306000.0  POINT (648253.5860489546 6857297.009368011)\n107        92046        970000.0  POINT (648683.1091345731 6857851.726287753)\n108        92046        503500.0  POINT (648571.3124345234 6857491.665249509)\n109        92046        205500.0  POINT (648981.7409996054 6857909.247547192)\n110        92046        228500.0 POINT (648598.0913474265 6858118.3484089915)\n111        92046        800000.0  POINT (647136.7811021408 6857139.067643004)\n112        92046        222500.0  POINT (648738.3752642459 6857749.048677924)\n113        92046        767450.0  POINT (647785.1800145854 6857571.635678657)\n114        92046        260000.0  POINT (648655.3145567531 6857888.889314086)\n115        92046        589800.0    POINT (648842.26847136 6858055.586920664)\n116        92046       1030000.0  POINT (648662.5793102679 6857863.472379892)\n117        92046        345000.0  POINT (648570.8298081184 6857263.385796428)\n118        92046        160000.0  POINT (648622.1500261321 6858289.488063858)\n119        92046        330477.0  POINT (648852.8578339639 6858165.577332498)\n120        92046        483000.0  POINT (648719.4835037439 6858045.663296326)\n121        92046        372000.0  POINT (648847.5940616168 6858317.962182348)\n122        92046        210200.0  POINT (648667.1165070973 6857347.485636481)\n123        92046        515000.0  POINT (648850.0533148871 6857498.095169536)\n124        92046        340000.0  POINT (648850.0617425455 6857823.675229571)\n125        92046        395000.0  POINT (648701.7505466867 6857609.933525973)\n126        92046       8000000.0  POINT (648993.6900013619 6857581.449411331)\n127        92046        166500.0  POINT (647840.2187721037 6857649.199538395)\n128        92046        723500.0  POINT (649231.9215223892 6858027.476270867)\n129        92046        750000.0  POINT (648976.3685187113 6857657.548237317)\n130        92046        234000.0  POINT (648822.8710234208 6857862.833627273)\n131        92046        220000.0  POINT (649115.9075986168 6857693.016405692)\n132        92046        485000.0  POINT (648769.1196886537 6858170.208560413)\n133        92046        200000.0  POINT (649090.0735370005 6858179.612777848)\n134        92046        142000.0  POINT (648725.1659040925 6857708.467936824)\n135        92046        365000.0  POINT (648253.5860489546 6857297.009368011)\n136        92046        260000.0   POINT (648939.1372006665 6858382.76081476)\n137        92046        870000.0  POINT (648869.2223644454 6858206.130668555)\n138        92046        295000.0   POINT (648939.1372006665 6858382.76081476)\n139        92046        579000.0  POINT (648806.3685044717 6858180.999106071)\n140        92046        494700.0   POINT (648640.8973599158 6857985.53486181)\n141        92046        135000.0   POINT (648920.4501951905 6858411.83640823)\n142        92046        477850.0  POINT (649111.7097755716 6858076.455533053)\n143        92046       1430000.0  POINT (648835.4789673199 6858359.545203149)\n144        92046        390000.0  POINT (648439.8801454629 6857237.417144276)\n145        92046        433500.0  POINT (648472.6226718617 6857880.724895322)\n146        92046       1514165.0   POINT (648637.800898405 6858323.931340848)\n147        92046        267000.0  POINT (649263.5342242402 6858006.739532214)\n148        92046        310000.0  POINT (649295.6702760116 6858196.936661265)\n149        92046        115000.0  POINT (647138.8010902915 6857110.916840643)\n150        92046       4114500.0  POINT (648936.2868413572 6858184.189969288)\n151        92046        307100.0  POINT (649218.3073049119 6857831.224107595)\n152        92046        502000.0  POINT (648540.7512678353 6857349.606500061)\n153        92046        450510.0  POINT (648475.4850291172 6857566.572119682)\n154        92046       1725810.1  POINT (648739.9350008911 6858257.644127853)\n155        92046        220000.0  POINT (648656.5508645539 6858152.857082505)\n156        92046        685200.0  POINT (648540.7512678353 6857349.606500061)\n157        92046       1312000.0  POINT (648968.3460731722 6857755.582010028)\n158        92046        320000.0  POINT (649115.9075986168 6857693.016405692)\n159        92046        457750.0  POINT (647132.4532310887 6857220.168441692)\n160        92046        675000.0    POINT (648942.9485438883 6858206.9264486)\n161        92046       1493100.0  POINT (648819.5560504808 6858194.337370638)\n162        92046        853000.0  POINT (647290.3790878529 6857223.845666437)\n163        92046        850000.0  POINT (648952.1778056159 6858087.643258516)\n164        92046        162000.0  POINT (648489.2033060552 6857887.360493669)\n165        92046        155000.0  POINT (648701.7505466867 6857609.933525973)\n166        92046        180500.0   POINT (649058.3784240817 6858132.29938791)\n167        92046        385100.0  POINT (646848.3377431062 6857230.104753369)\n168        92046        332150.0   POINT (648868.960776704 6857860.091865977)\n169        92046        110000.0  POINT (646817.3248578722 6857299.775039599)\n170        92046        180000.0  POINT (648064.4010161356 6857533.545312459)\n171        92046        420000.0  POINT (648627.1853143852 6857390.983019724)\n172        92046        313800.0  POINT (648839.8413069112 6857888.703623221)\n173        92046        328000.0   POINT (648591.647532882 6858112.067381612)\n174        92046        950000.0  POINT (648589.6960826326 6857949.961249605)\n175        92046        750000.0  POINT (648545.5246339716 6857283.403027889)\n176        92046        260200.0  POINT (648918.493747101 6857689.7485299315)\n177        92046        834950.0  POINT (647320.8234581281 6857228.350391503)\n178        92046        257500.0 POINT (648886.9702178618 6858112.1248790575)\n179        92046        540000.0  POINT (648892.0211952404 6858251.964739535)\n180        92046        530000.0  POINT (648532.2289982246 6857605.653697633)\n181        92046        762400.0   POINT (649359.2852898339 6858166.58016212)\n182        92046        288000.0   POINT (649051.546943132 6858340.851696438)\n183        92046        375000.0  POINT (648540.7512678353 6857349.606500061)\n184        92046        376000.0  POINT (648715.9307936946 6857876.454881754)\n185        92046        546300.0  POINT (647422.4048362639 6857205.856522966)\n186        92046        190000.0  POINT (647056.4231141229 6857081.089734482)\n187        92046        635000.0  POINT (649338.0084681726 6858235.707452644)\n188        92046        123000.0  POINT (648622.1500261321 6858289.488063858)\n189        92046        332000.0  POINT (649231.7186773627 6857828.215572633)\n190        92046        210000.0  POINT (647884.9559399559 6857577.520919721)\n191        92046        376000.0  POINT (648686.4358819125 6858318.830421146)\n192        92046        750000.0 POINT (647469.3713766701 6857283.1563239675)\n193        92046        751550.0  POINT (648446.1887577414 6857492.665237582)\n194        92046        475000.0  POINT (648733.0088398684 6857524.147836505)\n195        92046        641000.0  POINT (648327.6317740963 6857357.061601076)\n196        92046        265000.0 POINT (648886.9702178618 6858112.1248790575)\n197        92046        213000.0   POINT (648920.4501951905 6858411.83640823)\n198        92046        205000.0  POINT (648622.1500261321 6858289.488063858)\n199        92046        751500.0  POINT (648043.1367993946 6857700.973845492)\n200        92046        391000.0  POINT (647948.3721789496 6857492.331529673)\n201        92046        305000.0  POINT (649176.2649504811 6857843.379224875)\n202        92046        678000.0  POINT (648540.7512678353 6857349.606500061)\n203        92046        301650.0   POINT (648868.960776704 6857860.091865977)\n204        92046        625000.0  POINT (648382.2457683148 6857288.523706433)\n205        92046        690700.0  POINT (648197.7853842956 6857314.742878021)\n206        92046        662500.0    POINT (648705.3444505826 6857700.7483875)\n207        92046       1580000.0  POINT (649434.9580332411 6858154.800079054)\n208        92046        250000.0  POINT (648889.8064628252 6857867.469362068)\n209        92046        444000.0  POINT (648738.3752642459 6857749.048677924)\n210        92046        365310.0 POINT (648263.6650557676 6857266.1183687635)\n211        92046        232000.0  POINT (648926.7130348864 6858022.595635265)\n212        92046        265000.0  POINT (649330.2463870875 6857935.658088959)\n213        92046        132000.0  POINT (647051.7196936611 6857193.328669022)\n214        92046        145000.0  POINT (647132.4532310887 6857220.168441692)\n215        92046        459500.0  POINT (648327.6317740963 6857357.061601076)\n216        92046        743000.0  POINT (648357.4923906225 6857683.709915089)\n217        92046         85950.0  POINT (648638.4736858582 6857571.019360576)\n218        92046        222000.0   POINT (648878.321925859 6858163.907189131)\n219        92046        218000.0 POINT (648927.0100060462 6858281.3457058575)\n220        92046        630000.0  POINT (649053.4420801804 6858222.300189867)\n221        92046       1133400.0   POINT (648008.8993370642 6857723.51987576)\n222        92046        380160.0 POINT (648263.6650557676 6857266.1183687635)\n223        92046        396000.0  POINT (648837.5791385439 6858239.768606471)\n224        92046        283291.0  POINT (648668.6781096942 6857573.642954695)\n225        92046        182000.0   POINT (648423.3609427738 6857609.40100587)\n226        92046        170000.0  POINT (648915.7682187437 6858422.107701227)\n227        92046        541250.0  POINT (648549.7233291205 6858170.261145249)\n228        92046        130000.0  POINT (648489.2033060552 6857887.360493669)\n229        92046        248000.0   POINT (648186.3362034896 6857660.66273053)\n230        92046        296860.0  POINT (649467.8271563016 6858118.152487754)\n231        92046        361150.0  POINT (648805.0102872341 6858126.969918038)\n232        92046        385000.0  POINT (648889.8064628252 6857867.469362068)\n233        92046        282000.0  POINT (648866.1960694728 6858179.359154897)\n234        92046        340000.0  POINT (648967.5965979596 6857945.732999014)\n235        92046        232400.0  POINT (649283.8308690797 6858270.429476408)\n236        92046        337500.0  POINT (648675.2056978745 6857581.035256968)\n237        92046        800100.0  POINT (648187.5894370808 6857135.253857177)\n238        92046        131000.0  POINT (647051.6917541078 6857326.985722946)\n239        92046        130000.0  POINT (649249.3481587956 6858256.720559735)\n240        92046        480000.0  POINT (647113.4630399247 6857255.701726799)\n241        92046        499000.0  POINT (648540.7512678353 6857349.606500061)\n242        92046       1000000.0  POINT (648439.8801454629 6857237.417144276)\n243        92046        870000.0  POINT (648031.4943697476 6857434.320300825)\n244        92046        856000.0  POINT (647334.5435889064 6857154.169733983)\n245        92046        680000.0  POINT (649188.5523661779 6858131.045833966)\n246        92046        385000.0  POINT (648466.9129502296 6857783.590675344)\n247        92046        180000.0  POINT (647840.2187721037 6857649.199538395)\n248        92046        502000.0  POINT (647539.6181161851 6857263.283559329)\n249        92046        317500.0  POINT (648466.8928207515 6857847.417129061)\n250        92046       1153847.0  POINT (648960.7842283559 6857755.759768379)\n251        92046        377000.0  POINT (648998.3506298378 6858002.728139267)\n252        92046        265000.0  POINT (648847.5940616168 6858317.962182348)\n253        92046        250000.0   POINT (649051.546943132 6858340.851696438)\n254        92046        519000.0  POINT (648228.7360118822 6857162.240318559)\n255        92046        745000.0  POINT (647533.1913883415 6857299.702564755)\n256        92046        239000.0  POINT (648695.8135176771 6857785.452491249)\n257        92046        258000.0  POINT (648924.0571849649 6858212.875038971)\n258        92046        249000.0  POINT (648725.1659040925 6857708.467936824)\n259        92046        442500.0  POINT (648733.0088398684 6857524.147836505)\n260        92046        369000.0  POINT (648850.0533148871 6857498.095169536)\n261        92046        646000.0   POINT (648868.960776704 6857860.091865977)\n262        92046        275000.0  POINT (648674.8575409041 6857442.155413407)\n263        92046        279000.0  POINT (647948.3721789496 6857492.331529673)\n264        92046       1440000.0  POINT (648396.5880749924 6857637.549283861)\n265        92046        200000.0 POINT (649154.5007568196 6857990.7930006515)\n266        92046       1500000.0   POINT (648800.864769796 6857765.620621859)\n267        92046        277000.0   POINT (648868.960776704 6857860.091865977)\n268        92046       1407000.0  POINT (648792.1456549703 6857975.634911752)\n269        92046        652000.0  POINT (647993.3130931311 6857524.953289739)\n270        92046        535000.0  POINT (647898.4202261526 6857613.093667737)\n271        92046        370000.0  POINT (648609.3566901543 6857898.192151285)\n272        92046        586538.4   POINT (648252.5163570014 6857226.52119363)\n273        92046        399642.0  POINT (648627.1853143852 6857390.983019724)\n274        92046        577400.0  POINT (648537.5425983313 6857832.000576756)\n275        92046        260000.0  POINT (648064.4010161356 6857533.545312459)\n276        92046        283000.0  POINT (649218.3073049119 6857831.224107595)\n277        92046        324000.0  POINT (648850.0533148871 6857498.095169536)\n278        92046        432000.0  POINT (648466.9129502296 6857783.590675344)\n279        92046        174000.0  POINT (648638.4736858582 6857571.019360576)\n280        92046        520000.0    POINT (648310.5579911153 6857690.6886879)\n281        92046        370150.0  POINT (648627.1853143852 6857390.983019724)\n\n\nLes transactions extraites sont maintenant chargées en mémoire et on les transforme dans un format sf, qui facilite leur manipulation en R via le package sf.\n\ntransactions_malakoff &lt;- \n  sf::st_as_sf(transactions_malakoff, wkt = \"geom_text\", crs = 2154) |&gt;\n  rename(geometry=geom_text)\n\ntransactions_malakoff\n\nSimple feature collection with 281 features and 2 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 646817.3 ymin: 6857081 xmax: 649467.8 ymax: 6858422\nProjected CRS: RGF93 v1 / Lambert-93\nFirst 10 features:\n   code_commune valeur_fonciere                 geometry\n1         92046        255000.0 POINT (647793.9 6857548)\n2         92046       1269230.0 POINT (648887.7 6858397)\n3         92046       1160000.0 POINT (648007.1 6857446)\n4         92046        233000.0 POINT (648674.9 6857442)\n5         92046        356000.0 POINT (648756.7 6857824)\n6         92046        885000.0 POINT (648871.7 6858374)\n7         92046        155300.0 POINT (647062.4 6857159)\n8         92046        295285.7 POINT (648967.6 6857946)\n9         92046       1000960.0 POINT (648370.9 6858007)\n10        92046        159000.0 POINT (648610.8 6857865)\n\n\nUne fois les données prêtes, on intersecte les points avec le traigle représentant le centre-ville de Malakoff. On utilise de nouveaux les outils du package sf.\n\nbool_mask &lt;- transactions_malakoff |&gt; \n  sf::st_transform(4326) |&gt; \n  sf::st_intersects(triangle, sparse = FALSE)\n\nin_triangle &lt;- transactions_malakoff[bool_mask,]\nout_triangle &lt;- transactions_malakoff[!bool_mask,]\n\nUne fois que chaque transaction est identifiée comme étant à l’intérieur ou à l’extérieur du Triangle, le calcul de la médiane des prix est immédiat.\n\nmedian_in &lt;- median(in_triangle$valeur_fonciere)\nmedian_out &lt;- median(out_triangle$valeur_fonciere)\n\nprint(glue(\"Médiane des prix dans le Triangle d'Or de Malakoff : \", median_in))\n\nMédiane des prix dans le Triangle d'Or de Malakoff : 377000\n\nprint(glue(\"Médiane des prix dans le reste de Malakoff : \", median_out))\n\nMédiane des prix dans le reste de Malakoff : 370075\n\n\nLa médiane des prix est un peu plus élevée dans le Triangle qu’en dehors.\n\n\nPartie 3 : Part de ménages pauvres à Malakoff et à Montrouge\nPour finir, on se place dans le cas où : - On souhaite extraire des informations d’un fichier volumineux (les données carroyées de l’Insee). - Mais il n’est pas possible de filtrer les données par des requêtes attributaires (par exemple, il n’est pas possible de faire code_commune = 92049).\nAinsi, nous allons : - Utiliser les contours géogrpahiques des deux communes - Filtrer les données par intersections géographiques des carreaux et des communes, à l’aide de DuckDB - Faire les calculs localement après l’extraction des carreaux d’intérêt.\nPour commencer, on décrit les données carroyées comme précédemment :\n\ndescribe_dvf &lt;- dbGetQuery(con, \"DESCRIBE SELECT * FROM read_parquet('carreaux.parquet')\")\nprint(describe_dvf)\n\n   column_name column_type null  key default extra\n1   idcar_200m     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n2    idcar_1km     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n3    idcar_nat     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n4    i_est_200     INTEGER  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n5    i_est_1km     INTEGER  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n6     lcog_geo     VARCHAR  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n7          ind      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n8          men      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n9     men_pauv      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n10    men_1ind      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n11    men_5ind      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n12    men_prop      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n13     men_fmp      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n14     ind_snv      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n15    men_surf      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n16    men_coll      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n17    men_mais      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n18    log_av45      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n19   log_45_70      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n20   log_70_90      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n21    log_ap90      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n22     log_inc      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n23     log_soc      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n24     ind_0_3      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n25     ind_4_5      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n26    ind_6_10      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n27   ind_11_17      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n28   ind_18_24      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n29   ind_25_39      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n30   ind_40_54      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n31   ind_55_64      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n32   ind_65_79      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n33     ind_80p      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n34     ind_inc      DOUBLE  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n35    geometry    GEOMETRY  YES &lt;NA&gt;    &lt;NA&gt;  &lt;NA&gt;\n\npreview &lt;- dbGetQuery(con, \"SELECT * FROM read_parquet('carreaux.parquet') LIMIT 10\")\nprint(preview)\n\n                       idcar_200m                       idcar_1km\n1  CRS3035RES200mN2029800E4252400 CRS3035RES1000mN2029000E4252000\n2  CRS3035RES200mN2029800E4254200 CRS3035RES1000mN2029000E4254000\n3  CRS3035RES200mN2030000E4254200 CRS3035RES1000mN2030000E4254000\n4  CRS3035RES200mN2030000E4254600 CRS3035RES1000mN2030000E4254000\n5  CRS3035RES200mN2030200E4254200 CRS3035RES1000mN2030000E4254000\n6  CRS3035RES200mN2030400E4253800 CRS3035RES1000mN2030000E4253000\n7  CRS3035RES200mN2030400E4255200 CRS3035RES1000mN2030000E4255000\n8  CRS3035RES200mN2030600E4252200 CRS3035RES1000mN2030000E4252000\n9  CRS3035RES200mN2030600E4253400 CRS3035RES1000mN2030000E4253000\n10 CRS3035RES200mN2030600E4253600 CRS3035RES1000mN2030000E4253000\n                         idcar_nat i_est_200 i_est_1km lcog_geo ind men\n1  CRS3035RES8000mN2024000E4248000         1         1    2A041 3.0 1.4\n2  CRS3035RES8000mN2024000E4248000         1         1    2A041 2.0 0.9\n3  CRS3035RES8000mN2024000E4248000         1         1    2A041 1.0 0.5\n4  CRS3035RES8000mN2024000E4248000         1         1    2A041 1.0 0.5\n5  CRS3035RES8000mN2024000E4248000         1         1    2A041 3.0 1.4\n6  CRS3035RES8000mN2024000E4248000         1         1    2A041 2.0 0.8\n7  CRS3035RES8000mN2024000E4248000         1         1    2A041 4.0 1.8\n8  CRS3035RES8000mN2024000E4248000         1         1    2A041 2.0 0.8\n9  CRS3035RES8000mN2024000E4248000         1         1    2A041 5.0 2.1\n10 CRS3035RES8000mN2024000E4248000         1         1    2A041 1.5 0.6\n   men_pauv men_1ind men_5ind men_prop men_fmp  ind_snv men_surf men_coll\n1       0.3      0.5      0.1      0.8     0.1  84270.6    182.5      0.3\n2       0.2      0.3      0.0      0.5     0.0  56180.4    121.7      0.2\n3       0.1      0.2      0.0      0.3     0.0  28090.2     60.8      0.1\n4       0.1      0.2      0.0      0.3     0.0  28090.2     60.8      0.1\n5       0.3      0.5      0.1      0.8     0.1  84270.6    182.5      0.3\n6       0.1      0.1      0.0      0.5     0.2  48048.7    100.8      0.1\n7       0.4      0.6      0.1      1.0     0.1 112360.8    243.4      0.4\n8       0.1      0.1      0.0      0.5     0.2  48048.7    100.8      0.1\n9       0.2      0.3      0.1      1.3     0.4 120121.7    252.0      0.2\n10      0.1      0.1      0.0      0.4     0.1  36036.5     75.6      0.0\n   men_mais log_av45 log_45_70 log_70_90 log_ap90 log_inc log_soc ind_0_3\n1       1.1      0.0       0.0       0.5      0.8     0.1       0     0.0\n2       0.7      0.0       0.0       0.3      0.5     0.1       0     0.0\n3       0.4      0.0       0.0       0.2      0.3     0.0       0     0.0\n4       0.4      0.0       0.0       0.2      0.3     0.0       0     0.0\n5       1.1      0.0       0.0       0.5      0.8     0.1       0     0.0\n6       0.7      0.0       0.1       0.2      0.5     0.0       0     0.0\n7       1.4      0.1       0.0       0.6      1.0     0.1       0     0.0\n8       0.7      0.0       0.1       0.2      0.5     0.0       0     0.0\n9       1.9      0.1       0.2       0.4      1.3     0.1       0     0.1\n10      0.6      0.0       0.1       0.1      0.4     0.0       0     0.0\n   ind_4_5 ind_6_10 ind_11_17 ind_18_24 ind_25_39 ind_40_54 ind_55_64 ind_65_79\n1      0.0      0.2       0.2       0.3       0.2       0.6       0.2       1.0\n2      0.0      0.1       0.1       0.2       0.1       0.4       0.1       0.7\n3      0.0      0.0       0.1       0.1       0.1       0.2       0.1       0.3\n4      0.0      0.0       0.1       0.1       0.1       0.2       0.1       0.3\n5      0.0      0.2       0.2       0.3       0.2       0.6       0.2       1.0\n6      0.0      0.0       0.2       0.1       0.2       0.5       0.5       0.4\n7      0.0      0.2       0.2       0.4       0.2       0.8       0.3       1.4\n8      0.0      0.0       0.2       0.1       0.2       0.5       0.5       0.4\n9      0.1      0.1       0.4       0.3       0.6       1.2       1.1       0.9\n10     0.0      0.0       0.1       0.1       0.2       0.4       0.3       0.3\n   ind_80p ind_inc\n1      0.3       0\n2      0.3       0\n3      0.1       0\n4      0.1       0\n5      0.3       0\n6      0.1       0\n7      0.5       0\n8      0.1       0\n9      0.2       0\n10     0.1       0\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         geometry\n1  02, 04, 00, 00, 00, 00, 00, 00, 5b, b9, 94, 49, b9, 9e, b8, 4a, 22, c0, 94, 49, 6f, a0, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, ea, 93, 78, 3c, 3d, 97, 32, 41, ce, 27, ef, 34, d7, 13, 57, 41, 2a, a9, c7, 7d, 2b, 97, 32, 41, 3d, 97, 74, 5d, 09, 14, 57, 41, 3c, 3e, ef, 78, f2, 97, 32, 41, 4e, 21, 49, d0, 0d, 14, 57, 41, a6, e7, b0, 37, 04, 98, 32, 41, 77, e3, c3, a7, db, 13, 57, 41, ea, 93, 78, 3c, 3d, 97, 32, 41, ce, 27, ef, 34, d7, 13, 57, 41\n2  02, 04, 00, 00, 00, 00, 00, 00, 52, f1, 94, 49, f9, 9f, b8, 4a, 19, f8, 94, 49, af, a1, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, 8b, 50, 80, 11, 3c, 9e, 32, 41, 6b, be, af, 3d, ff, 13, 57, 41, af, b4, 38, 52, 2a, 9e, 32, 41, cf, 6a, 33, 66, 31, 14, 57, 41, e2, 8d, 63, 4d, f1, 9e, 32, 41, 63, 73, d9, d8, 35, 14, 57, 41, b3, e7, bb, 0c, 03, 9f, 32, 41, c0, f9, 55, b0, 03, 14, 57, 41, 8b, 50, 80, 11, 3c, 9e, 32, 41, 6b, be, af, 3d, ff, 13, 57, 41\n3  02, 04, 00, 00, 00, 00, 00, 00, c4, f0, 94, 49, 8b, a1, b8, 4a, 8b, f7, 94, 49, 41, a3, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, af, b4, 38, 52, 2a, 9e, 32, 41, cf, 6a, 33, 66, 31, 14, 57, 41, fe, 05, f3, 92, 18, 9e, 32, 41, fd, 67, a9, 8e, 63, 14, 57, 41, 79, 23, 0d, 8e, df, 9e, 32, 41, d8, 3d, 4f, 01, 68, 14, 57, 41, e2, 8d, 63, 4d, f1, 9e, 32, 41, 63, 73, d9, d8, 35, 14, 57, 41, af, b4, 38, 52, 2a, 9e, 32, 41, cf, 6a, 33, 66, 31, 14, 57, 41\n4  02, 04, 00, 00, 00, 00, 00, 00, 34, fd, 94, 49, d2, a1, b8, 4a, fb, 03, 95, 49, 88, a3, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, 8b, c9, 8e, 48, b8, 9f, 32, 41, 35, 51, 7a, 4b, 3a, 14, 57, 41, 7e, a3, 27, 89, a6, 9f, 32, 41, d1, e8, ef, 73, 6c, 14, 57, 41, 28, 87, 42, 84, 6d, a0, 32, 41, ef, 68, 8b, e6, 70, 14, 57, 41, c5, 68, ba, 43, 7f, a0, 32, 41, 4d, 04, 16, be, 3e, 14, 57, 41, 8b, c9, 8e, 48, b8, 9f, 32, 41, 35, 51, 7a, 4b, 3a, 14, 57, 41\n5  02, 04, 00, 00, 00, 00, 00, 00, 36, f0, 94, 49, 1c, a3, b8, 4a, fd, f6, 94, 49, d2, a4, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, fe, 05, f3, 92, 18, 9e, 32, 41, fd, 67, a9, 8e, 63, 14, 57, 41, 1c, 44, af, d3, 06, 9e, 32, 41, eb, b6, 11, b7, 95, 14, 57, 41, 1a, a8, b8, ce, cd, 9e, 32, 41, 1e, 5a, b7, 29, 9a, 14, 57, 41, 79, 23, 0d, 8e, df, 9e, 32, 41, d8, 3d, 4f, 01, 68, 14, 57, 41, fe, 05, f3, 92, 18, 9e, 32, 41, fd, 67, a9, 8e, 63, 14, 57, 41\n6  02, 04, 00, 00, 00, 00, 00, 00, 38, e3, 94, 49, 66, a4, b8, 4a, ff, e9, 94, 49, 1c, a6, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, 84, 9f, 9d, dd, 78, 9c, 32, 41, 70, ef, b6, d1, 8c, 14, 57, 41, ce, 38, 7d, 1e, 67, 9c, 32, 41, f4, f5, 11, fa, be, 14, 57, 41, 7d, 23, 75, 19, 2e, 9d, 32, 41, e2, bc, c1, 6c, c3, 14, 57, 41, 9d, 41, a6, d8, 3f, 9d, 32, 41, b4, e8, 66, 44, 91, 14, 57, 41, 84, 9f, 9d, dd, 78, 9c, 32, 41, 70, ef, b6, d1, 8c, 14, 57, 41\n7  02, 04, 00, 00, 00, 00, 00, 00, bf, 0e, 95, 49, 5f, a5, b8, 4a, 86, 15, 95, 49, 15, a7, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, 53, 1d, e2, ba, e9, a1, 32, 41, fc, 38, 1a, f4, ab, 14, 57, 41, 60, b4, 4c, fb, d7, a1, 32, 41, 5b, dc, 73, 1c, de, 14, 57, 41, af, 59, 47, f6, 9e, a2, 32, 41, 69, 75, ff, 8e, e2, 14, 57, 41, 7e, 79, ed, b5, b0, a2, 32, 41, 51, 05, a6, 66, b0, 14, 57, 41, 53, 1d, e2, ba, e9, a1, 32, 41, fc, 38, 1a, f4, ab, 14, 57, 41\n8  02, 04, 00, 00, 00, 00, 00, 00, ec, b0, 94, 49, db, a4, b8, 4a, b3, b7, 94, 49, 90, a6, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, 0c, ef, ca, 46, 2f, 96, 32, 41, 1e, aa, d9, 63, 9b, 14, 57, 41, 36, 1c, 32, 88, 1d, 96, 32, 41, d2, 91, 28, 8c, cd, 14, 57, 41, 4a, 75, 16, 83, e4, 96, 32, 41, 8c, 81, 01, ff, d1, 14, 57, 41, f0, fd, bf, 41, f6, 96, 32, 41, 16, cb, b2, d6, 9f, 14, 57, 41, 0c, ef, ca, 46, 2f, 96, 32, 41, 1e, aa, d9, 63, 9b, 14, 57, 41\n9  02, 04, 00, 00, 00, 00, 00, 00, 3b, d6, 94, 49, b0, a5, b8, 4a, 02, dd, 94, 49, 66, a7, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, 68, 80, 8e, 28, d9, 9a, 32, 41, 81, e6, a2, 14, b6, 14, 57, 41, e2, 6b, 91, 69, c7, 9a, 32, 41, d9, a4, f0, 3c, e8, 14, 57, 41, 9e, e3, 77, 64, 8e, 9b, 32, 41, 29, 90, aa, af, ec, 14, 57, 41, 7c, ad, 85, 23, a0, 9b, 32, 41, d5, 03, 5d, 87, ba, 14, 57, 41, 68, 80, 8e, 28, d9, 9a, 32, 41, 81, e6, a2, 14, b6, 14, 57, 41\n10 02, 04, 00, 00, 00, 00, 00, 00, 73, dc, 94, 49, d4, a5, b8, 4a, 39, e3, 94, 49, 8a, a7, b8, 4a, 02, 00, 00, 00, 01, 00, 00, 00, 05, 00, 00, 00, 00, 00, 00, 00, 7c, ad, 85, 23, a0, 9b, 32, 41, d5, 03, 5d, 87, ba, 14, 57, 41, 9e, e3, 77, 64, 8e, 9b, 32, 41, 29, 90, aa, af, ec, 14, 57, 41, af, b9, 5e, 5f, 55, 9c, 32, 41, 20, 50, 5f, 22, f1, 14, 57, 41, ce, 38, 7d, 1e, 67, 9c, 32, 41, f4, f5, 11, fa, be, 14, 57, 41, 7c, ad, 85, 23, a0, 9b, 32, 41, d5, 03, 5d, 87, ba, 14, 57, 41\n\n\nExtraction des carreaux intersectant Malakoff :\n\nPréparation du contour de Malakoff (reprojection dans le même système que les données carroyées, transformation en Well-Known-Text pour être compatible avec DuckDB)\nRequête sur le fichier carreaux.parquet à l’aide de DuckDB et de la fonction ST_Intersects\nTransformation des carreaux extraits en objet sf (uniquement utile pour la cartographie ici).\n\n\nmalakoff_2154 &lt;- sf::st_transform(malakoff, 2154)\nmalakoff_wkt &lt;- sf::st_as_text(sf::st_geometry(malakoff_2154))\n\ngeo_query &lt;- glue(\"\n  FROM read_parquet('carreaux.parquet')\n  SELECT\n      *, ST_AsText(geometry) AS geom_text\n  WHERE ST_Intersects(\n      geometry,\n      ST_GeomFromText('{malakoff_wkt}')\n  )\n\")\n\ncarr_malakoff &lt;- dbGetQuery(con, geo_query)\n\ncarr_malakoff &lt;-\n  carr_malakoff |&gt;\n  sf::st_as_sf(wkt = \"geom_text\", crs = 2154) |&gt;\n  select(-geometry) |&gt;\n  rename(geometry=geom_text)\n\nmapview(carr_malakoff) + mapview(sf::st_boundary(malakoff)) \n\n\n\n\n\nOn réitère l’opération pour Montrouge :\n\nmontrouge_2154 &lt;- sf::st_transform(montrouge, 2154)\nmontrouge_wkt &lt;- sf::st_as_text(sf::st_geometry(montrouge_2154))\n\ngeo_query &lt;- glue(\"\n  FROM read_parquet('carreaux.parquet')\n  SELECT\n      *, ST_AsText(geometry) AS geom_text\n  WHERE ST_Intersects(\n      geometry,\n      ST_GeomFromText('{montrouge_wkt}')\n  )\n\")\n\ncarr_montrouge &lt;- dbGetQuery(con, geo_query)\n\ncarr_montrouge &lt;-\n  carr_montrouge |&gt;\n  sf::st_as_sf(wkt = \"geom_text\", crs = 2154) |&gt;\n  select(-geometry) |&gt;\n  rename(geometry=geom_text)\n\nmapview(carr_montrouge) + mapview(sf::st_boundary(montrouge)) \n\n\n\n\n\nEnfin, on calcule la proportion moyenne de ménages pauvre dans l’ensemble des carreaux extraits :\n\nmean_menpauvres_malakoff &lt;- round(100 * sum(carr_malakoff$men_pauv) / sum(carr_malakoff$men), 2)\nmean_menpauvres_montrouge &lt;- round(100 * sum(carr_montrouge$men_pauv) / sum(carr_montrouge$men), 2)\n\nprint(glue(\"Part de ménages pauvres dans les carreaux de Malakoff : \", mean_menpauvres_malakoff))\n\nPart de ménages pauvres dans les carreaux de Malakoff : 12.01\n\nprint(glue(\"Part de ménages pauvres dans les carreaux de Montrouge : \", mean_menpauvres_montrouge))\n\nPart de ménages pauvres dans les carreaux de Montrouge : 11.02\n\n\nA noter que cette démarche est améliorable sur plusieurs plans : - rationalisation des requêtes, - pertinence statistique des résultats - réplicabilité du code",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Application 1 - Données spatiales"
    ]
  },
  {
    "objectID": "applications/ape.html",
    "href": "applications/ape.html",
    "title": "Application 1",
    "section": "",
    "text": "Cette application illustrera certains apports des outils du NLP pour la codification automatique des déclarations d’activité dans la nomenclature des activités françaises. On pourra coder dans un notebook au sein de l’environnement SSP Cloud suivant:",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1"
    ]
  },
  {
    "objectID": "applications/ape.html#exploration-du-jeu-de-données",
    "href": "applications/ape.html#exploration-du-jeu-de-données",
    "title": "Application 1",
    "section": "Exploration du jeu de données",
    "text": "Exploration du jeu de données\nCe tutoriel se propose d’illustrer la problématique de la classification automatique par le biais de l’algorithme d’apprentissage supervisé fastText à partir des données issues des déclarations Sirene.\nLe code pour lire les données est directement fourni:\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom wordcloud import WordCloud\n\nDATA_PATH = \"https://minio.lab.sspcloud.fr/projet-formation/diffusion/mlops/data/firm_activity_data.parquet\"\nNAF_PATH = \"https://minio.lab.sspcloud.fr/projet-formation/nouvelles-sources/data/naf2008_liste_n5.xls\"\nnaf = pd.read_excel(NAF_PATH, skiprows = 2)\nnaf['Code'] = naf['Code'].str.replace(\".\",\"\")\ntrain = pd.read_parquet(DATA_PATH)\ntrain = train.merge(naf, left_on = \"nace\", right_on = \"Code\")\ntrain.head(5)\n\n\n\n\n\n\n\n\nnace\ntext\nCode\nLibellé\n\n\n\n\n0\n8220Z\nMISSIONS PONCTUELLES A L AIDE D UNE PLATEFORME\n8220Z\nActivités de centres d'appels\n\n\n1\n8553Z\nINSPECTEUR AUTOMOBILE\n8553Z\nEnseignement de la conduite\n\n\n2\n5520Z\nLA LOCATION TOURISTIQUE DE LOGEMENTS INSOLITES...\n5520Z\nHébergement touristique et autre hébergement d...\n\n\n3\n4791A\nCOMMERCE DE TOUT ARTICLES ET PRODUITS MARCHAND...\n4791A\nVente à distance sur catalogue général\n\n\n4\n9499Z\nREGROUPEMENT RETRAITE\n9499Z\nAutres organisations fonctionnant par adhésion...\n\n\n\n\n\n\n\nLe premier exercice a vocation à illustrer la manière classique de rentrer dans un corpus de données textuelles. La démarche n’est pas particulièrement originale mais permet d’illustrer les enjeux du nettoyage de texte.\n\n\n Exercice 1 \n\nCréer une fonction pour compter le nombre de textes contenant une séquence de caractères donnée dans le corpus. La tester avec “data science” et “boulanger”.\nFaire une fonction pour afficher le wordcloud de notre corpus dans son ensemble et de certaines catégories pour comprendre la nature de notre corpus.\nRetirer les stopwords à partir de la liste des mots disponibles dans SpaCy.\n\n\n\nAide\n\nfrom nltk.tokenize import word_tokenize\nimport spacy\n\nnlp = spacy.load(\"fr_core_news_sm\")\nstop_words = #liste de stopwords\n\n# Function to remove stopwords\ndef remove_stopwords(text):\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if word.lower() not in stop_words]\n    return ' '.join(filtered_text)\n\ndef remove_single_letters(text):\n    word_tokens = word_tokenize(text)\n    filtered_text = [word for word in word_tokens if len(word) &gt; 1]\n    return ' '.join(filtered_text)\n\n# Apply the function to the 'text' column\ntrain['text_clean'] = (train['text']\n    .apply(remove_stopwords)\n    .apply(remove_single_letters)\n)\n\n\nRefaire quelques uns des nuages de mots et étudier la différence avant nettoyage.\n\n\n\nDans une démarche exploratoire, le plus simple est de commencer par compter les mots de manière indépendante (approche sac de mot). Par exemple, de manière naturelle, nous avons beaucoup plus de déclarations liées à la boulangerie que liées à la data science:\n\nfilter_train_data(train, \"data science\").head(5)\nfilter_train_data(train, \"boulanger\").head(5)\n\nNombre d'occurrences de la séquence 'data science': 54\nNombre d'occurrences de la séquence 'boulanger': 1928\n\n\n\n\n\n\n\n\n\nnace\ntext\nCode\nLibellé\n\n\n\n\n90\n1071C\nBOULANGERIE PATISSERIE VIENNOISERIE\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n107\n1071C\nBOULANGERIE PATISSERIE FABRICATION\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n153\n1071C\nBOULANGERIE PATISSERIE GLACES CONFISERIES BOIS...\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n314\n1071C\nBOULANGERIE PATISSERIE VIENNOISERIE CONFISE...\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n487\n1071C\nBOULANGERIE PATISSERIE ACHAT VENTE ET MAINT...\n1071C\nBoulangerie et boulangerie-pâtisserie\n\n\n\n\n\n\n\nLes wordclouds peuvent servir à rapidement visualiser la structure d’un corpus. On voit ici que notre corpus est très bruité car nous n’avons pas nettoyé celui-ci:\n\n\n\n\n\n\n\n\n\nPour commencer à se faire une idée sur les spécificités des catégories, on peut représenter le corpus de certaines d’entre elles ? Arrivez-vous à inférer la catégorie de la NAF en question ? Si oui, vous utilisez sans doute des heuristiques proches de celles que nous allons mettre en oeuvre dans notre algorithme de classification.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNéanmoins, à ce stade, les données sont encore très bruitées. La première étape classique est de retirer les stop words et éventuellement des termes spécifiques à notre corpus. Par exemple, pour des données de caisse, on retirera les bruits, les abréviations, etc. qui peuvent bruiter notre corpus.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1"
    ]
  },
  {
    "objectID": "applications/ape.html#premier-algorithme-dapprentissage-supervisé",
    "href": "applications/ape.html#premier-algorithme-dapprentissage-supervisé",
    "title": "Application 1",
    "section": "Premier algorithme d’apprentissage supervisé",
    "text": "Premier algorithme d’apprentissage supervisé\nNous avons nettoyé nos données. Cela devrait améliorer la pertinence de nos modèles en réduisant le ratio signal/bruit. Nous allons généraliser notre nettoyage de texte en appliquant un peu plus d’étapes que précédemment. Nous allons notamment raciniser nos mots.\nPour cela, récupérer les fichiers suivants:\n\nconstants.py\npreprocessor.py\nutils.py\n\net mettre ceux-ci dans le même dossier que votre notebook Jupyter.\nLe code de nettoyage est directement fourni:\n\nfrom processor import Preprocessor\npreprocessor = Preprocessor()\n\n# Preprocess data before training and testing\nTEXT_FEATURE = \"text\"\nY = \"nace\"\n\ndf = preprocessor.clean_text(train, TEXT_FEATURE).drop('text_clean', axis = \"columns\")\ndf.head(2)\n\n\n\n\n\n\n\n\nnace\ntext\nCode\nLibellé\n\n\n\n\n0\n8220Z\nmission ponctuel aid plateform\n8220Z\nActivités de centres d'appels\n\n\n1\n8553Z\ninspecteur automobil\n8553Z\nEnseignement de la conduite\n\n\n\n\n\n\n\nNous allons commencer à entraîner un modèle dont le plongement de mot est de faible dimension. Voici les paramètres qui seront utiles pour le prochain exercice.\n\nimport pathlib\n\nparams = {\n    \"dim\": 25,\n    \"label_prefix\": \"__label__\"\n}\n\ndata_path = pathlib.Path(\"./data\")\ndata_path.mkdir(parents=True, exist_ok=True)\n\ndef write_training_data(df, params, training_data_path=None):\n    warnings.filterwarnings(\"ignore\", \"Setuptools is replacing distutils.\")\n    if training_data_path is None:\n        training_data_path = get_root_path() / \"data/training_data.txt\"\n\n    with open(training_data_path, \"w\", encoding=\"utf-8\") as file:\n        for _, item in df.iterrows():\n            formatted_item = f\"{params['label_prefix']}{item[Y]} {item[TEXT_FEATURE]}\"\n            file.write(f\"{formatted_item}\\n\")\n    return training_data_path.as_posix()\n\n\n\n Exercice 2 \n\nDécouper notre échantillon complet en train et test.\nfastText effectue son entraînement à partir d’objets stockés dans un .txt. Utiliser la fonction write_training_data de la manière suivante pour l’écrire.\n\n# Write training data in a .txt file (fasttext-specific)\ntraining_data_path = write_training_data(df_train, params, pathlib.Path(str(data_path.absolute()) + \"/training_data.txt\"))\n\nAvec l’aide de la documentation de la librairie Python, entraîner votre modèle de classification.\nSauvegarder le modèle sous forme de binaire, cela pourra éventuellement servir ultérieurement.\nRenvoyer les trois catégories les plus probables pour les nouveaux libellés suivants:\n\nlist_libs = [\"vendeur d'huitres\", \"boulanger\"]\n\nSur l’ensemble du jeu de test, renvoyer la meilleure prédiction pour chaque descriptif d’activités. Evaluer la performance globale et la performance classe par classe, par exemple en calculant le rappel (pour les classes de plus de 200 cas).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1"
    ]
  },
  {
    "objectID": "applications/ape.html#pour-aller-plus-loin-introduction-au-mlops",
    "href": "applications/ape.html#pour-aller-plus-loin-introduction-au-mlops",
    "title": "Application 1",
    "section": "Pour aller plus loin, introduction au MLOps",
    "text": "Pour aller plus loin, introduction au MLOps\nOn utilise dans cette application un modèle de Machine Learning (ML) pour prédire l’activité des entreprises à partir de texte descriptifs. Les méthodes de ML sont quasiment indispensables pour traiter du texte, mais utiliser des modèles de ML pour servir des cas d’usage réels demande de respecter un certain nombre de bonnes pratiques pour que tout se passe convenablement, en particulier:\n\nTracking propre des expérimentations\nVersioning des modèles, en même temps que des données et du code correspondants\nMise à disposition efficace du modèle aux utilisateurs\nMonitoring de l’activité du modèle servi\nRéentraînement du modèle\n\nUne introduction à ces bonnes pratiques, auxquelles on fait régulièrement référence à travers le terme MLOps, est donné dans cette formation (dépôt associé).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 1"
    ]
  },
  {
    "objectID": "textes_exemples.html",
    "href": "textes_exemples.html",
    "title": "Application 2",
    "section": "",
    "text": "L’objectif de ce TP est d’explorer les aspects suivants du traitement du langage naturel:\n\nPreprocessing\nElasticSearch: indexation et requêtage\n\nIl est disponible sur cette page en version web et peut être ouvert sur l’environnement SSPCloud où ElasticSearch est disponible en cliquant sur le bouton suivant:",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Application 2"
    ]
  },
  {
    "objectID": "administratives.html",
    "href": "administratives.html",
    "title": "Données administratives",
    "section": "",
    "text": "La baisse généralisée au niveau européen des taux de réponse1 (Luiten, Hox, and Leeuw 2020; Beck et al. 2022), qui accroît les coûts de collecte et rend plus difficile celle-ci sur certaines sous-populations, notamment les plus jeunes, nécessite de trouver des solutions pour répondre à la demande toujours accrue de statistique officielle.\nComme développé dans l’introduction, les données administratives sont des données de gestion produites par l’administration. Le processus de production statistique, où la collecte de donnée est construite de manière à mesurer le plus objectivement possible un phénomène cible, diffère du processus de production administratif. Pour cette dernière, la donnée est produite de sorte à faciliter la gestion. L’exploitation de celle-ci à des fins de production de statistique ou de recherche n’est pas le moteur de leur construction. L’exploitation de cette donnée est une affaire d’opportunité. Cette perte de contrôle du processus de production, qui fait que l’exploitant de la donnée se retrouve en aval de son processus de production, a tout de même des bénéfices : l’exhaustivité sur une population cible et la plus haute fréquence de ces données. Ceci explique qu’elles deviennent de plus en plus importantes dans la production de statistique officielle.\nCe chapitre revient sur le contexte d’utilisation des données administratives, leurs différences avec d’autres sources de données et les apports de celles-ci à la production de savoir statistique.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#nature-des-données",
    "href": "administratives.html#nature-des-données",
    "title": "Données administratives",
    "section": "Nature des données",
    "text": "Nature des données\nLes données statistiques traditionnelles (sondage ou recensement) sont produites pour informer. Cette finalité guide la conception de celles-ci, que ce soit au niveau du design, des concepts mesurés ou des retraitements post-collecte. La logique des données administratives est toute autre. Il s’agit de bases dont la finalité de construction est la gestion, c’est-à-dire l’enregistrement d’événements pour déclencher des actions (remboursement, paiement, etc.).\nCet aspect transactionnel de la donnée adminstrative change ainsi le processus de production. Ces bases sont susceptibles d’être mises à jour à plusieurs échéances. D’abord, leur structure n’est pas figée dans le temps. Selon les événements à enregistrer, la structure du fichier de données évoluera. Par exemple, un nouveau crédit d’impôt amènera à l’ajout d’une catégorie dans les déclarations fiscales ce qui se traduira par un changement du fichier de gestion. A ce premier facteur d’évolution peut s’ajouter des changements à plus brève échéance. La collecte de données administratives est un processus vivant. Les données sont généralement modifiables au cours d’un exercice de gestion voire au-delà. La donnée n’est stabilisée qu’après plusieurs cycles de gestion et sa continuité, au niveau de l’unité statistique, ne va pas de soi. Par exemple, une entreprise changeant d’identifiant SIREN pour une raison liée à un changement administratif (par exemple une fusion) ne sera identifiable dans différents millésimes de données administratives que si on est en mesure de relier les différents identifiants sous lequel elle apparaît.\nLes données administratives peuvent provenir de plusieurs origines. Elles sont en premier lieu issues de processus de gestion interne à l’administration concernée. Par exemple, pour être en mesure de gérer les remboursements liés au système de protection sociale français, l’assurance maladie collecte et enregistre de nombreuses informations sur les actes médicaux. Cette collecte est automatisée grâce à la carte vitale et au système d’information de l’assurance maladie ou passe par des déclarations papiers normalisées.\nUne seconde source d’origine des données administratives sont les déclarations administratives2 (Rivière 2018). Par exemple, les déclarations fiscales des ménages sont annuelles, avec un calendrier déterminé à l’avance (qui dépend du format, papier ou internet). Ce calendrier inclut d’ailleurs des possibilités allongées de retour sur la donnée fournie. L’obligation de certaines déclarations administratives se traduit par un pouvoir coercitif, pouvant prendre diverses formes, comme celle d’engager des poursuites. Ceci réduit le risque de non-déclaration ou de déclaration faussée mais ne l’annihile pas non plus. Selon la nature de la donnée, ces poursuites peuvent être pénales et les amendes non négligeables. L’existence de ces moyens coercitifs permet d’anticiper une information exhaustive sur la sous-population concernée par la donnée et honnête3.\nSi les données administratives sont devenues centrales dans le champ de la production statistique, c’est certes de par leur nature exhaustive mais aussi du fait de leur disponibilité à faible coût marginal. Les données administratives étant collectées et centralisées dans un système informatique à des fins de gestion, leur mise à disposition pour d’autres usages, s’il soulève certains enjeux sur lesquels nous reviendrons comme les questions de confidentialité, est marginalement peu coûteux. L’utilisation de ces données est ainsi une affaire d’opportunité: comme ces données sont disponibles et, sous un certain cadre juridique et technique, peuvent être ré-utilisables à d’autres fins, si elles fournissent une information de qualité, il est utile pour la production statistique de les exploiter.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#quelle-différence-avec-les-autres-sources-de-données-numériques",
    "href": "administratives.html#quelle-différence-avec-les-autres-sources-de-données-numériques",
    "title": "Données administratives",
    "section": "Quelle différence avec les autres sources de données numériques ?",
    "text": "Quelle différence avec les autres sources de données numériques ?\nCette propriété des données administratives qu’est le coût marginal faible rapproche celles-ci des traces numériques. Les entreprises du numérique ont pu centrer leur modèle économique autour de la collecte et de la valorisation de données justement parce que la collecte de nouvelles informations est d’un coût marginal nul. Il en va de même avec les données de gestion: la collecte d’une information supplémentaire sur une unité ou d’une unité supplémentaire n’est pas coûteuse. Dans le monde de la donnée numérique, il est certes nécessaire d’engager des investissements pour être en mesure de collecter des données de manière massive ou mettre à l’échelle un processus de collecte devenu plus ambitieux que le plan initial mais la donnée marginale ne coûte pas très cher puisque, comme nous allons le voir, la collecte de celle-ci est reportée sur un tiers.\nDès lors, la distinction entre données administratives et données numériques, telles qu’on peut formaliser le buzzword “big-data”, apparaît floue. La distinction correspond en premier lieu à l’origine des données. La donnée administrative est une donnée produite par la sphère administrative. Dans sa nature, son processus de production ne diffère pas de celui de la donnée privée. Dans les deux cas, un acteur effectue une activité (par exemple déclarer quelque chose) et cette activité va être transformée en information plus ou moins normalisée pour intégrer un système d’information et être stockée dans les serveurs d’un acteur centralisateur. Dans les deux cas, la personne dont la donnée a été collectée pourra éventuellement corriger l’information et/ou produire de nouvelles activités.\nLa différence entre données administratives et données privée est ainsi plutôt une différence de degré que de nature. Les données administratives sont généralement collectées à plus faible fréquence. Par exemple, le rythme de collecte de nombreuses données est annuel pour correspondre aux rythmes des campagnes fiscales. Mais certaines sources sont à des rythmes plus fréquents. Par exemple la DSN, sur laquelle nous reviendrons, est collectée à un rythme mensuel. Certaines données sont mêmes enregistrées à des rythmes qui n’ont pas grand chose à envier avec les traces numériques du big data. Par exemple, les systèmes d’information SIVIC et SIDEP, respectivement celui de suivi des entrées à l’hôpital des personnes malades du Covid et celui des tests, étaient mis à jour quotidiennement. De même, le système d’information de l’assurance maladie est mis à jour en continu en fonction des nouveaux événements qui appellent un remboursement. Bien qu’on n’associe pas forcément les données administratives avec une collecte en temps réel, il ne s’agit ainsi pas d’un critère les discriminant vis à vis des traces numériques.\nLa différence principale, peut-être, entre les données administratives et les données privées est que pour les premières, le champ est connu par le fait que celles-ci sont issues d’une collecte d’une population bien ciblée. Comme indiqué précédemment, comme la collecte de données administrative est souvent assortie de prérogatives légales, la population cible est généralement bien identifiée. Dans le monde de la donnée privée, comme c’est l’activité qui génère la donnée, le champ dépend de la base d’utilisateurs. Selon le type de données, celle-ci peut être plus ou moins large. Même parmi les données privées où les populations sont les plus larges, la couverture de la population n’est pas parfaite. Par exemple, les smartphones sont largement partagés dans la population. Néanmoins, cette technologie a un moindre taux de pénétration dans certaines population, notamment les plus agées. De plus, les opérateurs ont des parts de marché potentiellement hétérogènes (en fonction de critères d’âge ou territoriaux). Pour les opérateurs, il est difficile d’évaluer le champ de leur clientèle puisque cette information nécessite une enquête, et ainsi souffre de taux de réponse imparfaits ou de réponses incorrectes. Le champ est donc incertain puisqu’il n’est pas possible pour les producteurs de données privées d’apparier de manière automatique ces données avec les données administratives. Même s’il n’est pas toujours possible d’apparier des données administratives entre elles pour des raisons légales, le fait de fournir des informations communes dans différentes sources (état civil voire NIR) à un même acteur (l’Etat), facilite l’association entre les sources lorsque celle-ci est autorisée.\nLes 5V du big-data, initialement listés dans un rapport de MacKinsey, ne sont pas l’apanage des données privées. Il y a peut-être une différence de degré avec le big-data mais certainement pas de nature:\n\nVolume: certaines données administratives représentent des volumes conséquents. La DSN représente ainsi plus d’1To de données par an ;\nVélocité: certaines données, notamment celles de l’assurance maladie, sont à haute fréquence ;\nVariété: l’Etat collecte et exploite des données de natures très différentes ;\nVéracité: les données collectées par l’Etat ne sont pas à l’abri d’erreurs mais ces dernières, qu’elles soient volontaires ou non, pouvant être couteuses, les données sont normalement de meilleure qualité que celles auto-déclarées sans contrôle ex-post ;\nValeur: les données collectées par l’Etat sont d’une grande valeur même si elles ne sont pas monétisées. La valorisation par l’Etat n’est bien-sûr pas individuelle mais la collecte de données qui sont ensuite agrégées permet de créer une statistique publique, qui est un bien public, sans valeur de marché mais avec une valeur sociale.\n\nFinalement, il y a peu de différence entre les données administratives et certaines données privées disponibles sous forme structurée. Par exemple, les données générées par les paiements par cartes bancaires (données du GIE CB)  ne sont pas d’une nature très différente de données administratives. Comme celles-ci, il s’agit de données structurées issues d’un organisme centralisateur (le GIE CB) et mises à disposition consolidées pour la statistique publique.\n\nUne donnée plus sensible\nL’aspect exhaustif, sur un certain champ d’unités et d’informations de gestion, des données administratives peut les rendre, au niveau individuel, assez sensibles. La question de la confidentialité et de la sensibilité des données fournie à l’administration n’est pas nouvelle, il s’agit de la raison d’être du secret statistique défini dans l’une des lois les plus importantes de la statistique publique, à savoir la loi de 1951. Les informations fournies dans le cadre de certaines enquêtes peuvent être sensibles (informations sur le revenu ou le patrimoine, la santé, l’appartenance à certains groupes sociaux…). Cependant, l’aspect non exhaustif des enquêtes rend plus difficile la réidentification après la phase d’anonymisation. Avec les données administratives, l’information fournie peut parfois être moins précise mais le caractère exhaustif de celles-ci fait qu’en combinant plusieurs sources de données la réindentification est facilitée.\nLa question de la confidentialité est donc, au même titre que pour les données privées, devenu un enjeu dans le domaine des données administratives. Il est à noter que par rapport aux données privées cette question ne se pose pas au même niveau. Au niveau de la collecte de données, c’est-à-dire de la transformation d’une activité en donnée de gestion, là où l’utilisateur d’un service numérique bénéficie d’une relative liberté sur le choix des données collectées du fait du RGPD, ce n’est pas le cas pour l’utilisateur d’un service géré par l’Etat. Ce privilège de l’Etat s’appuie sur des décrets qui définissent des missions de service public. Cependant, au niveau des traitements mis en oeuvre, du stockage puis de la diffusion de la donnée, des conditions restrictives s’appliquent aussi à l’Etat. Exemple: SNDS.\n\n\n\n\n\n\nCadre légal\n\n\n\nCet encadré résume des éléments juridiques listés par Isnard (2018).\nLes membres du service statistique public (SSP) bénéficient d’une disposition importante qui facilite énormément le travail du statisticien. Ce sont les seuls organismes à pouvoir mettre en œuvre l’article 7bis de la loi de 1951. Cet article leur permet de se faire communiquer, à des fins d’élaboration de statistiques publiques, tout fichier de gestion d’une administration ou d’une personne privée gérant un service public, dès lors que le Conseil national de l’information statistique a été consulté et que la demande émane du ministre chargé de l’économie (en pratique du directeur général de l’Insee). Cette mesure, insérée dans la loi du 7 juin 1951 par la loi du 26 décembre 1986, a permis une exploitation large des données administratives et ainsi un allègement de la charge de réponse aux enquêtes.\nL’utilisation de déclarations ou de sources administratives à des fins statistiques est préconisée par le code de bonnes pratiques de la statistique européenne dans le but d’alléger la charge statistique des déclarants. En France, ceci est rendu possible par la loi de 1951 relative à l’obligation, à la coordination et au secret en matière de statistique et a été réaffirmé récemment par la loi pour une République numérique (2016).",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#processus-de-production",
    "href": "administratives.html#processus-de-production",
    "title": "Données administratives",
    "section": "Processus de production",
    "text": "Processus de production\nLe processus de production de la donnée administrative est différent de celui de la donnée traditionnelle. La différence principale est la place centrale d’une autorité gestionnaire, qui centralise la donnée, dans le modèle de production des données administratives (Rivière 2018). Cet acteur doit être distingué de l’administration qui exploite le flux, que ce soit à des fins de gestion ou d’exploitation statistique.\nLa Table 1 donne quelques exemples de plateformes centralisatrices. Ces dernières ne se contentent pas de centraliser ou mettre à disposition la donnée, elles ont aussi en charge la normalisation de celle-ci à partir de systèmes d’informations divers. La normalisation est un enjeu majeur car elle seule permet l’exploitation des données: la collecte étant en général réalisée automatiquement via des auto-déclarations, les plateformes centralisatrices récupèrent des informations aux contenus hétérogènes.\n\n\n\nTable 1: Exemples d’autorités centralisatrices (Rivière 2018)\n\n\n\n\n\nDonnée\nAutorité centralisatrice\n\n\n\n\nDSN\nGip-MDS\n\n\nDonnées hospitalières\nATIH-10\n\n\nSI gestion des eaux\nSANDRE12\n\n\n\n\n\n\nLa Figure 1 résume la place du GIP-MDS dans le processus de production de la DSN:\n\n\n\n\n\n\nFigure 1: Schéma de la place du GIP-MDS dans la production de la DSN. Source: Humbert-Bottin (2018).",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#usage-de-la-donnée-administrative",
    "href": "administratives.html#usage-de-la-donnée-administrative",
    "title": "Données administratives",
    "section": "Usage de la donnée administrative",
    "text": "Usage de la donnée administrative\nL’usage de ces données est de deux nature: l’usage à des fins de gestion (la finalité pour laquelle elles sont construites) et l’usage à des fins d’analyse (la finalité fortuite). Ces peut aller au-delà de l’administration concernée. Par exemple, la déclaration sociale nominative n’est pas utilisée exclusivement par le Ministère du Travail mais aussi par la DGFIP, les institutions de prévoyance, les organismes de retraite, l’Acoss, pour leurs propres usages de gestion ; les données de SIRENE servent de référence, de preuve pour les entreprises, elles sont utilisées par les chambres de commerce et d’industrie ou par les greffes des tribunaux de commerce (Rivière 2018).\n\nUn usage accru pour apparier des sources\nCertaines sources administratives ont un rôle particulier dans le processus de production statistique car elle permettent d’identifier des unités statistiques dans plusieurs sources. Le Répertoire national d’identification des personnes physiques (RNIPP), le répertoire Sirene pour les entreprises ou encore XXX pour les logements, sont des sources qui permettent de relier des unités statistiques entre plusieurs sources. On parle d’appariements pour désigner ce type d’opérations où plusieurs sources de données sont associées grâce à une information commune. Cela peut se faire sur la base d’une information exacte, en général un identifiant unique fourni par un des référentiels, ou de manière floue à partir d’informations non uniques mais qui, combinées, peuvent aider à identifier une unité (nom, raison sociale d’une entreprise, adresse, etc.).\nCes répertoires administratifs sont ainsi des sources devenues centrales dans le processus de production statistique. Ils permettent d’enrichir d’autres sources administratives, ou des enquêtes, d’informations administratives. Ces dernières peuvent ainsi permettre d’alléger certains questionnaires d’enquêtes ou de concentrer ceux-ci sur des informations qui ne sont pas disponibles dans les sources administratives.\n\n\n\n\n\n\nLe CSNS\n\n\n\nUn enjeu fort existe autour de la production d’un code statistique non signifiant (CSNS) pour les besoins de mise en œuvre de traitements à finalité de statistique publique impliquant le numéro de sécurité sociale (NIR) ou des traits d’identité, en particulier les appariements au sein du Service statistique publique. La version finale est prévue pour la fin de l’année 2022.\n\n\n\n\nUn changement de la place de l’analyste de la donnée\nCette situation change la place du statisticien dans le processus de production de la statistique officielle. Il convient de transformer en aval les données pour répondre aux besoins de l’analyse statistique. Cela implique un contrôle qualité ex-post, éventuellement un travail de reconstitution et de consolidation.\nCette situation change également la place des chercheurs dans le processus de production de la donnée. Comme le statisticien, le chercheur n’est plus associé à l’amont de la production de données. Cependant celui-ci est, généralement, encore plus en aval que le statisticien public. Il reçoit les données généralement consolidées, anonymisées et éventuellement appariées entre différentes sources. A cet égard, les données administratives scandinaves sont parmi les données les plus utilisées par les chercheurs sur le marché du travail car elles constituent une source depuis longtemps centralisée et mise à disposition de manière anonymisée.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#en-conclusion-quels-avantages-et-inconvénients",
    "href": "administratives.html#en-conclusion-quels-avantages-et-inconvénients",
    "title": "Données administratives",
    "section": "En conclusion, quels avantages et inconvénients ?",
    "text": "En conclusion, quels avantages et inconvénients ?\nLa production et l’usage de données administratives se sont généralisés. La numérisation croissante de l’économie est amené à confirmer cette tendance. L’utilisation par la statistique publique de données privées, sous leur forme structurée, n’est qu’un prolongement de cette dynamique. Ces dernières permettent d’enrichir l’information dont dispose l’administration avec des informations collectées dans le cadre d’activités économiques détachées de l’administration.\nLes avantages des données administratives sont multiples. En premier lieu, la collecte automatisée de celle-ci, associée à un pouvoir public coercitif, permet d’atteindre sur un champ d’unités statistiques bien définies (usuellement par le biais d’un décret), une forme d’exhaustivité. Cette dernière permet de construire des statistiques plus fines. Si aujourd’hui il est possible pour des chercheurs de zoomer sur le très haut de la distribution de revenu (voir les travaux de Piketty), c’est parce que l’aspect exhaustif des données permet d’avoir des groupements suffisamment nombreux pour assurer la confidentialité de ces groupes.\nUne fois payé le coût d’investissement pour automatiser la production statistique à partir de données de gestion, les données administratives ouvrent la voie à la production à plus haute fréquence de statistiques officielles. La production annuelle ou infra-annuelle de statistiques n’est possible qu’avec un nombre restreint d’enquêtes - dans la plupart des cas, les résultats d’enquêtes sont connus avec du retard. La publication quotidienne par le service statistique du Ministère de la Santé (la DREES) et Santé Publique France d’indicateurs sur la pandémie est un bon exemple de l’intérêt de ces données. Ces dernières ont permis un suivi très fin par la puissance publique mais aussi par la société civile des évolutions de l’épidémie.\nUn autre avantage des données administratives est que les informations qui sont disponibles dans celles-ci sont certes diverses (nous reviendrons sur cela dans le prochain chapitre à travers quelques exemples) mais elles sont, sur certains champs, très fiables. Elles souffrent normalement moins de biais de réponses même si elles n’en sont pas exemptées (les déclarations erronées à l’administration fiscale existent, qu’il s’agisse d’un comportement volontaire ou non).\nCes données soulèvent de nouveaux défis pour la statistique publique. En premier lieu, elles amènent à redéfinir le rôle du métier dans le processus de production de la donnée. Ceci est vrai dans le monde de la donnée administrative mais aussi dans le domaine des données privées. Comme l’utilisateur de données ne contrôle pas le champ ou la définition du concept mesuré, c’est le concentrateur, cet acteur dont l’activité est spécialisée autour de la collecte et de la gestion du flux, qui intervient à cet étape. Il peut ainsi être amené à faire évoluer le champ, la définition du phénomène mesuré ou encore le formulaire sans que l’analyste de données n’ait son mot à dire. Pour reprendre l’exemple des données quotidiennes, l’apparition de variants à plusieurs reprises a amené à des évolutions, parfois sans préavis, du type de donnée collectée, enregistrée. Les données déjà collectées n’ayant pas vocation à intégrer ces informations qui n’avaient pas de sens au moment de la collecte, c’est à l’analyste de données de faire des choix méthodologiques pour reconstruire une série cohérente. Le statisticien, parce qu’il intervient plus en aval, change donc de rôle. Les données administratives n’étant pas construites pour mesurer un phénomène qui a du sens pour le statisticien public (ou l’analyste de la donnée privée), c’est à lui de reconstruire à partir de l’information de gestion la réalité statistique derrière (Salgado and Oancea 2020). Le travail de l’analyse de données va au donc au delà de la simple reconstruction de variable, ou du contrôle qualité, il est également nécessaire de réfléchir au concept mesuré pour ne pas construire d’“artefact”, au sens de Bourdieu. Cette problématique se pose, de la même manière, à la recherche et à l’exploitation de données privées.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "administratives.html#footnotes",
    "href": "administratives.html#footnotes",
    "title": "Données administratives",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPar exemple, le taux de réponse a baissé pour l’enquête en face-à-face Cadre de vie et sécurité de 72 % à 66 % entre 2012 et 2021. En ce qui concerne SRCV (Statistiques sur les ressources et les conditions de vie), le taux de réponse est passé de 85 % à 80 % entre 2010 et 2019. Des événements ponctuels comme la crise du Covid-19 peuvent de plus avoir des effets très forts sur le taux de réponse. Par exemple, en 2020, à la date du 23 avril, le taux de réponse à l’enquête sur la production industrielle en mars, qui sert d’indicateur avancé de l’activité économique, était inférieur d’environ 20 points de pourcentage à ce qui est observé lors d’un mois habituel (voir blog de l’Insee).↩︎\nObligation est faite à un certain nombre d’entités (individus, entreprises, organismes publics) de fournir des informations respectant une certaine forme, selon certaines modalités (internet, papier) et temporalités.↩︎\nCertaines enquêtes, reconnues d’utilité publique, comme l’enquête emploi, le recensement ou encore l’enquête ressources et conditions de vie (SRCV), sont obligatoires. Bien que cela permette d’avoir des taux de réponse élevés, cela n’assure pas un taux de 100%. Comme cela a été évoqué précédemment, le taux de réponse de SRCV est par exemple passé de 85 % à 80 % entre 2010 et 2019. Pour plus d’informations sur les enquêtes obligatoires, voir la description du CNIS et la liste des enquêtes concernées parmi les enquêtes auprès des particuliers.↩︎",
    "crumbs": [
      "Home",
      "Données administratives",
      "Données administratives"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Données émergentes (ENSAI)",
    "section": "",
    "text": "Ce site web centralise les contenus du cours de Master de l’ENSAI sur les données émergentes\n\n\n\n\n\n\nNote\n\n\n\nCe site est encore dans une version très provisoire. Le contenu peut ainsi être amené à évoluer rapidement"
  },
  {
    "objectID": "introduction.html",
    "href": "introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "L’histoire de la statistique est une suite d’évolutions de la discipline où les données émergentes un jour deviennent le lendemain traditionnelles. Le XIXe siècle, qui est celui où la statistique s’est constituée en temps que discipline autonome et s’est dotée d’une partie des concepts qui en font aujourd’hui les fondements, est ainsi une période où de nombreuses données ont émergé et ont pu entraîner des révolutions scientifiques. Parmi celles-ci, la construction de la loi normale, qui constitue aujourd’hui l’objet central de la statistique, correspond au besoin de construire de nouveaux concepts et outils afin de structurer dans une théorie commune un ensemble de nouvelles données. La manière dont Gauss a collecté et synthétisé un ensemble d’observations astronomiques a ainsi permis de construire la méthode des moindres carrés et le concept de loi normale, appréhendé à partir des erreurs d’observations.\nL’accès à des recensements par des universitaires à la fin du XIXe siècle a été un élément moteur de la constitution de la sociologie en temps que discipline autonome. Les registres de décès ont ainsi permis à Durkheim de participer aux débats sociologiques sur le suicide et de proposer une interprétation sociologique de ses causes à rebours des approches psychologisantes qui étaient fréquentes à l’époque. Avant Durkheim, l’usage novateur des monographies a permis de dessiner les prémisses de la sociologie en temps que discipline autonome. Les avancées de la statistique au cours du XXe siècle sont intimement liées à la génération des enquêtes ou des sondages.\nLes notions d’échantillonnage, de représentativité, ou encore de marges d’erreur, qui sont au coeur de la statistique moderne, ont permis de rendre traditionnel ce nouveau mode de collecte. Ces enquêtes sont aujourd’hui encore très utilisées dans la production statistique moderne ou dans les études économiques et sociologiques.\nLa prolifération de traces numériques, parce qu’elle a créé de nouvelles opportunités pour la puissance publique ou pour des acteurs privés de valoriser des données, est un moteur d’évolution de la statistique. L’émergence du concept de data-science, qu’on le considère comme un ensemble de pratiques ou uniquement comme un buzzword, est intimement lié à la multiplication des traces numériques. Les nouvelles disciplines ou méthodes qui se sont développées récemment sont intrinsèquement liées aux données émergentes. La vitesse à laquelle se développent les innovations dans le domaine de la data-science est d’une ampleur inédite du fait de la multiplicité des données collectées et des acteurs impliqués. IBM estimait en effet que 2.5 quintillions d’octets de données étaient générés chaque jour il y a environ 10 ans. Dans un ouvrage sur l’histoire de la statistique, Hacking (1990) parle déjà en 1990 du début d’une “avalanche de chiffres”.",
    "crumbs": [
      "Home",
      "Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#les-données-émergentes-dans-le-temps-long",
    "href": "introduction.html#les-données-émergentes-dans-le-temps-long",
    "title": "Introduction",
    "section": "",
    "text": "L’histoire de la statistique est une suite d’évolutions de la discipline où les données émergentes un jour deviennent le lendemain traditionnelles. Le XIXe siècle, qui est celui où la statistique s’est constituée en temps que discipline autonome et s’est dotée d’une partie des concepts qui en font aujourd’hui les fondements, est ainsi une période où de nombreuses données ont émergé et ont pu entraîner des révolutions scientifiques. Parmi celles-ci, la construction de la loi normale, qui constitue aujourd’hui l’objet central de la statistique, correspond au besoin de construire de nouveaux concepts et outils afin de structurer dans une théorie commune un ensemble de nouvelles données. La manière dont Gauss a collecté et synthétisé un ensemble d’observations astronomiques a ainsi permis de construire la méthode des moindres carrés et le concept de loi normale, appréhendé à partir des erreurs d’observations.\nL’accès à des recensements par des universitaires à la fin du XIXe siècle a été un élément moteur de la constitution de la sociologie en temps que discipline autonome. Les registres de décès ont ainsi permis à Durkheim de participer aux débats sociologiques sur le suicide et de proposer une interprétation sociologique de ses causes à rebours des approches psychologisantes qui étaient fréquentes à l’époque. Avant Durkheim, l’usage novateur des monographies a permis de dessiner les prémisses de la sociologie en temps que discipline autonome. Les avancées de la statistique au cours du XXe siècle sont intimement liées à la génération des enquêtes ou des sondages.\nLes notions d’échantillonnage, de représentativité, ou encore de marges d’erreur, qui sont au coeur de la statistique moderne, ont permis de rendre traditionnel ce nouveau mode de collecte. Ces enquêtes sont aujourd’hui encore très utilisées dans la production statistique moderne ou dans les études économiques et sociologiques.\nLa prolifération de traces numériques, parce qu’elle a créé de nouvelles opportunités pour la puissance publique ou pour des acteurs privés de valoriser des données, est un moteur d’évolution de la statistique. L’émergence du concept de data-science, qu’on le considère comme un ensemble de pratiques ou uniquement comme un buzzword, est intimement lié à la multiplication des traces numériques. Les nouvelles disciplines ou méthodes qui se sont développées récemment sont intrinsèquement liées aux données émergentes. La vitesse à laquelle se développent les innovations dans le domaine de la data-science est d’une ampleur inédite du fait de la multiplicité des données collectées et des acteurs impliqués. IBM estimait en effet que 2.5 quintillions d’octets de données étaient générés chaque jour il y a environ 10 ans. Dans un ouvrage sur l’histoire de la statistique, Hacking (1990) parle déjà en 1990 du début d’une “avalanche de chiffres”.",
    "crumbs": [
      "Home",
      "Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "introduction.html#la-production-renouvelée-de-données-de-la-puissance-publique",
    "href": "introduction.html#la-production-renouvelée-de-données-de-la-puissance-publique",
    "title": "Introduction",
    "section": "La production renouvelée de données de la puissance publique",
    "text": "La production renouvelée de données de la puissance publique\nLa puissance publique est une productrice historique de données. Les registres administratifs ou comptables sont une source de données très appréciée des historiens. Si elles n’atteignent pas les volumétries actuelles, ces sources sont néanmoins les ancêtres de nos données administratives actuelles. Les recensements de population sont également une des productions historiques de données. Le comptage de la population et des impôts fait partie intégrante du processus de constitution de la puissance publique centralisatrice (Desrosières 2010). Curieusement, la tablette Kish de l’empire sumérien (environ 3500 av. J.-C.), l’un des plus anciens exemples d’écriture humaine, semble être un document administratif destiné à des fins statistiques.\nLa statistique publique, si elle est aujourd’hui entendue beaucoup plus largement que par le passé, et qu’elle dispose d’une indépendance vis-à-vis d’autres branches de l’Etat, c’est parce qu’elle est un élément essentiel pour pour permettre le bon fonctionnement de l’économie et de la démocratie. Le slogan de l’Insee, “mesurer pour comprendre”, correspond bien à cette idée. Les statistiques officielles essaient d’objectiver les phénomènes socio-économiques par la collecte de données et la construction de concepts cohérents avec le phénomène mesuré.\nLes enquêtes sont historiquement une source privilégiée puisque la conception de celles-ci, en amont de la collecte et des retraitements post-collecte, est justement effectuée en fonction des réutilisations futures. Les questions sont ainsi conçues pour s’approcher au plus près des phénomènes qu’on désire quantifier et l’échantillonnage puis les redressements post-collecte permettront de contrôler la population sur laquelle portent les statistiques construites. L’inconvénient est que cette production nécessite des moyens et un temps conséquents (en amont de la collecte, lors de celle-ci puis à l’issue de celle-ci). De plus, les enquêtes ne sont pas à l’abri d’erreurs dans la collecte, qu’il s’agisse d’omissions ou réponses erronnées, qu’elles soient volontaires ou non. A ces problèmes s’ajoute la baisse historique des taux de réponse (Rivière 2018).\nL’Etat n’accumule pas uniquement de la connaissance sur sa population par le biais d’enquête. Les registres des impôts, de l’assurance maladie, etc. sont des sources de gestion par lesquelles chaque individu communique un certain nombre d’informations sur lui. On parle de données administratives pour regrouper cet ensemble de sources qui sont produites par la puissance publique et dont la collecte répond à des enjeux de gestion mais pas à des besoins de statistique publique. La définition qu’en donnait Desrosières (2004), résume bien ceci: “une source administrative est issue d’une institution dont la finalité n’est pas de produire une telle information, mais dont les activités de gestion impliquent la tenue, selon des règles générales, de fichiers ou de registres individuels, dont l’agrégation n’est qu’un sous-produit”. Les besoins de la statistique publique ne sont donc pas à la source de la collecte mais on peut utiliser celle-ci comme opportunité pour enrichir la connaissance de phénomènes socio-économiques (Connelly et al., Einav et al.). Certaines informations disponibles dans ces données sont très génériques et communes à de nombreuses bases de gestion (l’état civil notamment), ce qui peut faciliter l’association entre elles, alors que d’autres sont propres à chaque source. Outre la possibilité de disposer d’informations sur une population plus importante, la différence principale entre ces sources de données, historiquement collectées par papier et de plus en plus par collecte numérique, et les enquêtes est que les premières ne sont pas conçues initialement à des fins de statistique donc le statisticien n’en contrôle pas la conceptualisation et la collecte. Néanmoins, ces sources peuvent fournir des informations très précieuses à la statistique publique. Si on est en mesure de relier celles-ci à une enquête, il devient possible d’enrichir ou de corriger certaines informations collectées si les concepts présents dans l’enquête correspondent à ceux de la source administrative.\nLes données administratives deviennent ainsi de plus en plus fréquemment mobilisées dans la production officielle de statistiques ou dans les études économiques. La numérisation de l’économie et des démarches administratives, parce qu’elle a facilité la constitution de bases et l’association entre celles-ci, a accéléré le mouvement de constitution de grands répertoires administratifs. Parmi les principaux exploités par la statistique publique : la DSN, Fidéli, le SNDS… La construction de ces sources, car celles-ci nécessitent pour leur usage à des fins statistiques une reconstruction, implique également un changement des institutions collectant la donnée. Ce n’est plus l’Insee qui collecte directement la donnée (que ce soit à son compte ou pour le compte d’autres institutions comme les services statistiques ministériels) mais des ministères. Ces derniers peuvent, ou non, exploiter ces données à leur propre compte mais aussi mettre à disposition la donnée brute ou une version retravaillée de celle-ci. Par exemple, la Direction Générale des Finances Publiques (DGFiP) est, par son rôle de collecte des impôts, un acteur central dans la constitution de bases sur les revenus qui permettent de produire de nombreuses statistiques socio-économiques. De même, la Caisse Nationale d’Assurance Maladie (CNAM) est, par son rôle de gestionnaire du système français de sécurité sociale, un élément central dans la constitution du Système national des données de santé (SNDS).\nLa multiplication de traces numériques collectées non plus seulement par les acteurs publics mais aussi par des acteurs privés a permis de produire de nouvelles sources de données, à une fréquence ou à une échelle inédite. A ce premier facteur qu’est l’intensification de la production de statistique, s’ajoute la demande croissante de la population et des décideurs publics pour des statistiques plus détaillées et disponibles plus rapidement. Cela a ainsi amené à une intensification de la disponibilité de statistiques, dont la production n’est plus le monopole de la puissance publique. Afin de pouvoir produire ces statistiques, tout en satisfaisant aux critères usuels de qualité sur lesquels nous reviendrons, la statistique publique se doit d’innover dans la collecte traditionnelle, l’utilisation de nouvelles statistiques et concepts ou dans les processus de valorisation de données auquel elle accédait déjà. Parmi ces trois facteurs, nous allons principalement nous concentrer sur le deuxième, c’est-à-dire la valorisation de nouvelles sources de données, qu’il s’agisse de données produites par l’administration ou de données privées. Le premier point - l’innovation dans les méthodes de collecte traditionnelles - renvoie, entre autres, à la question du multimode. Enfin, en ce qui concerne le troisième élément - la rénovation des processus de production - il y a des éléments connexes à notre problématique (certaines méthodes sont intrinsèquement liées à de nouvelles sources) mais aussi certains qui le dépassent. Nous n’allons donc pas nous concentrer sur ceux-ci bien qu’il se peut que nous évoquions à plusieurs reprises ces enjeux.",
    "crumbs": [
      "Home",
      "Introduction",
      "Introduction"
    ]
  },
  {
    "objectID": "textes.html",
    "href": "textes.html",
    "title": "Données textuelles et non structurées",
    "section": "",
    "text": "Les données textuelles sont aujourd’hui parmi les types de données les plus prometteurs pour la statistique publique et l’un des champs les plus actifs de la recherche en data science. Pour cause, de plus en plus de services existent sur le web qui conduisent à la collecte de données textuelles. En outre, des nouvelles méthodes pour collecter et traiter ces traces numériques particulières ont été développées dans les dernières années.\nUne partie des méthodes d’analyse qui appartiennent à la palette des compétences des data scientists spécialistes du traitement de données textuelles sont en réalité assez anciennes. Par exemple, la distance de Levensthein a été proposée pour la première fois en 1965, l’ancêtre des réseaux de neurone actuels est le perceptron qui date de 1957, etc.1 Néanmoins, le fait que certaines entreprises du net basent leur business model sur le traitement et la valorisation de la donnée textuelle, notamment Google, Facebook et Twitter, a amené à renouveler le domaine.\nLa statistique publique s’appuie également sur la collecte et le traitement de données textuelles. Les collectes de données officielles ne demandent pas exclusivement d’informations sous le forme de texte. Les premières informations demandées sont généralement un état civil, une adresse, etc. C’est ensuite, en fonction du thème de l’enquête, que d’autres informations textuelles seront collectées: un nom d’entreprise, un titre de profession, etc. Les données administratives elles-aussi comportent souvent des informations textuelles. Ces données défient l’analyse statistique car cette dernière, qui vise à détecter des grandes structures à partir d’observations multiples, doit s’adapter à la différence des données textuelles: le langage est un champ où certaines des notions usuelles de la statistique (distance, similarité notamment) doivent être revues.\nCe chapitre propose un panorama très incomplet de l’apport des données non structurées, principalement textuelles, pour la statistique et l’analyse de données. Nous évoquerons plusieurs sources ou méthodes de collecte. Nous ferons quelques détours par des exemples pour aller plus loin.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#présentation",
    "href": "textes.html#présentation",
    "title": "Données textuelles et non structurées",
    "section": "Présentation",
    "text": "Présentation\nLe webscraping est une méthode de collecte de données qui repose sur le moissonnage d’objets de grande dimension (des pages web) afin d’en extraire des informations ponctuelles (du texte, des nombres…). Elle désigne les techniques d’extraction du contenu des sites Internet. C’est une pratique très utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n’existant pas forcément sous la forme de fichiers exportables.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#enjeux-pour-la-statistique-publique",
    "href": "textes.html#enjeux-pour-la-statistique-publique",
    "title": "Données textuelles et non structurées",
    "section": "Enjeux pour la statistique publique",
    "text": "Enjeux pour la statistique publique\nLe webscraping présente un certain nombre d’enjeux en termes de légalité, qui ne seront pas enseignés dans ce cours. En particulier, la Commission nationale de l’informatique et des libertés (CNIL) a publié en 2020 de nouvelles directives sur le webscraping reprécisant qu’aucune donnée ne peut être réutilisée à l’insu de la personne à laquelle elle appartient.\nLe webscraping est un domaine où la reproductibilité est compliquée à mettre en oeuvre. Une page web évolue régulièrement et d’une page web à l’autre, la structure peut être très différente ce qui rend certains codes difficilement généralisables. Par conséquent, la meilleure manière d’avoir un programme fonctionnel est de comprendre la structure d’une page web et dissocier les éléments exportables à d’autres cas d’usages des requêtes ad hoc.\nUn code qui fonctionne aujourd’hui peut ainsi très bien ne plus fonctionner au bout de quelques semaines. Il apparaît préférable de privilégier les API qui sont un accès en apparence plus compliqué mais en fait plus fiable à moyen terme. Cette difficulté à construire une extraction de données pérenne par webscraping une illustration du principe “there is no free lunch”. La donnée est au cœur du business model de nombreux acteurs, il est donc logique qu’ils essaient de restreindre la moisson de leurs données.\nLes APIs sont un mode d’accès de plus en plus généralisé à des données. Cela permet un lien direct entre fournisseurs et utilisateurs de données, un peu sous la forme d’un contrat. Si les données sont ouvertes avec restrictions, on utilise des clés d’authentification. Avec les API, on structure sa demande de données sous forme de requête paramétrée (source désirée, nombre de lignes, champs…) et le fournisseur de données y répond, généralement sous la forme d’un résultat au format JSON. Python et JavaScript sont deux outils très populaires pour récupérer de la donnée selon cette méthode. Pour plus de détails, vous pouvez explorer le chapitre sur les API dans le cours de Python de l’ENSAE.\nOn n’est pas à l’abri de mauvaises surprises avec les APIs (indisponibilité, limite atteinte de requêtes…) mais cela permet un lien plus direct avec la dernière donnée publiée par un producteur. L’avantage de l’API est qu’il s’agit d’un service du fournisseur de données, qui en tant que service va amener un producteur à essayer de répondre à une demande dans la mesure du possible. Le webscraping étant un mode d’accès à la donnée plus opportuniste, où le réel objectif du producteur de données n’est pas de fournir de la donnée mais une page web, il n’y a aucune garantie de service ou de continuité.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#exemples-dans-la-statistique-publique",
    "href": "textes.html#exemples-dans-la-statistique-publique",
    "title": "Données textuelles et non structurées",
    "section": "Exemples dans la statistique publique",
    "text": "Exemples dans la statistique publique",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#implémentations",
    "href": "textes.html#implémentations",
    "title": "Données textuelles et non structurées",
    "section": "Implémentations",
    "text": "Implémentations\nPython est le langage le plus utilisé par les scrappers. BeautifulSoup sera suffisant quand vous voudrez travailler sur des pages HTML statiques. Dès que les informations que vous recherchez sont générées via l’exécution de scripts Javascript, il vous faudra passer par des outils comme Selenium. De même, si vous ne connaissez pas l’URL, il faudra passer par un framework comme Scrapy, qui passe facilement d’une page à une autre (“crawl”). Scrapy est plus complexe à manipuler que BeautifulSoup : si vous voulez plus de détails, rendez-vous sur la page du tutoriel Scrapy. Pour plus de détails, voir le TP sur le webscraping en 2e année de l’ENSAE.\nLes utilisateurs de R privilégieront httr and rvest qui sont les packages les plus utilisés. Il est intéressant d’accorder de l’attention à polite. Ce package vise à récupérer des données en suivant les recommandations de bonnes pratiques sur le sujet, notamment de respecter les instructions dans robots.txt (“The three pillars of a polite session are seeking permission, taking slowly and never asking twice”).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#du-bag-of-words-aux-modèles-de-langage",
    "href": "textes.html#du-bag-of-words-aux-modèles-de-langage",
    "title": "Données textuelles et non structurées",
    "section": "Du bag of words aux modèles de langage",
    "text": "Du bag of words aux modèles de langage\nL’objectif du Natural Langage Processing (NLP) est de transformer une information de très haute dimension (une langue est un objet éminemment complexe) en information à dimension plus limitée qui peut être exploitée par un ordinateur.\nLa première approche pour entrer dans l’analyse d’un texte est généralement l’approche bag of words ou topic modeling. Dans la première, il s’agit de formaliser un texte sous forme d’un ensemble de mots où on va piocher plus ou moins fréquemment dans un sac de mots possibles. Dans la seconde, il s’agit de modéliser le processus de choix de mots en deux étapes (modèle de mélange): d’abord un choix de thème puis, au sein de ce thème, un choix de mots plus ou moins fréquents selon le thème.\nDans ces deux approches, l’objet central est la matrice document-terme. Elle formalise les fréquences d’occurrence de mots dans des textes ou des thèmes. Néanmoins, il s’agit d’une matrice très creuse: même un texte au vocabulaire très riche n’explore qu’une petite partie du dictionnaire des mots possibles.\nL’idée derrière les embeddings est de proposer une information plus condensée qui permet néanmoins de capturer les grandes structures d’un texte. Il s’agit par exemple de résumer l’ensemble d’un corpus en un nombre relativement restreint de dimensions. Ces dimensions ne sont pas prédéterminées mais plutôt inférées par un modèle qui essaie de trouver la meilleure partition des dimensions pour rapprocher les termes équivalents. Chacune de ces dimensions va représenter un facteur latent, c’est à dire une variable inobservée, de la même manière que les composantes principales produites par une ACP. Techniquement, au lieu de représenter les documents par des vecteurs sparse de très grande dimension (la taille du vocabulaire) comme on l’a fait jusqu’à présent, on va les représenter par des vecteurs denses (continus) de dimension réduite (en général, autour de 100-300).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#intérêt-des-modèles-de-langage",
    "href": "textes.html#intérêt-des-modèles-de-langage",
    "title": "Données textuelles et non structurées",
    "section": "Intérêt des modèles de langage",
    "text": "Intérêt des modèles de langage\nPar exemple, un humain sait qu’un document contenant le mot “Roi” et un autre document contenant le mot “Reine” ont beaucoup de chance d’aborder des sujets semblables.\n\n\n\n\n\n\nFigure 1: Schéma illustratif de word2vec.\n\n\n\nPourtant, une vectorisation de type comptage ou TF-IDF ne permet pas de saisir cette similarité : le calcul d’une mesure de similarité (norme euclidienne ou similarité cosinus) entre les deux vecteurs donnera une valeur très faible, puisque les mots utilisés sont différents.\nA l’inverse, un modèle word2vec (voir Figure 1) bien entraîné va capter qu’il existe un facteur latent de type “royauté”, et la similarité entre les vecteurs associés aux deux mots sera forte.\nLa magie va même plus loin : le modèle captera aussi qu’il existe un facteur latent de type “genre”, et va permettre de construire un espace sémantique dans lequel les relations arithmétiques entre vecteurs ont du sens ; par exemple (voir Figure 2) :\n\\[\\text{king} - \\text{man} + \\text{woman} ≈ \\text{queen}\\]\nChaque mot est représenté par un vecteur de taille fixe (comprenant \\(n\\) nombres), de façon à ce que deux mots dont le sens est proche possèdent des représentations numériques proches. Ainsi les mots « chat » et « chaton » devraient avoir des vecteurs de plongement assez similaires, eux-mêmes également assez proches de celui du mot « chien » et plus éloignés de la représentation du mot « maison ».\nComment ces modèles sont-ils entraînés ? Via une tâche de prédiction résolue par un réseau de neurones simple. L’idée fondamentale est que la signification d’un mot se comprend en regardant les mots qui apparaissent fréquemment dans son voisinage. Pour un mot donné, on va donc essayer de prédire les mots qui apparaissent dans une fenêtre autour du mot cible.\nEn répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié, on obtient finalement des embeddings pour chaque mot du vocabulaire, qui présentent les propriétés discutées précédemment.\n\n\n\n\n\n\nFigure 2: Illustration des word embeddings.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#les-modèles-de-langage-aujourdhui",
    "href": "textes.html#les-modèles-de-langage-aujourdhui",
    "title": "Données textuelles et non structurées",
    "section": "Les modèles de langage aujourd’hui",
    "text": "Les modèles de langage aujourd’hui\nLa méthode de construction d’un plongement lexical présentée ci-dessus est celle de l’algorithme Word2Vec. Il s’agit d’un modèle open-source développé par une équipe de Google en 2013. Word2Vec a été le pionnier en termes de modèles de plongement lexical.\nLe modèle GloVe constitue un autre exemple (Pennington, Socher, and Manning 2014). Développé en 2014 à Stanford, ce modèle ne repose pas sur des réseaux de neurones mais sur la construction d’une grande matrice de co-occurrences de mots. Pour chaque mot, il s’agit de calculer les fréquences d’apparition des autres mots dans une fenêtre de taille fixe autour de lui. La matrice de co-occurrences obtenue est ensuite factorisée par une décomposition en valeurs singulières. Il est également possible de produire des plongements de mots à partir du modèle de langage BERT, développé par Google en 2019, dont il existe des déclinaisons dans différentes langues, notamment en Français (les modèles CamemBERT ou FlauBERT).\nEnfin, le modèle FastText, développé en 2016 par une équipe de Facebook, fonctionne de façon similaire à Word2Vec mais se distingue particulièrement sur deux points :\n\nEn plus des mots eux-mêmes, le modèle apprend des représentations pour les n-grams de caractères (sous-séquences de caractères de taille \\(n\\), par exemple « tar », « art » et « rte » sont les trigrammes du mot « tarte »), ce qui le rend notamment robuste aux variations d’orthographe ;\nLe modèle a été optimisé pour que son entraînement soit particulièrement rapide.\n\nLe modèle GPT-3 (acronyme de Generative Pre-trained Transformer 3) a aujourd’hui le vent en poupe. Celui-ci a été développé par la société OpenAI et rendu public en 2020 (Brown et al. 2020). GPT-3 est le plus gros modèle de langage jamais entraîné avec 175 milliards de paramètres. Il sert de brique de base à plusieurs applications utilisant l’analyse textuelle pour synthétiser, à partir d’une instruction, des éléments importants et proposer un texte cohérent. Github Copilot l’utilise pour transformer une instruction en proposition de code, à partir d’un grand corpus de code open source. Algolia l’utilise pour transformer une instruction en mots clés de recherche afin d’améliorer la pertinence des résultats.\nEn ce moment, le champ du prompt engineering est en effervescence. Les modèles de langage comme GPT-3 permettent en effet d’extraire les éléments qui permettent de mieux discriminer les thèmes d’un texte.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#utilisation-dans-un-processus-de-créatoin-de-contenu-créatif",
    "href": "textes.html#utilisation-dans-un-processus-de-créatoin-de-contenu-créatif",
    "title": "Données textuelles et non structurées",
    "section": "Utilisation dans un processus de créatoin de contenu créatif",
    "text": "Utilisation dans un processus de créatoin de contenu créatif\nLa publication par l’organisation Open AI de son modèle de génération de contenu créatif Dall-E-2 (un jeu de mot mélangeant Dali et Wall-E) a créé un bruit inédit dans le monde de la data-science. Un compte Twitter (Weird Dall-E Mini Generations) propose de nombreuses générations de contenu drôles ou incongrues. Le bloggeur tech Casey Newton a pu parler d’une révolution créative dans le monde de l’IA.\nLa Figure 3 montre un exemple d’image générée par DALL-E-2.\n\n\n\n\n\n\nFigure 3: “A Shiba Inu dog wearing a beret and black turtleneck”\n\n\n\nLes modèles générateurs d’image DallE et Stable Diffusion peuvent, schématiquement, être décomposés en deux niveaux de réseaux de neurones:\n\nle contenu de la phrase est analysé par un modèle de langage comme GPT-3 ;\nles éléments importants de la phrase (recontextualisés) sont ensuite transformés en image à partir de modèles entraînés à reconnaître des images.\n\n\n\n\n\n\nStable Diffusion est une version plus accessible que DALL-E pour les utilisateurs de Python.\n\n\n\n\n\nSi vous êtes intéressés par ce type de modèle, vous pouvez tester les exemples du cours de Python de l’ENSAE. Vous pouvez tester “Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene” pour générer une image comme celle-ci dessous ou chercher à obtenir l’image de votre choix:",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#catégorisation",
    "href": "textes.html#catégorisation",
    "title": "Données textuelles et non structurées",
    "section": "Catégorisation",
    "text": "Catégorisation\nA l’Insee, plusieurs modèles de classification de libellés textuels dans des nomenclatures reposent sur l’algorithme de plongement lexical FastText. Les derniers mis en oeuvre sont les suivants:\n\ncatégorisation des professions dans la nomenclature des PCS ;\ncatégorisation des entreprises dans la nomenclature d’activité APE ;\ncatégorisation des produits dans la nomenclature des COICOP.\n\nLes deux premiers devraient servir prochainement à la production de statistiques officielles. Le troisième est une expérimentation encore en cours.",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#appariements",
    "href": "textes.html#appariements",
    "title": "Données textuelles et non structurées",
    "section": "Appariements",
    "text": "Appariements\n\n\n\nSource: Galiana and Suarez Castillo (2022).",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "textes.html#footnotes",
    "href": "textes.html#footnotes",
    "title": "Données textuelles et non structurées",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPour remonter plus loin dans la ligne du temps des données textuelles, on peut penser au Soundex, un algorithme d’indexation des textes dans les annuaires dont l’objectif était de permettre de classer à la suite des noms qui ne déviaient que par une différence typographique et non sonore.↩︎",
    "crumbs": [
      "Home",
      "Analyse textuelle",
      "Données textuelles et non structurées"
    ]
  },
  {
    "objectID": "administratives_exemples.html",
    "href": "administratives_exemples.html",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "4 exemples de nature différente:\n\nla DSN: base de gestion transmise à l’Insee et la DARES pour la production ;\nSirene: répertoire géré par l’Insee, utilisé par d’autres acteurs ;\nFidéli: agrégation et mise en cohérence de plusieurs sources ;\nSNDS: mise en cohérence de données de gestion hospitalières et de l’assurance maladie, enjeu encore plus fort de confidentialité ;\n\n\n\n\n\n\n\nLes autres répertoires de la statistique publique\n\n\n\n\nFilosofi (Fichier localisé social et fiscal): répertoire de synthèse des sources fiscales ;\nLa Base permanente des équipements (BPE): répertoire d’équipements et services.\n\n\n\n\n\n\n\n\n\n\n\nLes DADS et la DSN\n\n\n\nDescriptions sur le site de l’Insee de la Déclaration annuelle de données sociales (DADS) et de la Déclaration sociale nominative (DSN).\n\n\nLa Déclaration sociale nominative est aujourd’hui le mode d’échanges de données sociales des entreprises vers l’administration, et concerne toutes les entreprises du secteur privé. Elle résulte d’un projet de simplification administrative qui s’est étalé sur près de dix ans : la collecte des données est adossée au processus générateur de la collecte des cotisations sociales, c’est-à-dire au processus de paie (Humbert-Bottin 2018). En plus de réduire la charge imposée aux entreprise, la DSN garantit une bien meilleure qualité et l’exhaustivité de l’information recueillie.\n\n\nLes déclarations sociales font partie des tâches administratives historiquement imposées aux entreprises françaises. La déclaration sociale nominative (DSN), née à la fin des années 2000, a été instituée par la loi de simplification du 22 mars 2012, dite loi Warsman. Elle est obligatoire pour toutes les entreprises depuis début 2017.\nLes déclarations sociales reposaient auparavant sur des formulaires Cerfa dont le contenu était fixé par les textes fondant la collecte des données utiles aux organismes de protection sociale et à l’administration pour l’exercice de leurs missions. Non seulement les déclarants étaient amenés à fournir plusieurs fois la même information, mais ils devaient surtout fournir une information qui n’était pas naturellement produite par leur système de gestion, ce qui était source d’incohérences et d’erreurs dans les déclarations. La DSN met en œuvre une logique fondamentalement différente : elle s’approche au plus près du fait générateur des rémunérations et cotisations sociales dans le domaine de la protection sociale, la paie. Elle repose sur un modèle unique de cette dernière et un échange de données primaires de gestion entre l’émetteur, qui fait la paie, et tous les organismes et administrations qui ont besoin de ces données sociales pour recouvrer des cotisations et servir des droits. Elle opère donc un déplacement de la charge de traitement des données de l’amont (l’entreprise déclarante) vers l’aval.\nLa DSN se fait au niveau de chaque établissement avec un principe clé : chaque salarié doit apparaître dans la déclaration. Cette dernière se fait de manière mensuelle et reflète la paie du mois \\(M-1\\), avec certaines possibilités de correction.\n\n\n\n\n\n\nFigure 1: Schéma explicatif des changements apportés par la DSN. Source : Humbert-Bottin (2018).\n\n\n\n\n\n\nLa DSN présente de nombreux avantages. Elle constitue une source unique et cohérente entre administrations. Avec la DSN, on est sûr que les employeurs et les salariés sont identifiés de la même façon quel que soit l’organisme destinataire de l’information (Renne 2018).\nElle a aussi permis une forte réduction des charges pour les entreprisess (“dites le nous bien une seule fois”). Par exemple, depuis janvier 2018, les entreprises n’ont plus obligation de fournir leur effectif salarié de fin de période, celui-ci pouvant être recalculé directement par les organismes destinataires à partir des informations individuelles transmises sur les salariés (Renne 2018).\nLa fréquence mensuelle de transmission des données permet un meilleur suivi des changements infra-annuels. Auparavant, les entreprises transmettaient des données multiples à diverses échéances et à différents organismes, globalisées par établissement.\nLa DSN n’a pas vocation à servir un besoin spécifique, mais au contraire à couvrir différents usages. Les systèmes d’informations des administrations utilisatrices (Insee, DARES, Pole Emploi, etc.) reçoivent une liste spécifique de données, fixée par arrêté selon leurs missions et se sont synchronisés au fur et à mesure de l’élargissement du périmètre. Depuis 2019, la DSN est le support du prélèvement à la source pour les salariés.\n\n\n\nPlusieurs challenges se posent au moment d’utiliser les données issues de la DSN à des fins statistiques. Tout d’abord, les données sont complexes, ce qui implique un certain coût d’entrée. Elles sont aussi volumineuses (environ 1To par an, sans la fonction publique) et leur traitement requiert ainsi des ressources informatiques conséquentes et des outils adaptés. On constate bien un transfert d’une partie de la charge des entreprises vers les systèmes d’information en aval.\nAutres challenges liés à l’exploitation statistique:\n\nparvenir à relier les concepts administratifs à des réalités économiques ;\néviter les “artefacts” au sens de Bourdieu.\n\n\n\n\n\nLe Système national d’identification et du répertoire des entreprises et de leurs établissements (Sirene) est un répertoire administré par l’Insee qui centralise de l’information sur chacun des 32 millions d’établissements (dont 13 millions d’établissements actifs) existant en France. En particulier, il attribue un numéro SIREN aux entreprises, organismes et associations ainsi qu’un numéro SIRET aux établissements de ces entités.\nL’utilité du numéro SIRET est multiple. S’il constitue avant tout la preuve juridique de l’existence d’un établissement, il permet également d’effectuer un certain nombre de démarches commerciales et administratives.\nAinsi, il sert à :\n\nÉmettre des factures, mais aussi des documents commerciaux. En effet, il est obligatoire de faire apparaître le numéro sur chacun de ces documents. En outre, si l’entreprise à un site internet, le numéro doit apparaître dans les mentions légales ;\nObtenir des informations officielles sur les sociétés. Grâce au SIRET, tout prestataire ou client peut vérifier la fiabilité des données que l’entreprise lui fournit, via une recherche sur internet notamment ;\nProuver l’existence légale de la compagnie. Ce numéro permet en effet de l’identifier auprès de ses clients, prestataires, co-contractants et par l’administration fiscale ;\nProduire des statistiques à partir de la base Sirene et du numéro SIRET. En effet, ces deux éléments donnent accès à des informations capitales que l’INSEE peut réutiliser et analyser.\n\nPour la statistique publique, Sirene met à disposition des utilisateurs un code APE (pour activité principale exercée) choisi dans la Nomenclature d’activité française (NAF) pour chaque établissement (APET) et pour chaque entreprise (APEN), ainsi que sa localisation, sa catégorie juridique, son effectif salarié et l’historique des mouvements (création, cessation, etc.). Le répertoire SIRENE est aussi la base de référence pour toutes les études et enquêtes statistiques sur les entreprises.\n\n\n\nLe Fichier démographique sur les logements et les individus (Fidéli) est une base annuelle exhaustive de données statistiques sur les logements et de leurs occupants. Fidéli est en réalité un assemblage raisonné de données administratives conçu pour répondre à des finalités en matière de statistiques démographiques.\nCet appariement met en regard:\n\ndes données d’origine fiscale: fichier de la taxe d’habitation, fichier des propriétés bâties, fichiers d’imposition des personnes et fichier des déclarations de revenus. Ces données sont de nature démographique pour les personnes et la structure des ménages, ainsi que sur les revenus perçus au sein des foyers;\ndes données contextuelles pour décrire les adresses: coordonnées, appartenance à des mailles géographiques (IRIS, quartiers de la ville), etc. ;\ndes informations sur les agrégats de revenus déclarés et les montants de prestations sociales reçues.\n\nFidéli fournit des possibilités d’études poussées sur des sujets extrêmement variés et à des échelles géographiques fines. Des exemples de projets de recherche récents :\n\nDynamiques de l’organisation du territoire et des inégalités spatiales en milieux urbains pollués ;\nCaractérisation spatiale de la vulnérabilité sociale à la hausse des températures en milieu urbain ;\nEvaluation de l’impact de la majoration de la taxe d’habitation sur les résidences secondaires…\n\n\n\n\nLe Système national des données de santé (SNDS) est un entrepôt de données médico-administratives pseudonymisées couvrant l’ensemble de la population française et contenant l’ensemble des soins présentés au remboursement. Le SNDS peut être vu comme un appariement des grandes bases médico-administratives nationales, notamment :\n\nles données de l’assurance maladie (base SNIIRAM) ;\nles données des hôpitaux (base PMSI) ;\nles causes médicales de décès (base du CépiDC de l’Inserm).\n\nLe SNDS est un dispositif quasiment sans équivalent en Europe ou dans le monde. Il contient un flux annuel de 1,2 milliards de feuilles de soins, 11 millions de séjours hospitaliers et 500 millions d’actes (plus de 3000 variables) qui représentes 450 To de données.\nUne des grandes forces du SNDS est qu’il fait le lien entre médecine de ville et médecine hospitalière, ce qui permet de travailler sur les parcours de soin complets des patients pour des études, recherches ou évaluations présentant un caractère d’intérêt public. Les finalités autorisées pour les traitements sont :\n\nl’information sur la santé et l’offre de soins ;\nl’évaluation des politiques de santé ;\nl’évaluation des dépenses de santé ;\nl’information des professionnels de santé sur leur activité ;\nla veille et la sécurité sanitaires ;\nla recherche, les études, l’évaluation et l’innovation en santé.\n\n\n\n\n\n\n\nMise à disposition des données\n\n\n\nCréé par la Loi du 24 juillet 2019 relative à l’organisation et la transformation du système de santé, le Health Data Hub est un groupement d’intérêt public qui associe 56 parties prenantes, en grande majorité issues de la puissance publique (CNAM, CNRS, Haute Autorité de santé, France Assos Santé, etc.). Le Health Data Hub est en charge de mettre en œuvre les grandes orientations stratégiques relatives au Système National des Données de Santé fixées par l’Etat.\nL’offre du Health Data Hub s’articule autour de 4 enjeux stratégiques:\n\nmettre en valeur le patrimoine des données de santé, en appuyant leur collecte, leur standardisation et leur documentation, en fournissant un hébergement à l’état de l’art sécurisé et un accompagnement dans la mise en conformité RGPD ;\nfaciliter l’usage des données, en proposant un catalogue de données documentées, ainsi qu’une plateforme d’analyse et des outils à l’état de l’art ;\nprotéger les données et les citoyens, en garantissant un très haut niveau de sécurité à travers une démarche éthique de protection des données et de transparence ;\ninnover avec l’ensemble des acteurs, en développant des partenariats académiques et industriels, et en appuyant la dynamique de développement d’outils open source et de l’open data.\n\n\n\n\n\n\n\n\n\nConfidentialité et données de santé\n\n\n\nPour protéger l’identité des patients et garantir la confidentialité des données, chaque patient est repéré dans l’ensemble du SNDS par un pseudonyme, obtenu par l’application au NIR d’un procédé cryptographique irréversible appelé FOIN. Les données du SNDS sont conservées pour une durée totale de 20 ans, puis archivées pour une durée de 10 ans.\nL’accès aux données du SNDS et leur analyse ne peut se faire que dans un cadre d’hébergement très restrictif respectant le référentiel de sécurité du SNDS, afin de garantir la traçabilité des accès et des traitements, la confidentialité des données et leur intégrité.\n\n\n\n\nL’EDP-Santé est un enrichissement des données de l’échantillon démographique permanent (EDP) avec des informations issues du SNDS sur les années 2008-2022. Ce traitement a fait l’objet d’une autorisation de la CNIL et s’inscrit dans le cadre du règlement général sur la protection des données (RGPD), ainsi que la loi relative à l’informatique, aux fichiers et aux libertés (n° 78-17 du 6 janvier 1978 modifiée). Constitué dans le cadre de la stratégie nationale de santé 2018-2022, les données ne sont exploitables que par les personnes habilitées au sein de la DREES et sont conservées pour une période de 5 ans.\nL’EDP-Santé contient :\n\nles données issues de l’EDP concernent l’état civil, la situation familiale, la vie professionnelle (diplôme, situation professionnelle, données relatives à l’activité salariée) et des informations d’ordre économique (revenus, situation fiscale) ;\nles données issues du SNDS sur les recours aux soins et les données issues des certificats de décès.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#la-dsn",
    "href": "administratives_exemples.html#la-dsn",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Les DADS et la DSN\n\n\n\nDescriptions sur le site de l’Insee de la Déclaration annuelle de données sociales (DADS) et de la Déclaration sociale nominative (DSN).\n\n\nLa Déclaration sociale nominative est aujourd’hui le mode d’échanges de données sociales des entreprises vers l’administration, et concerne toutes les entreprises du secteur privé. Elle résulte d’un projet de simplification administrative qui s’est étalé sur près de dix ans : la collecte des données est adossée au processus générateur de la collecte des cotisations sociales, c’est-à-dire au processus de paie (Humbert-Bottin 2018). En plus de réduire la charge imposée aux entreprise, la DSN garantit une bien meilleure qualité et l’exhaustivité de l’information recueillie.\n\n\nLes déclarations sociales font partie des tâches administratives historiquement imposées aux entreprises françaises. La déclaration sociale nominative (DSN), née à la fin des années 2000, a été instituée par la loi de simplification du 22 mars 2012, dite loi Warsman. Elle est obligatoire pour toutes les entreprises depuis début 2017.\nLes déclarations sociales reposaient auparavant sur des formulaires Cerfa dont le contenu était fixé par les textes fondant la collecte des données utiles aux organismes de protection sociale et à l’administration pour l’exercice de leurs missions. Non seulement les déclarants étaient amenés à fournir plusieurs fois la même information, mais ils devaient surtout fournir une information qui n’était pas naturellement produite par leur système de gestion, ce qui était source d’incohérences et d’erreurs dans les déclarations. La DSN met en œuvre une logique fondamentalement différente : elle s’approche au plus près du fait générateur des rémunérations et cotisations sociales dans le domaine de la protection sociale, la paie. Elle repose sur un modèle unique de cette dernière et un échange de données primaires de gestion entre l’émetteur, qui fait la paie, et tous les organismes et administrations qui ont besoin de ces données sociales pour recouvrer des cotisations et servir des droits. Elle opère donc un déplacement de la charge de traitement des données de l’amont (l’entreprise déclarante) vers l’aval.\nLa DSN se fait au niveau de chaque établissement avec un principe clé : chaque salarié doit apparaître dans la déclaration. Cette dernière se fait de manière mensuelle et reflète la paie du mois \\(M-1\\), avec certaines possibilités de correction.\n\n\n\n\n\n\nFigure 1: Schéma explicatif des changements apportés par la DSN. Source : Humbert-Bottin (2018).\n\n\n\n\n\n\nLa DSN présente de nombreux avantages. Elle constitue une source unique et cohérente entre administrations. Avec la DSN, on est sûr que les employeurs et les salariés sont identifiés de la même façon quel que soit l’organisme destinataire de l’information (Renne 2018).\nElle a aussi permis une forte réduction des charges pour les entreprisess (“dites le nous bien une seule fois”). Par exemple, depuis janvier 2018, les entreprises n’ont plus obligation de fournir leur effectif salarié de fin de période, celui-ci pouvant être recalculé directement par les organismes destinataires à partir des informations individuelles transmises sur les salariés (Renne 2018).\nLa fréquence mensuelle de transmission des données permet un meilleur suivi des changements infra-annuels. Auparavant, les entreprises transmettaient des données multiples à diverses échéances et à différents organismes, globalisées par établissement.\nLa DSN n’a pas vocation à servir un besoin spécifique, mais au contraire à couvrir différents usages. Les systèmes d’informations des administrations utilisatrices (Insee, DARES, Pole Emploi, etc.) reçoivent une liste spécifique de données, fixée par arrêté selon leurs missions et se sont synchronisés au fur et à mesure de l’élargissement du périmètre. Depuis 2019, la DSN est le support du prélèvement à la source pour les salariés.\n\n\n\nPlusieurs challenges se posent au moment d’utiliser les données issues de la DSN à des fins statistiques. Tout d’abord, les données sont complexes, ce qui implique un certain coût d’entrée. Elles sont aussi volumineuses (environ 1To par an, sans la fonction publique) et leur traitement requiert ainsi des ressources informatiques conséquentes et des outils adaptés. On constate bien un transfert d’une partie de la charge des entreprises vers les systèmes d’information en aval.\nAutres challenges liés à l’exploitation statistique:\n\nparvenir à relier les concepts administratifs à des réalités économiques ;\néviter les “artefacts” au sens de Bourdieu.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#sirene",
    "href": "administratives_exemples.html#sirene",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Le Système national d’identification et du répertoire des entreprises et de leurs établissements (Sirene) est un répertoire administré par l’Insee qui centralise de l’information sur chacun des 32 millions d’établissements (dont 13 millions d’établissements actifs) existant en France. En particulier, il attribue un numéro SIREN aux entreprises, organismes et associations ainsi qu’un numéro SIRET aux établissements de ces entités.\nL’utilité du numéro SIRET est multiple. S’il constitue avant tout la preuve juridique de l’existence d’un établissement, il permet également d’effectuer un certain nombre de démarches commerciales et administratives.\nAinsi, il sert à :\n\nÉmettre des factures, mais aussi des documents commerciaux. En effet, il est obligatoire de faire apparaître le numéro sur chacun de ces documents. En outre, si l’entreprise à un site internet, le numéro doit apparaître dans les mentions légales ;\nObtenir des informations officielles sur les sociétés. Grâce au SIRET, tout prestataire ou client peut vérifier la fiabilité des données que l’entreprise lui fournit, via une recherche sur internet notamment ;\nProuver l’existence légale de la compagnie. Ce numéro permet en effet de l’identifier auprès de ses clients, prestataires, co-contractants et par l’administration fiscale ;\nProduire des statistiques à partir de la base Sirene et du numéro SIRET. En effet, ces deux éléments donnent accès à des informations capitales que l’INSEE peut réutiliser et analyser.\n\nPour la statistique publique, Sirene met à disposition des utilisateurs un code APE (pour activité principale exercée) choisi dans la Nomenclature d’activité française (NAF) pour chaque établissement (APET) et pour chaque entreprise (APEN), ainsi que sa localisation, sa catégorie juridique, son effectif salarié et l’historique des mouvements (création, cessation, etc.). Le répertoire SIRENE est aussi la base de référence pour toutes les études et enquêtes statistiques sur les entreprises.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#fidéli",
    "href": "administratives_exemples.html#fidéli",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Le Fichier démographique sur les logements et les individus (Fidéli) est une base annuelle exhaustive de données statistiques sur les logements et de leurs occupants. Fidéli est en réalité un assemblage raisonné de données administratives conçu pour répondre à des finalités en matière de statistiques démographiques.\nCet appariement met en regard:\n\ndes données d’origine fiscale: fichier de la taxe d’habitation, fichier des propriétés bâties, fichiers d’imposition des personnes et fichier des déclarations de revenus. Ces données sont de nature démographique pour les personnes et la structure des ménages, ainsi que sur les revenus perçus au sein des foyers;\ndes données contextuelles pour décrire les adresses: coordonnées, appartenance à des mailles géographiques (IRIS, quartiers de la ville), etc. ;\ndes informations sur les agrégats de revenus déclarés et les montants de prestations sociales reçues.\n\nFidéli fournit des possibilités d’études poussées sur des sujets extrêmement variés et à des échelles géographiques fines. Des exemples de projets de recherche récents :\n\nDynamiques de l’organisation du territoire et des inégalités spatiales en milieux urbains pollués ;\nCaractérisation spatiale de la vulnérabilité sociale à la hausse des températures en milieu urbain ;\nEvaluation de l’impact de la majoration de la taxe d’habitation sur les résidences secondaires…",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "administratives_exemples.html#snds",
    "href": "administratives_exemples.html#snds",
    "title": "Quelques exemples approfondis de données administratives",
    "section": "",
    "text": "Le Système national des données de santé (SNDS) est un entrepôt de données médico-administratives pseudonymisées couvrant l’ensemble de la population française et contenant l’ensemble des soins présentés au remboursement. Le SNDS peut être vu comme un appariement des grandes bases médico-administratives nationales, notamment :\n\nles données de l’assurance maladie (base SNIIRAM) ;\nles données des hôpitaux (base PMSI) ;\nles causes médicales de décès (base du CépiDC de l’Inserm).\n\nLe SNDS est un dispositif quasiment sans équivalent en Europe ou dans le monde. Il contient un flux annuel de 1,2 milliards de feuilles de soins, 11 millions de séjours hospitaliers et 500 millions d’actes (plus de 3000 variables) qui représentes 450 To de données.\nUne des grandes forces du SNDS est qu’il fait le lien entre médecine de ville et médecine hospitalière, ce qui permet de travailler sur les parcours de soin complets des patients pour des études, recherches ou évaluations présentant un caractère d’intérêt public. Les finalités autorisées pour les traitements sont :\n\nl’information sur la santé et l’offre de soins ;\nl’évaluation des politiques de santé ;\nl’évaluation des dépenses de santé ;\nl’information des professionnels de santé sur leur activité ;\nla veille et la sécurité sanitaires ;\nla recherche, les études, l’évaluation et l’innovation en santé.\n\n\n\n\n\n\n\nMise à disposition des données\n\n\n\nCréé par la Loi du 24 juillet 2019 relative à l’organisation et la transformation du système de santé, le Health Data Hub est un groupement d’intérêt public qui associe 56 parties prenantes, en grande majorité issues de la puissance publique (CNAM, CNRS, Haute Autorité de santé, France Assos Santé, etc.). Le Health Data Hub est en charge de mettre en œuvre les grandes orientations stratégiques relatives au Système National des Données de Santé fixées par l’Etat.\nL’offre du Health Data Hub s’articule autour de 4 enjeux stratégiques:\n\nmettre en valeur le patrimoine des données de santé, en appuyant leur collecte, leur standardisation et leur documentation, en fournissant un hébergement à l’état de l’art sécurisé et un accompagnement dans la mise en conformité RGPD ;\nfaciliter l’usage des données, en proposant un catalogue de données documentées, ainsi qu’une plateforme d’analyse et des outils à l’état de l’art ;\nprotéger les données et les citoyens, en garantissant un très haut niveau de sécurité à travers une démarche éthique de protection des données et de transparence ;\ninnover avec l’ensemble des acteurs, en développant des partenariats académiques et industriels, et en appuyant la dynamique de développement d’outils open source et de l’open data.\n\n\n\n\n\n\n\n\n\nConfidentialité et données de santé\n\n\n\nPour protéger l’identité des patients et garantir la confidentialité des données, chaque patient est repéré dans l’ensemble du SNDS par un pseudonyme, obtenu par l’application au NIR d’un procédé cryptographique irréversible appelé FOIN. Les données du SNDS sont conservées pour une durée totale de 20 ans, puis archivées pour une durée de 10 ans.\nL’accès aux données du SNDS et leur analyse ne peut se faire que dans un cadre d’hébergement très restrictif respectant le référentiel de sécurité du SNDS, afin de garantir la traçabilité des accès et des traitements, la confidentialité des données et leur intégrité.\n\n\n\n\nL’EDP-Santé est un enrichissement des données de l’échantillon démographique permanent (EDP) avec des informations issues du SNDS sur les années 2008-2022. Ce traitement a fait l’objet d’une autorisation de la CNIL et s’inscrit dans le cadre du règlement général sur la protection des données (RGPD), ainsi que la loi relative à l’informatique, aux fichiers et aux libertés (n° 78-17 du 6 janvier 1978 modifiée). Constitué dans le cadre de la stratégie nationale de santé 2018-2022, les données ne sont exploitables que par les personnes habilitées au sein de la DREES et sont conservées pour une période de 5 ans.\nL’EDP-Santé contient :\n\nles données issues de l’EDP concernent l’état civil, la situation familiale, la vie professionnelle (diplôme, situation professionnelle, données relatives à l’activité salariée) et des informations d’ordre économique (revenus, situation fiscale) ;\nles données issues du SNDS sur les recours aux soins et les données issues des certificats de décès.",
    "crumbs": [
      "Home",
      "Données administratives",
      "Quelques exemples approfondis de données administratives"
    ]
  },
  {
    "objectID": "geolocalized_data.html",
    "href": "geolocalized_data.html",
    "title": "Données géolocalisées",
    "section": "",
    "text": "Disposer de données géolocalisées pour produire de la statistique publique est un besoin qui se fait de plus en plus fort. Pour cause, un intérêt croissant est accordé aux caractéristiques spatiales des phénomènes que la statistique publique a pour rôle de décrire. Le comité d’experts des Nations Unies sur la gestion de l’information géospatiale mondiale (UN-GGIM) a d’ailleurs reconnu l’importance cruciale d’intégrer les informations géospatiales aux statistiques et aux données socio-économiques et le développement d’une infrastructure statistique géospatiale.\nLa production et la diffusion accrue de données géolocalisées dépasse le cadre de la statistique publique. La généralisation de traces numériques géolocalisées (données mobile, GPS, localisation d’adresses IP…) a entraîné une multiplication des acteurs valorisant des données spatiales. Certains acteurs de l’écosystème de la donnée sont spécialisés dans la collecte ou la valorisation de sources géolocalisées collectées par d’autres.\nUn premier apport fondamental des données géolocalisées est qu’elles permettent de calculer des indicateurs avec une granularité spatiale plus fine que les découpages administratifs ou historiques classiques. Cette approche permet d’éclairer des phénomènes socio-économiques locaux comme les problématiques de mixité (Galiana, Sémécurbe, et al. 2020). L’Insee met à disposition en open-data des données très fines sur une grande variété de facteur. Les sites officiels geoportail et statistiques-locales.insee.fr ou encore les sites faits par des tiers comme celui d’Etienne Côme ou hubblo permettent d’explorer la richesse des sources fines mises à disposition. Pour désigner les sources les plus fines, on parle de données carroyées, publiées sur des carreaux pouvant aller de 200 mètres à plusieurs kilomètres de côté (voir Figure 1). Une telle granularité permet de capter certains phénomènes démographiques ou socio-économiques qui ne sont pas détectables au niveau de l’IRIS ou de la commune1.\n\n\n\n\n\n\nFigure 1: Carte des densités de population sur des carreaux de largeur d’un kilomètre à Lyon et ses alentours en 2017 (calculées à partir de Filosofi). Source : géoportail.",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Données géolocalisées"
    ]
  },
  {
    "objectID": "geolocalized_data.html#footnotes",
    "href": "geolocalized_data.html#footnotes",
    "title": "Données géolocalisées",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLe projet gridviz porté par Eurostat vise à proposer un outil facilitant la construction de mosaiques agrégées à partir de données spatiales.↩︎",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Données géolocalisées"
    ]
  },
  {
    "objectID": "series_temporelles.html",
    "href": "series_temporelles.html",
    "title": "Nowcasting",
    "section": "",
    "text": "La statistique publique publiant des statistiques à intervalle régulier, une longue tradition de production et d’exploitation de séries temporelles précède les innovations récentes. La constance de la méthodologie, ou les harmonisations faites a posteriori - par exemple la technique de la rétropolation en comptabilité nationale - assure en théorie une forme de comparabilité et permet de considérer les productions statistiques comme des séries temporelles. Parmi les productions statistiques dont la formalisation a été la plus précoce, la comptabilité nationale tient une bonne place. Le Système de Comptabilité Nationale (SCN) est ainsi un cadre international harmonisé qui permet la construction de séries temporelles depuis l’après-guerre. La France est l’un des pays pour lesquels il est possible de remonter le plus loin dans le passé avec des séries harmonisées depuis XXXX.\nEn raison de délais imposés par la collecte et le traitement des données, certains indicateurs supposés donner des informations sur la situation actuelle sont publiés avec du retard et ne peuvent pas jouer leur rôle dans la prise de décision publique. C’est pourquoi la statistique publique participe aussi à la réalisation de prévisions à court terme de valeurs d’indicateurs macro-économiques. Si la production de séries statistiques récurrentes fait partie des missions de tous les instituts statistiques, l’Insee a également des missions plus spécifiques dans le domaine des séries temporelles. La construction d’indicateurs conjoncturels prospectifs, au service du débat public et de la prise de décision politique, en fait partie. A l’Insee cette mission prospective est assurée par le département de la conjoncture qui construit des indicateurs et des analyses prospectifs sur l’activité économiques des prochains trimestres. Ce département mobilise historiquement des données dont la remontée est plus rapide que celles utilisées pour la construction des agrégats macroéconomiques de la comptabilité nationale. Cependant, la collecte accrue de traces numériques a permis l’accès à des données à haute fréquence pouvant être mobilisées pour disposer de signaux sur la situation macroéconomique actuelle ou très récente.\nDans ce cadre, l’Insee, QuantCube1, Paris School of Economics2, CANDRIAM et la Société Générale ont créé une Chaire de recherche Mesures de l’économie, nowcasting ‐ au‐delà du PIB en 2021. Cette Chaire a pour objectif de travailler sur l’amélioration des prévisions économiques, en particulier grâce à la mobilisation de nouvelles sources de données. Parmi ces nouvelles sources, on trouve les actualités (Bortoli, Combes, and Renault 2018), les médias sociaux, les données satellitaires, les réseaux professionnels et les avis de consommateurs, ainsi que les données sur le commerce international, la consommation d’électricité et le transport routier (Fornaro 2020), le transport maritime, l’immobilier, l’hôtellerie et les télécommunications. Un autre objectif de la Chaire est de travailler sur la mesure de nouveaux indicateurs économiques, de bien-être ou de développement durable (au‐delà du PIB), ici encore en utilisant ces données nouvelles.\nCôté technique, des modèles autorégressifs de type bridge models ou mixed-data sampling (Schumacher 2016), des dynamic factor models (Stock and Watson 2010) ou plus récemment des modèles de Deep Learning de type LSTM (Hopp 2021) sont souvent utilisés pour combiner des indicateurs de type soft comme le climat des affaires ou le sentiment des consommateurs avec des indicateurs hard comme la production industrielle, le commerce de détail, les prix de l’immobilier, etc. à différentes fréquences. La littérature fait particulièrement état de l’utilisation de deux sources de données massive permettant d’obtenir des indicateurs soft (Richardson 2018), même si bien d’autres sources ont été expertisées :\n‑ les statistiques de recherches sur Internet basées sur la fréquence de recherche de mots‑clés ou de sujets spécifiques ; ‑ les médias sociaux sur Internet (Twitter).",
    "crumbs": [
      "Home",
      "Nowcasting",
      "Nowcasting"
    ]
  },
  {
    "objectID": "series_temporelles.html#footnotes",
    "href": "series_temporelles.html#footnotes",
    "title": "Nowcasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nStart‐up proposant des prévisions macroéconomiques fondées sur le Big Data et l’intelligence artificielle↩︎\nsociété internationale de gestion d’actifs↩︎\nDans le domaine de la monétique, le Groupement des cartes bancaires est un groupement d’intérêt économique privé qui réunit la plupart des établissements financiers français dans le but d’assurer l’interbancarité des cartes de paiement.↩︎",
    "crumbs": [
      "Home",
      "Nowcasting",
      "Nowcasting"
    ]
  },
  {
    "objectID": "nowcasting_exemples.html",
    "href": "nowcasting_exemples.html",
    "title": "Application",
    "section": "",
    "text": "L’objectif de ce TP est de montrer comment récupérer des données à partir de l’API de Twitter et de montrer un début d’exemple d’exploitation de ces données.\nLe TP peut être lancé sur le SSP Cloud en cliquant sur le bouton suivant:\n\nUn second bouton si l’installation de Pytorch est trop longue:",
    "crumbs": [
      "Home",
      "Nowcasting",
      "Application"
    ]
  },
  {
    "objectID": "geolocalized_data_exemples.html",
    "href": "geolocalized_data_exemples.html",
    "title": "Application",
    "section": "",
    "text": "L’objectif de ce TP est de donner un exemple introductif d’utilisation de données géolocalisées dans le cadre de la production de statistique publique. Plus précisément, il propose de manipuler les données du Registre Parcellaire Graphique (RPG), une base de données géographiques administrative servant de référence à l’instruction des aides de la politique agricole commune (PAC). Ces données sont mises en regard:\n\ndes données du Drias de simulation de l’évolution climatique pour le siècle en cours sur la France;\ndes données ERA5 agro-météorologiques de surface quotidiens pour la période allant de 1979 à aujourd’hui.\n\nLe TP composé de plusieurs applications décrites ci-dessous et disponibles sur ce site.\nUn environnement de travail peut être lancé sur le SSP Cloud en cliquant sur le bouton suivant:\n\n\nApplication 0 (optionnelle) : Création d’une base de données PostgreSQL\nLes données du RPG étant volumineuses (on a vu que c’était souvent le cas pour les données administratives), il faut pouvoir les requêter depuis une base de données. Les données sont disponibles dans une base de données PostgreSQL (avec l’extension pour données spatiales PostGIS) prête à l’emploi. Néanmoins, cette application explique comment procéder pour créer une telle base de données sur la plateforme SSP Cloud.\n\n\nApplication 1 : Première manipulation du RPG\nL’objectif de cette première étape est d’effectuer des premières requêtes géographiques permettant d’examiner les cultures à proximité d’un point géographique donné, et de comparer la composition observée avec les compositions départementale, régionale, etc. On propose également de mettre au point une interface de type tableau de bord permettant d’obtenir ces informations interactivement.\n\n\nApplication 2 : Exposition des cultures au déficit de précipitations\nL’objectif de cette application est de mettre en regard cultures et prévisions climatiques localement, pour identifier des cultures particulièrement mises en danger par le changement climatique en France.\n\n\nApplication 3 : Evolution des cultures, lien avec le climat passé\nAprès avoir regardé vers l’avenir, il est temps de jeter un coup d’oeil dans le rétroviseur, et de regarder comment l’évolution des températures au cours des 40 dernières années a pu influencer certaines cultures en France. On estimera l’évolution des dates potentielles de récolte du maïs grain dans les différents bassins de productions français depuis 1980.",
    "crumbs": [
      "Home",
      "Données géolocalisées",
      "Application"
    ]
  },
  {
    "objectID": "images.html",
    "href": "images.html",
    "title": "Images",
    "section": "",
    "text": "Les images sont des données qui sont utilisées depuis longtemps de manière automatique. Une image pour un ordinateur est représentée par un tableau en 2 ou 3 dimensions (images en nuances de gris et images en couleur respectivement). En 2 dimensions, l’image a ainsi une longueur \\(L\\) et une largeur \\(W\\) : elle est constituée de \\(L \\times W\\) pixels, chacun associé à une valeur entière comprise entre 0 et 255 (ou parfois à une valeur décimale comprise entre 0 et 1), comme illustré en Figure 1.\n\n\n\n\n\n\nFigure 1: Représentation du logo de Python en nuances de gris avec une faible résolution. La valeur de chaque pixel (entier allant de 0 pour un pixel complètement noir à 255 pour un pixel complètement blanc) figure à l’emplacement de ce dernier.\n\n\n\nUne image en couleur est constituée de 3 canaux (RGB pour Red, Green et Blue). Chacun des \\(L \\times W\\) pixels de l’image est ainsi associé à 3 valeurs entières comprises entre 0 et 225 (ou à 3 valeurs décimales comprises entre 0 et 1), comme illustré en Figure 2.\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 2: Représentation du logo de Python en couleurs. L’image du haut correspond à la superposition des trois canaux représentés sur la rangée inférieure.\n\n\n\nLe domaine de la vision par ordinateur (computer vision) a vu le jour dans les années 1960 avec le développement des premiers algorithmes cherchant à extraire de l’information d’images. Par exemple, Sobel and Feldman (1973) introduit la méthode suivante pour faire de la détection de contours sur une image \\(A\\).\nOn calcule\n\\[\nG_x = \\begin{bmatrix}\n+1 & 0 & -1\\\\\n+2 & 0 & -2\\\\\n+1 & 0 & -1\n\\end{bmatrix} \\star A \\quad \\text{et} \\quad G_y = \\begin{bmatrix}\n+1 & +2 & +1\\\\\n0 & 0 & 0\\\\\n-1 & -2 & -1\n\\end{bmatrix} \\star A\n\\]\noù \\(\\star\\) est l’opérateur de convolution 2-dimensionnel en traitement du signal (illustré en Figure 4).\nAlors l’image \\(G = \\sqrt{G_x^2 + G_y^2}\\) fournit une représentation des contours de l’image \\(A\\). Une illustration de l’application de cette méthode est donnée en Figure 3.\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 3: L’image de droite est obtenue par application sur l’image de gauche de la méthode de détection de contours introduite par Sobel and Feldman (1973). Source : Wikipedia.\n\n\n\n\n\n\n\n\n\nFigure 4: Illustration de l’opérateur de convolution 2-dimensionnel \\(\\star\\). Le noyau (matrice en bleu sur le dessin) est multiplié par -1 et glisse sur la matrice de gauche. Une multiplication élément par élément est faite sur chaque sous-matrice de la taille du noyau. Pour chacune de ces multiplication, les coefficients sont ensuite sommés pour donner une valeur de sortie unique. Par exemple ici, la valeur du pixel en vert correspond au calcul \\(3 = 1*(-1) + 1*1 + 1*2 + 1*1\\).",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#réseaux-de-neurone-convolutifs",
    "href": "images.html#réseaux-de-neurone-convolutifs",
    "title": "Images",
    "section": "Réseaux de neurone convolutifs",
    "text": "Réseaux de neurone convolutifs\nUne architecture de modèles a joué un rôle particulièrement important dans cette révolution : les réseaux de neurones convolutifs (voir LeCun et al. 1989 pour un des articles fondateurs). Ces réseaux de neurones sont constitués d’un enchaînement de couches convolutives, chacune composée de trois étapes :\n\nUne étape de convolution utilisant l’opérateur \\(\\star\\) décrit ci-dessus qui transforme un tenseur 3-dimensionnel de taille \\((H, W, C)\\) en entrée en un tenseur de taille \\((H', W', C')\\) ou \\(H'\\), \\(W'\\) et \\(C'\\) dépendent de la taille du noyau de convolution choisi ;\nUne étape de détection où une fonction non-linéaire est appliquée au tenseur obtenu en sortie de l’étape de convolution ;\nUne étape de pooling où chaque canal du tenseur en entrée voit sa hauteur et largeur réduite à l’aide une fonction qui remplace chaque valeur par une statistique impliquant les valeurs des pixels voisins (fréquemment, la valeur maximale dans un voisinage rectangulaire : c’est l’opération de max pooling).\n\nLa succession de ces opérations est résumée dans la Figure 5\n\n\n\n\n\n\nFigure 5: Illustration d’une succession de séquences d’un réseau convolutionnel. Emprunté à https://www.analyticsvidhya.com/blog/2022/01/convolutional-neural-network-an-overview/\n\n\n\nLes tenseurs obtenus en sortie des couches convolutives sont appelés activation maps ou feature maps. Chaque feature map peut s’interpréter comme une carte qui indique les endroits où on peut trouver une feature particulière (par exemple un bord, une texture, une partie d’un objet, etc.) au sein de l’image. Les features pertinentes (c’est-à-dire les coefficients des filtres de convolution utilisés) sont apprises par le réseau de neurones au cours de la phase d’entraînement. On peut voir ces features comme des structures latentes qui combinées ensemble génèrent un objet sur l’image finale.\nLes réseaux de neurones convolutifs présentent plusieurs caractéristiques essentielles pour des tâches de vision par ordinateur, qui expliquent en partie leur succès : une invariance (relative) à la translation, la rotation et à l’échelle. Ces caractéristiques permettent aux modèles d’abstraire l’identité d’un objet de détails spécifiques aux images données en entrée tels que la position et l’orientation de cet objet par rapport à la caméra.",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#segmentation-sémantique",
    "href": "images.html#segmentation-sémantique",
    "title": "Images",
    "section": "Segmentation sémantique",
    "text": "Segmentation sémantique\nLa segmentation sémantique est une tâche de vision par ordinateur qui consiste à associer une étiquette ou une catégorie à chaque pixel d’une image (illustration en Figure 7). Plusieurs architectures de réseaux de neurones convolutifs entraînées sur des gros jeux d’entraînement obtiennent des performances très élevées sur des jeux de données d’évaluation de référence, comme l’architecture DeepLabV3 (Chen et al. 2017). Les principaux frameworks de Deep Learning fournissent des implémentations de modèles de segmentation sémantique (avec ou sans coefficients pré-entraînés) : c’est le cas du package Python torchvision par exemple qui propose une implémentation des modèles DeepLabV3, FCN et LRASPP.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Segmentation sémantique effectuée sur une photo de chat (partie gauche de la Figure). Sur le masque de segmentation (partie droite de la Figure), les pixels verts sont associés à la classe chat tandis que les pixels roses sont associés à la classe arrière-plan. Source : Hugging Face.\n\n\n\n\n\n\n\n\n\nFigure 7: Un autre exemple de segmentation sémantique, issu de ce blog",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#utilisation-de-données-dobservation-satellitaire",
    "href": "images.html#utilisation-de-données-dobservation-satellitaire",
    "title": "Images",
    "section": "Utilisation de données d’observation satellitaire",
    "text": "Utilisation de données d’observation satellitaire\n\nNature de la donnée\nDans le domaine des données d’Earth Observation, qui regroupent en fait différentes sources de données (radars, orthophotographies…), les données photographiques issues de satellites ont une place de choix. Celles-ci permettent d’observer les territoires, que ce soit leur topologie ou leur usage et potentiellement d’en tirer des enseignements à diffuser sous la forme de statistiques publiques. Par exemple, l’utilisation de données satellitaires peut permettre d’améliorer la granularité spatiale et temporelles de statistiques publiées aujourd’hui sur la production agricole (part du territoire cultivé, nature des cultures…).\nDe manière générale, ces données ont beaucoup de potentiel lorsqu’elles sont utilisées en combinaison avec d’autres sources de données lorsqu’il s’agit de pallier des insuffisances ou des manques concernant les données traditionnellement utilisées pour la statistique publique. Par exemple, Steele et al. (2017) combinent données de satellites et données de téléphonie mobile pour estimer des taux de pauvreté. En France, les départements et régions d’outre-mer sont particulièrement concernés. Les données satellites permettraient d’y combler des imperfections des données administratives. Par exemple, les parcelles cadastrales y sont parfois mal identifiées ou rarement mises à jour. Les données satellitaires peuvent être utilisées pour fiabiliser cette information.\n\n\n\n\n\n\nNote\n\n\n\nLes données d’Earth Observation présentent des difficultés d’utilisation non-négligeables dans un contexte de production statistique :\n\nIl faut au moment de la production de la statistique désirée s’assurer que l’on parvient à des résultats statistiquement robustes ;\nProduire des statistiques de manière récurrente à partir d’une source de données demande d’avoir du recul sur le fonctionnement de la chaîne de traitement en production. Comme les données d’Earth Observation ne sont aujourd’hui utilisées que par peu d’instituts statistiques, il est difficile d’avoir un tel recul sans soi-même avoir une chaîne de traitement qui tourne depuis plusieurs années ;\nPour de nombreuses applications, on souhaite utiliser des images avec une résolution élevée mais aussi exploiter la haute fréquence temporelle de passage de certains satellites. Dans un tel cadre les données d’Earth Observation ont souvent un volume très important. Entraîner des modèles pertinents (les modèles de Deep Learning state-of-the-art sont complexes) demande d’avoir des ressources informatiques adaptées à disposition ;\nSelon les besoins, la résolution disponible peut ne pas correspondre aux besoins de la statistique.\n\n\n\n\n\nFournisseurs de données\nDes acteurs publient des données satellitaires en open data :\n\nLa NASA à travers son programme historique Landsat. Les dernières générations des satellites Landsat recueillent des images dans une dizaine de bandes spectrales (bandes visibles mais aussi bandes infrarouges) avec une résolution spatiale de 30 mètres (pour les bandes visibles) ;\nL’Agence spatiale européenne (ESA) a lancé le programme Sentinel-2 en 2015. Les images des satellites Sentinel-2 sont aussi disponibles en open data, sur 12 bandes avec une résolution spatiale de 10 mètres, plus fine que celle des images de Landsat. La périodicité de la couverture des satellites Sentinel-2 est relativement faible : ces derniers repassent au-dessus des mêmes zones tous les cinq jours.\n\nDes entreprises privées collectent aussi des images avec leurs propres satellites, parfois avec des meilleures résolutions que les images disponibles en libre accès, ce qui peut être nécessaire en fonction du cas d’usage envisagé. De manière générale, il y a toutefois un arbitrage à faire entre le détail local des mesures (résolution radiométrique, nombre de bandes spectrales) et la résolution spatiale des images. La richesse des images issues de satellites réside plutôt dans la première dimensions, alors que les orthophotographies par exemple sont à privilégier si on désire une plus haute résolution spatiale.\n\n\nPipeline\nLe traitement d’images de satellites se divise de manière classique en trois parties (Direction de la recherche et de l’innovation 2018) :\n\nd’abord vient le pré-traitement des données, qui inclut le stockage, le data managment, le contrôle de la qualité des données, l’inclusion d’autres sources et l’identification d’outils appropriés pour l’analyse.\nCe pré-traitement est suivi par une phase d’analyse, où l’on définit les indicateurs à calculer, les données à utiliser et où l’on applique la méthode analytique choisie.\nEnfin, au cours de la phase d’évaluation, on collecte et on interprète les résultats de l’analyse.\n\nDes méthodes historiques existent pour analyser des images de satellites (pour in fine produire des statistiques). Par exemple, l’utilisation de modèle physiques pour prédire la valeur d’une variable d’intérêt à partir de l’observation empirique de certaine bandes, ou encore de méthodes d’analyse d’images traditionnelles où des informations spatiales, relatives à des motifs, à des textures, etc. sert à segmenter l’image sous supervision humaine (OBIA). Récemment, le Machine Learning (et en particulier le Deep Learning) a fourni des outils d’analyse puissants facilement applicables aux images satellites.\n\n\nCas d’usage\nLes cas d’usage potentiels d’utilisation de ces données pour la statistique publique touchent de nombreux thèmes, qui incluent :\n\nLa supervision des forêts, de l’agriculture, des masses d’eau ;\nL’urbanisation et les infrastructures ;\nLa pollution environnementale et la qualité de l’air atmosphérique ;\n\nEn particulier, l’analyse d’images satellite peut permettre de calculer des indicateurs comme la proportion de surface agricole en agriculture intensive ou en agriculture durable, le pourcentage de masses d’eau présentant une bonne qualité de l’eau ambiante, la couverture forestière dans le cadre d’une gestion forestière durable, la perte nette permanente de forêts, etc.\nPlusieurs cas d’usage précis ont été ciblés aujourd’hui pour la statistique publique en France et donnent ou vont donner lieu à des travaux expérimentaux.\n\nUn des cas d’usage identifiés depuis un moment déjà est l’utilisation d’images satellites pour calculer les statistiques sur l’occupation et l’usage des sols sur le territoire français. Aujourd’hui, ces statistiques sont tirées de l’enquête Teruti conduite par le Bureau des statistiques structurelles environnementales et forestières du SSP (Ministère de l’Agriculture).\nUn échantillon de points est observé sur le terrain sur un cycle de 3 ans permettant d’estimer l’occupation des sols avec une précision qui reste satisfaisante à l’échelon départemental. L’échantillonnage des points se fait à partir de sources multiples, dont des données satellitaires (satellite SPOT) et des orthophotographies de l’IGN. En outre, une phase de validation des résultats de l’enquête est réalisée à partir d’une couche d’exploitation du sol issue de données de Sentinel-2 et réalisée de manière automatique est le Centre d’Etudes Spatiales sur la BIOsphere à Toulouse.\nDes travaux sont actuellement en cours pour encore davantage améliorer la phase d’échantillonnage à l’aide d’images satellitaires. En outre, une méthode automatique donnant des couches d’exploitation des sols avec une précision suffisante pour les besoins de la statistique publique pourrait permettre de diffuser des statistiques plus régulièrement qu’avec l’enquête Teruti et avec une granularité territoriale plus fine.\nLes parcelles cadastrales sont parfois mal identifiées dans les départements et région d’outre-mer, en particulier en Guyane et à Mayotte. Or ces parcelles sont utilisées pour des tirages d’échantillon par l’Insee, pour le recensement de la population par exemple.\nIci encore, des modèles de segmentation retournant des couches d’exploitation et d’usage des sols peuvent être utilisés pour consolider l’information disponible sur les parcelles cadastrales. Dans le cadre d’une expérimentation, un modèle de segmentation U-Net (Ronneberger, Fischer, and Brox 2015) pré-entraîné sur le jeu de données ImageNet a été fine-tuné sur un sous-échantillon du jeu annoté S2GLC (Sentinel-2 Global Land Cover). Ce modèle prend en entrée une image satellite et renvoie une prédiction pixel par pixel de la catégorie de terrain (en 10 classes), comme illustré en Figure 8. S’il est assez précis sur la catégorie surfaces artificielles et construction, ses prédictions pourraient servir à consolider les données cadastrales.\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 8: À gauche : image satellite issue de Sentinel-2. Au milieu : segmentation prédite par le modèle U-Net. À droite : vraie segmentation de l’image.\n\n\n\n\nL’enquête sur la structure des exploitations agricoles (Bureau des statistiques structurelles environnementales et forestières du SSP) dont la prochaine édition aura lieu en 2023 pose des questions sur les vergers. Il n’existe aujourd’hui pas de source administrative permettant de consolider les résultats de l’enquête sur cette thématique.Ainsi, un projet d’expérimentation utilisant des orthophotographies pour dénombrer le nombre d’arbres et la surface associée est envisagé. La librairie DeepForest propose des modèles pré-entraînés pour faire de la détection d’arbres (voir Figure 9) et pourra servir de point de départ pour cette expérimentation.\n\n\n\n\n\n\n\nFigure 9: Détection d’arbres sur une orthophotographie à l’aide de la librairie DeepForest.\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nPlusieurs questions méthodologiques essentielles se posent lorsqu’on exploite des données satellitaires grâce à des méthodes de Deep Learning :\n\nArchitectures des modèles\nUtilisation des différentes bandes\nPré-traitements sur les images : détection et suppression de nuages, amélioration de la résolution ;\nTransférabilité des modèles : est-ce qu’un modèle entraîné sur des images provenant d’un satellite fonctionnera correctement avec des images provenant d’un autre satellite ? Ou avec un réentraînement minimal ?\n\nUn enjeu majeur est l’obtention de données annotées (même si le pré-entraînement de modèles sur des jeux de données énormes réduit le besoin de données annotées pour la tâche considérée). Pour des tâches de prédiction de l’utilisation du sol, on peut par exemple mobiliser la base de données géographiques CORINE Land Cover, un inventaire biophysique qui fournit une photographie complète de l’occupation des sols, à des fréquences régulières.\nElle est issue de l’interprétation visuelle d’images satellitaires, avec des données complémentaires d’appui. Les classes d’occupation correspondent à une nomenclature comportant 44 postes.",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#la-reconnaissance-optique-de-caractères",
    "href": "images.html#la-reconnaissance-optique-de-caractères",
    "title": "Images",
    "section": "La reconnaissance optique de caractères",
    "text": "La reconnaissance optique de caractères\nL’administration française a été historiquement une grande productrice de fichiers sous format papier. Même si la numérisation des sources de collectes administratives réduit le volume de production papier, ce dernier mode de collecte est encore d’usage. Afin de réduire le temps de numérisation, il est donc utile de mettre en oeuvre des routines automatisées. Dans la même veine, l’administration a longtemps mis en oeuvre des publications (tableaux ou graphiques) sous format papier. Être en mesure de valoriser ce patrimoine de connaissance est un enjeu pour la recherche.\nLa reconnaissance optique de caractères (souvent abrégée par OCR pour Optical character recognition) désigne la tâche de conversion de texte manuscrit ou imprimé en texte encodé par un ordinateur. C’est une tâche essentielle pour exploiter des documents disponibles sous la forme d’images numériques.\nDévelopper son propre moteur d’OCR est une tâche très complexe mais heureusement des moteurs open source existent. Tesseract est un logiciel pour la reconnaissance de caractères open source depuis 2015. Tesseract offre plusieurs moteurs depuis sa version 4 : en plus du moteur historique, un moteur basé sur le Deep Learning (réseaux de neurones LSTM) est aujourd’hui disponible.\n\nApplication : extraction d’informations de documents scannés photographiés\nDes documents scannés ou photographies peuvent souvent constituer une source d’information précieuse pour la production de statistiques publiques.\nPar exemple, la Direction des Statistiques d’Entreprises (DSE) à l’Insee effectue de manière périodique un profilage des groupes de sociétés. Pour la statistique publique la notion d’entreprise est souvent associée à une définition purement juridique, c’est-à-dire à la notion d’unité légale inscrite au répertoire Sirene. Toutefois, aujourd’hui certaines unités légales sont détenues par d’autres et peuvent ainsi perdre une partie de leur autonomie. Le profilage consiste à identifier au sein des groupes les entreprises au sens économique, puis à collecter et calculer des statistiques sur ces nouveaux contours.\nLa plupart des catégories de sociétés ont l’obligation de déposer annuellement leurs comptes sociaux au Registre du commerce et des sociétés (RCS), afin d’en garantir la transparence. Les documents à déposer incluent les comptes annuels (bilan actif et passif, compte de résultats et annexes), le rapport de gestion pour les sociétés cotées, les documents portant sur l’affectation du résultat, etc. Dans le cas où une société possède des filiales ou participations au moins à hauteur de 10% du capital, elle doit inclure dans ses comptes sociaux un tableau des filiales et participations (voir Figure 10) offrant une vision financière synthétique des différentes filiales et participations détenues. Ce tableau est très utile pour consolider le profilage d’un groupe, car il centralise des informations qui sont difficiles à obtenir par ailleurs.\n\n\n\n\n\n\nFigure 10: Exemple d’un tableau des filiales et participations figurant dans les comptes sociaux d’une société.\n\n\n\nAujourd’hui, les profileurs de la DSE utilisent les comptes sociaux de manière manuelle. Ils récupèrent les comptes sociaux, souvent sous la forme de documents scannés, depuis une interface de programmation mise à disposition par l’Institut National de la Propriété Industrielle (INPI) et pour chaque groupe qui les intéresse, cherchent eux-mêmes l’emplacement du tableau des filiales et participations dans le document puis récupèrent les informations pertinentes pour la consolidation. La reconnaissance optique de caractères peut permettre de traiter automatiquement (au moins en partie) les comptes sociaux, ce qui permettrait à la fois de dégager du temps aux profileurs pour des activités à plus forte valeur ajoutée, mais aussi de consolider plus de comptes.\nUne chaîne de traitement complète envisagée pour l’extraction d’un tableaux filiales et participations est décrite ci-dessous :\n\nOn récupère l’exemplaire des comptes sociaux d’intérêt via un appel à l’API de l’INPI ;\nUn document est en général constitué de plusieurs pages. Pour identifier la page sur laquelle se trouve le tableau des filiales et participations, tout le texte de chaque page du document est extrait à l’aide d’un moteur de reconnaissance de caractères. Puis un modèle de forêt aléatoire qui a été entraîné sur des observations annotées à la main prend en entrée la totalité des mots présents sur chaque page, pour renvoyer en sortie une probabilité que le tableau des filiales et participations y soit présent. Pour un document donné, on retient la page avec la probabilité de sortie la plus élevée si cette dernière dépasse un certain seuil fixé empiriquement.\nL’extraction à proprement parler du tableau se fait ensuite en plusieurs étapes :\n\nD’abord l’image est pré-traitée : elle est remise droite dans le cas où le document a été scanné de travers, les couleurs sont inversées si on repère une zone de l’image où du texte blanc figure sur une zone sombre, etc. ;\nOn applique ensuite le modèle de segmentation TableNet (Paliwal et al. 2020) à l’image, qui retourne deux masques : le premier masque indique l’emplacement des tableaux au sein de l’image, et le deuxième indique l’emplacement des colonnes au sein de l’image (voir Figure 11). Ce modèle a été entraîné à partir du jeu de données annotées Marmot disponible en libre accès sur Internet et optionnellement à partir de données supplémentaires des comptes sociaux annotées à la main ;\nLes masques sont post-traités dans l’étape suivante où des artefacts sont retirés, la table et les colonnes sont remplis lorsque des trous apparaissent sur les masques, etc. ;\nLe contenu de chaque colonne est extrait (chaque caractère accompagné de sa position sur l’image) grâce à un moteur de reconnaissance optique de caractères (par exemple Tesseract) ;\nLes colonnes sont alignées pour reconstituer la table aussi bien que possible ;\nOn identifie les colonnes de la table utile pour la consolidation des comptes grâce à l’utilisation d’expressions régulières et d’une distance textuelle ;\nLe tableau avec les noms de colonnes nettoyés est enfin exporté (par exemple en format csv).\n\n\n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\n\n \n\n\n\n\nFigure 11: Exemple de masques bruts obtenus en sortie de TableNet. À gauche, le masque indiquant l’emplacement de la table. À droite, le masque indiquant l’emplacement des colonnes.\n\n\n\n\n\nExtraction d’information de tickets de caisse\nL’enquête Budget des Familles, réalisée par la Direction des statistiques démographiques et sociales (DSDS) de l’Insee, repose traditionnellement sur la collecte de tickets de caisse dont les champs sont manuellement repris et numérisés par les enquêteurs1. Toutefois, il existe aujourd’hui des méthodes pour automatiser cette extraction en utilisant des moteurs de reconnaissance optique de caractères.\nUne première idée envisageable est d’utiliser un moteur d’OCR pour récupérer ligne par ligne le texte figurant sur un ticket de caisse puis d’extraire l’information sous forme structurée avec une approche basée sur des règles. Les tickets de caisse se ressemblant en général beaucoup, cette approche fonctionne convenablement sur cette tâche quelque soit le ticket, mais elle présente tout de même des défauts de généralisabilité. Une approche Deep Learning end-to-end est préférable, même si elle nécessite des données annotées. De telles méthodes ont été testées dans le cadre de compétitions (notamment sur les jeux de données SROIE 2019 et Cord) et ont donné de bons résultats.\n\n\nReferences\n\n\nChen, Liang-Chieh, George Papandreou, Florian Schroff, and Hartwig Adam. 2017. “Rethinking Atrous Convolution for Semantic Image Segmentation.” CoRR abs/1706.05587. http://arxiv.org/abs/1706.05587.\n\n\nDirection de la recherche et de l’innovation, Commissariat général au développement durable –. 2018. Plan d’applications Satellitaires 2018 - Des Solutions Spatiales Pour Connaître Le Territoire.\n\n\nLeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. “Backpropagation Applied to Handwritten Zip Code Recognition.” Neural Computation 1 (4): 541–51. https://doi.org/10.1162/neco.1989.1.4.541.\n\n\nPaliwal, Shubham, Vishwanath D, Rohit Rahul, Monika Sharma, and Lovekesh Vig. 2020. “TableNet: Deep Learning Model for End-to-End Table Detection and Tabular Data Extraction from Scanned Document Images.” CoRR abs/2001.01469. http://arxiv.org/abs/2001.01469.\n\n\nRonneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. “U-Net: Convolutional Networks for Biomedical Image Segmentation.” CoRR abs/1505.04597. http://arxiv.org/abs/1505.04597.\n\n\nSobel, Irwin, and Gary Feldman. 1973. “A 3×3 Isotropic Gradient Operator for Image Processing.” Pattern Classification and Scene Analysis, January, 271–72.\n\n\nSteele, Jessica E., Pål Roe Sundsøy, Carla Pezzulo, Victor A. Alegana, Tomas J. Bird, Joshua Blumenstock, Johannes Bjelland, et al. 2017. “Mapping Poverty Using Mobile Phone and Satellite Data.” Journal of The Royal Society Interface 14 (127): 20160690. https://doi.org/10.1098/rsif.2016.0690.\n\n\nVoulodimos, Athanasios, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis. 2018. “Deep Learning for Computer Vision: A Brief Review.” Computational Intelligence and Neuroscience 2018: 1–13. https://doi.org/10.1155/2018/7068349.",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images.html#footnotes",
    "href": "images.html#footnotes",
    "title": "Images",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nPlus précisément, les ménages enquêtés se voient confier un carnet de dépenses qu’ils doivent remplir pendant une certaine période. Pour certaines dépenses les carnets sont renseignés à la main par un membre du ménage. Pour d’autres, le ménage a la possibilité d’inclure dans le carnet des tickets de caisse. Jusqu’à présent les enquêteurs étaient chargés de recopier le contenu des tickets de caisse pour rendre ces données exploitables.↩︎",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Images"
    ]
  },
  {
    "objectID": "images_exemples.html",
    "href": "images_exemples.html",
    "title": "Application",
    "section": "",
    "text": "L’objectif de ce TP est d’explorer l’exploitation d’images à l’aide de modèles de Deep Learning, avec une application sur les images satellitaires pour la statistique publique.\nLe TP peut être lancé sur le SSP Cloud en cliquant sur le bouton suivant:\n\nLe notebook classification_oiseau.ipynb traite un problème simple de classification d’image, à regarder dans un premier temps. Le notebook donnees_satellite.ipynb traite ensuite un problème de segmentation sémantique sur des images satellitaires, qui permet d’obtenir des cartes de couverture du territoire pouvant être utilisées pour produire des statistiques officielles.\nUn second bouton si l’installation de Pytorch est trop longue:",
    "crumbs": [
      "Home",
      "Analyse d'images",
      "Application"
    ]
  },
  {
    "objectID": "applications/data.html",
    "href": "applications/data.html",
    "title": "Exemple basique utilisation geoparquet",
    "section": "",
    "text": "import subprocess\n\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet dvf.parquet\", shell=True)\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet carreaux.parquet\", shell=True)\n\n\nfrom cartiflette import carti_download\n\n# 1. Fonds communaux\ncontours_villes_arrt = carti_download(\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    crs = 4326,\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n# 2. Départements \ndepartements = contours_villes_arrt.dissolve(\"INSEE_DEP\").reset_index()\n#buffer_75 = departements.loc[departements[\"INSEE_DEP\"] == \"75\"].to_crs(2154).buffer(distance = 2000).plot()"
  },
  {
    "objectID": "applications/data.html#la-base-dvf",
    "href": "applications/data.html#la-base-dvf",
    "title": "Exemple basique utilisation geoparquet",
    "section": "",
    "text": "import subprocess\n\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/dvf.parquet dvf.parquet\", shell=True)\nsubprocess.call(\"mc cp s3/projet-formation/nouvelles-sources/data/geoparquet/carreaux.parquet carreaux.parquet\", shell=True)\n\n\nfrom cartiflette import carti_download\n\n# 1. Fonds communaux\ncontours_villes_arrt = carti_download(\n    values = [\"75\", \"92\", \"93\", \"94\"],\n    crs = 4326,\n    borders=\"COMMUNE_ARRONDISSEMENT\",\n    filter_by=\"DEPARTEMENT\",\n    source=\"EXPRESS-COG-CARTO-TERRITOIRE\",\n    year=2022)\n\n# 2. Départements \ndepartements = contours_villes_arrt.dissolve(\"INSEE_DEP\").reset_index()\n#buffer_75 = departements.loc[departements[\"INSEE_DEP\"] == \"75\"].to_crs(2154).buffer(distance = 2000).plot()"
  },
  {
    "objectID": "applications/data.html#exploitation",
    "href": "applications/data.html#exploitation",
    "title": "Exemple basique utilisation geoparquet",
    "section": "Exploitation",
    "text": "Exploitation\n\nimport duckdb\nduckdb.execute(\"INSTALL spatial;\")\nduckdb.execute(\"LOAD spatial;\")\n\n\nreference_lon = 2.35  # Replace with your reference longitude\nreference_lat = 48.853 # Replace with your reference latitude\n\n\ncarreaux_duckdb = duckdb.sql(f\"\"\"\nFROM read_parquet('carreaux_geoparquet.parquet')\nSELECT\n    *, ST_AsText(geometry) AS geom_text\nWHERE st_distance(\n        st_centroid(geometry),\n        ST_Transform(st_point({reference_lat}, {reference_lon}), 'EPSG:4326', 'EPSG:2154')\n    ) / 1000 &lt; 2\n\"\"\")\n\n\ncarreaux = carreaux_duckdb.to_df()\ncarreaux = gpd.GeoDataFrame(carreaux)\ncarreaux = carreaux.drop(\"geometry\", axis = \"columns\")\ncarreaux['geometry'] = gpd.GeoSeries.from_wkt(carreaux['geom_text'])\ncarreaux = carreaux.set_geometry('geometry', crs=2154).drop(\"geom_text\", axis = \"columns\")\n\n\ncarreaux['prop_men_pauv'] = 100*carreaux['men_pauv']/carreaux['men']\n\n\nimport folium\nimport branca.colormap as cm\n\ncarreaux = carreaux.to_crs(4326)\ncentroid = carreaux.geometry.unary_union.centroid\nmap_center = [centroid.y, centroid.x]\n\n# Create a linear colormap\ncolormap = cm.LinearColormap(\n    colors=['green', 'yellow', 'red'],  # Color range\n    vmin=carreaux['prop_men_pauv'].min(),  # Minimum value of the variable\n    vmax=carreaux['prop_men_pauv'].max(),  # Maximum value of the variable\n    caption='Proportion ménages pauvres'  # Legend title\n)\n\ndef style_function(feature):\n    proportion = feature['properties']['prop_men_pauv']\n    return {\n        'fillColor': colormap(proportion),\n        'color': 'black',         # Border color\n        'weight': 1,              # Border thickness\n        'fillOpacity': 0.7,       # Fill transparency\n    }\n\nm = folium.Map(location=map_center, zoom_start=14, tiles='cartodbpositron')\n\n# Add GeoJson layer to the map with tooltips\nfolium.GeoJson(\n    carreaux,\n    name='carreaux Borders',\n    style_function=style_function,\n    tooltip=folium.GeoJsonTooltip(\n        fields=['idcar_200m', 'idcar_1km', 'idcar_nat', 'prop_men_pauv'],\n        aliases=['ID Car 200m:', 'ID Car 1km:', 'ID Car Nat:', 'Proportion ménages pauvres:'],\n        localize=True\n    )\n).add_to(m)\n\n# Add the colormap legend to the map\ncolormap.add_to(m)\n\n# Optional: Add layer control if you have multiple layers\nfolium.LayerControl().add_to(m)\n\n# Display the map\nm\n\n\nduckdb.sql(\"SELECT * FROM read_parquet('dvf_geoparquet.parquet') LIMIT 2\")"
  }
]