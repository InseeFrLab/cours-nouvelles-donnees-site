<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Images – Données émergentes</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<link href="./images_exemples.html" rel="next">
<link href="./textes_exemples.html" rel="prev">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-33f6b32835c6667f0f82d635fe7e5bb7.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Images – Données émergentes">
<meta property="og:description" content="">
<meta property="og:image" content="https://InseeFrLab.github.io/cours-nouvelles-donnees-site/img/images/python_pixels.png">
<meta property="og:site_name" content="Données émergentes">
<meta property="og:image:height" content="480">
<meta property="og:image:width" content="640">
<meta name="twitter:title" content="Images – Données émergentes">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://InseeFrLab.github.io/cours-nouvelles-donnees-site/img/images/python_pixels.png">
<meta name="twitter:creator" content="@linogaliana">
<meta name="twitter:image-height" content="480">
<meta name="twitter:image-width" content="640">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-sidebar docked nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="./index.html">
    <span class="navbar-title">Données émergentes</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="./index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link active" href="./introduction.html" aria-current="page"> 
<span class="menu-text">Introduction</span></a>
  </li>  
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-données-administratives" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Données administratives</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-données-administratives">    
        <li>
    <a class="dropdown-item" href="./administratives.html">
 <span class="dropdown-text">Données administratives</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./administratives_exemples.html">
 <span class="dropdown-text">Quelques exemples approfondis de données administratives</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-données-géolocalisées" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Données géolocalisées</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-données-géolocalisées">    
        <li>
    <a class="dropdown-item" href="./geolocalized_data.html">
 <span class="dropdown-text">Données géolocalisées</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./geolocalized_data_exemples.html">
 <span class="dropdown-text">Application</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./applications/application1.html">
 <span class="dropdown-text">Application 1 - Données spatiales</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-analyse-textuelle" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Analyse textuelle</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-analyse-textuelle">    
        <li>
    <a class="dropdown-item" href="./textes.html">
 <span class="dropdown-text">Données textuelles et non structurées</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./applications/ape.html">
 <span class="dropdown-text">Application 1</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./textes_exemples.html">
 <span class="dropdown-text">Application 2</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-analyse-dimages" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Analyse d’images</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-analyse-dimages">    
        <li>
    <a class="dropdown-item" href="./images.html">
 <span class="dropdown-text">Images</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./images_exemples.html">
 <span class="dropdown-text">Application</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-nowcasting" role="link" data-bs-toggle="dropdown" aria-expanded="false">
 <span class="menu-text">Nowcasting</span>
    </a>
    <ul class="dropdown-menu" aria-labelledby="nav-menu-nowcasting">    
        <li>
    <a class="dropdown-item" href="./series_temporelles.html">
 <span class="dropdown-text">Nowcasting</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./nowcasting_exemples.html">
 <span class="dropdown-text">Application</span></a>
  </li>  
    </ul>
  </li>
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./images.html">Analyse d’images</a></li><li class="breadcrumb-item"><a href="./images.html">Images</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
        <div class="sidebar-tools-collapse">
    <a href="https://twitter.com" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-twitter"></i></a>
    <div class="dropdown">
      <a href="" title="" id="quarto-navigation-tool-dropdown-0" class="quarto-navigation-tool dropdown-toggle px-1" data-bs-toggle="dropdown" aria-expanded="false" role="link" aria-label=""><i class="bi bi-github"></i></a>
      <ul class="dropdown-menu" aria-labelledby="quarto-navigation-tool-dropdown-0">
          <li>
            <a class="dropdown-item sidebar-tools-collapse-item" href="https://github.com/InseeFrLab/cours-nouvelles-donnees-site">
            Source Code
            </a>
          </li>
          <li>
            <a class="dropdown-item sidebar-tools-collapse-item" href="https://github.com/InseeFrLab/cours-nouvelles-donnees-site/issues">
            Report a Bug
            </a>
          </li>
      </ul>
    </div>
</div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">Introduction</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./introduction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Introduction</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Données administratives</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./administratives.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Données administratives</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./administratives_exemples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Quelques exemples approfondis de données administratives</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true">
 <span class="menu-text">Données géolocalisées</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-3" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-3" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./geolocalized_data.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Données géolocalisées</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./geolocalized_data_exemples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./applications/application1.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application 1 - Données spatiales</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true">
 <span class="menu-text">Analyse textuelle</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-4" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-4" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./textes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Données textuelles et non structurées</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./applications/ape.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application 1</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./textes_exemples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application 2</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true">
 <span class="menu-text">Analyse d’images</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-5" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-5" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./images.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Images</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./images_exemples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true">
 <span class="menu-text">Nowcasting</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-6" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-6" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./series_temporelles.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Nowcasting</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./nowcasting_exemples.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Application</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#la-révolution-du-deep-learning" id="toc-la-révolution-du-deep-learning" class="nav-link" data-scroll-target="#la-révolution-du-deep-learning">La révolution du Deep Learning</a>
  <ul class="collapse">
  <li><a href="#réseaux-de-neurone-convolutifs" id="toc-réseaux-de-neurone-convolutifs" class="nav-link" data-scroll-target="#réseaux-de-neurone-convolutifs">Réseaux de neurone convolutifs</a></li>
  <li><a href="#segmentation-sémantique" id="toc-segmentation-sémantique" class="nav-link" data-scroll-target="#segmentation-sémantique">Segmentation sémantique</a></li>
  </ul></li>
  <li><a href="#application-à-la-statistique-publique" id="toc-application-à-la-statistique-publique" class="nav-link" data-scroll-target="#application-à-la-statistique-publique">Application à la statistique publique</a>
  <ul class="collapse">
  <li><a href="#utilisation-de-données-dobservation-satellitaire" id="toc-utilisation-de-données-dobservation-satellitaire" class="nav-link" data-scroll-target="#utilisation-de-données-dobservation-satellitaire">Utilisation de données d’observation satellitaire</a>
  <ul class="collapse">
  <li><a href="#nature-de-la-donnée" id="toc-nature-de-la-donnée" class="nav-link" data-scroll-target="#nature-de-la-donnée">Nature de la donnée</a></li>
  <li><a href="#fournisseurs-de-données" id="toc-fournisseurs-de-données" class="nav-link" data-scroll-target="#fournisseurs-de-données">Fournisseurs de données</a></li>
  <li><a href="#pipeline" id="toc-pipeline" class="nav-link" data-scroll-target="#pipeline">Pipeline</a></li>
  <li><a href="#cas-dusage" id="toc-cas-dusage" class="nav-link" data-scroll-target="#cas-dusage">Cas d’usage</a></li>
  </ul></li>
  <li><a href="#la-reconnaissance-optique-de-caractères" id="toc-la-reconnaissance-optique-de-caractères" class="nav-link" data-scroll-target="#la-reconnaissance-optique-de-caractères">La reconnaissance optique de caractères</a>
  <ul class="collapse">
  <li><a href="#application-extraction-dinformations-de-documents-scannés-photographiés" id="toc-application-extraction-dinformations-de-documents-scannés-photographiés" class="nav-link" data-scroll-target="#application-extraction-dinformations-de-documents-scannés-photographiés">Application : extraction d’informations de documents scannés photographiés</a></li>
  <li><a href="#extraction-dinformation-de-tickets-de-caisse" id="toc-extraction-dinformation-de-tickets-de-caisse" class="nav-link" data-scroll-target="#extraction-dinformation-de-tickets-de-caisse">Extraction d’information de tickets de caisse</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/InseeFrLab/cours-nouvelles-donnees-site/edit/main/images.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/InseeFrLab/cours-nouvelles-donnees-site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./images.html">Analyse d’images</a></li><li class="breadcrumb-item"><a href="./images.html">Images</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Images</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Les images sont des données qui sont utilisées depuis longtemps de manière automatique. Une image pour un ordinateur est représentée par un tableau en 2 ou 3 dimensions (images en nuances de gris et images en couleur respectivement). En 2 dimensions, l’image a ainsi une longueur <span class="math inline">\(L\)</span> et une largeur <span class="math inline">\(W\)</span> : elle est constituée de <span class="math inline">\(L \times W\)</span> pixels, chacun associé à une valeur entière comprise entre 0 et 255 (ou parfois à une valeur décimale comprise entre 0 et 1), comme illustré en <a href="#fig-image-nb" class="quarto-xref">Figure&nbsp;1</a>.</p>
<div id="fig-image-nb" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-image-nb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/images/python_pixels.png" class="img-fluid figure-img" style="width:50.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-image-nb-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Représentation du logo de <code>Python</code> en nuances de gris avec une faible résolution. La valeur de chaque pixel (entier allant de 0 pour un pixel complètement noir à 255 pour un pixel complètement blanc) figure à l’emplacement de ce dernier.
</figcaption>
</figure>
</div>
<p>Une image en couleur est constituée de 3 canaux (RGB pour <em>Red</em>, <em>Green</em> et <em>Blue</em>). Chacun des <span class="math inline">\(L \times W\)</span> pixels de l’image est ainsi associé à 3 valeurs entières comprises entre 0 et 225 (ou à 3 valeurs décimales comprises entre 0 et 1), comme illustré en <a href="#fig-image-couleur" class="quarto-xref">Figure&nbsp;2</a>.</p>
<div id="fig-image-couleur" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-image-couleur-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 39.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 21.2%;justify-content: center;">
<p><img src="img/images/rgb_image.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 39.4%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 12.1%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 24.2%;justify-content: center;">
<p><img src="img/images/python_red.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 1.5%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 24.2%;justify-content: center;">
<p><img src="img/images/python_green.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 1.5%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 24.2%;justify-content: center;">
<p><img src="img/images/python_blue.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 12.1%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-image-couleur-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Représentation du logo de <code>Python</code> en couleurs. L’image du haut correspond à la superposition des trois canaux représentés sur la rangée inférieure.
</figcaption>
</figure>
</div>
<p>Le domaine de la vision par ordinateur (<em>computer vision</em>) a vu le jour dans les années 1960 avec le développement des premiers algorithmes cherchant à extraire de l’information d’images. Par exemple, <span class="citation" data-cites="sobel-73">Sobel and Feldman (<a href="#ref-sobel-73" role="doc-biblioref">1973</a>)</span> introduit la méthode suivante pour faire de la détection de contours sur une image <span class="math inline">\(A\)</span>.</p>
<p>On calcule</p>
<p><span class="math display">\[
G_x = \begin{bmatrix}
+1 &amp; 0 &amp; -1\\
+2 &amp; 0 &amp; -2\\
+1 &amp; 0 &amp; -1
\end{bmatrix} \star A \quad \text{et} \quad G_y = \begin{bmatrix}
+1 &amp; +2 &amp; +1\\
0 &amp; 0 &amp; 0\\
-1 &amp; -2 &amp; -1
\end{bmatrix} \star A
\]</span></p>
<p>où <span class="math inline">\(\star\)</span> est l’opérateur de convolution 2-dimensionnel en traitement du signal (illustré en <a href="#fig-convol" class="quarto-xref">Figure&nbsp;4</a>).</p>
<p>Alors l’image <span class="math inline">\(G = \sqrt{G_x^2 + G_y^2}\)</span> fournit une représentation des contours de l’image <span class="math inline">\(A\)</span>. Une illustration de l’application de cette méthode est donnée en <a href="#fig-sobel" class="quarto-xref">Figure&nbsp;3</a>.</p>
<div id="fig-sobel" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-sobel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 11.1%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: center;">
<p><img src="img/images/bike.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 3.7%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: center;">
<p><img src="img/images/bike_sobel.jpg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 11.1%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-sobel-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: L’image de droite est obtenue par application sur l’image de gauche de la méthode de détection de contours introduite par <span class="citation" data-cites="sobel-73">Sobel and Feldman (<a href="#ref-sobel-73" role="doc-biblioref">1973</a>)</span>. Source : <a href="https://en.wikipedia.org/wiki/Sobel_operator">Wikipedia</a>.
</figcaption>
</figure>
</div>
<div id="fig-convol" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-convol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/images/convol.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-convol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Illustration de l’opérateur de convolution 2-dimensionnel <span class="math inline">\(\star\)</span>. Le noyau (matrice en bleu sur le dessin) est multiplié par -1 et <em>glisse</em> sur la matrice de gauche. Une multiplication élément par élément est faite sur chaque sous-matrice de la taille du noyau. Pour chacune de ces multiplication, les coefficients sont ensuite sommés pour donner une valeur de sortie unique. Par exemple ici, la valeur du pixel en vert correspond au calcul <span class="math inline">\(3 = 1*(-1) + 1*1 + 1*2 + 1*1\)</span>.
</figcaption>
</figure>
</div>
</section>
<section id="la-révolution-du-deep-learning" class="level1">
<h1>La révolution du Deep Learning</h1>
<p>Dans les dernières années, l’apprentissage profond a permis une véritable révolution dans le domaine de la vision par ordinateur <span class="citation" data-cites="voulodimos-18">(<a href="#ref-voulodimos-18" role="doc-biblioref">Voulodimos et al. 2018</a>)</span>. Les réseaux de neurone ont permis l’introduction de modèles complexes qui parviennent à apprendre et à représenter des données sur plusieurs niveaux d’abstraction, à l’image de la manière dont le cerveau perçoit et comprend les informations multi-modales.</p>
<p>Ainsi, les performance <em>state-of-the-art</em> ont été largement améliorées pour une multitude de tâches différentes : classification d’image, segmentation sémantique, reconnaissance faciale et détection d’objets… Par exemple, dans le domaine de la robotique ou de la voiture autonome, ces modèles ont changé la donne en permettant que certaines opérations d’analyse et de décisions soient néanmoins applicables dans une grande diversité de scénarios.</p>
<section id="réseaux-de-neurone-convolutifs" class="level2">
<h2 class="anchored" data-anchor-id="réseaux-de-neurone-convolutifs">Réseaux de neurone convolutifs</h2>
<p>Une architecture de modèles a joué un rôle particulièrement important dans cette révolution : les <strong>réseaux de neurones convolutifs</strong> <span class="citation" data-cites="lecun-89">(voir <a href="#ref-lecun-89" role="doc-biblioref">LeCun et al. 1989</a> pour un des articles fondateurs)</span>. Ces réseaux de neurones sont constitués d’un enchaînement de couches convolutives, chacune composée de trois étapes :</p>
<ul>
<li><strong>Une étape de <em>convolution</em></strong> utilisant l’opérateur <span class="math inline">\(\star\)</span> décrit ci-dessus qui transforme un <a href="https://fr.wikipedia.org/wiki/Tenseur_(math%C3%A9matiques)">tenseur</a> 3-dimensionnel de taille <span class="math inline">\((H, W, C)\)</span> en entrée en un tenseur de taille <span class="math inline">\((H', W', C')\)</span> ou <span class="math inline">\(H'\)</span>, <span class="math inline">\(W'\)</span> et <span class="math inline">\(C'\)</span> dépendent de la taille du noyau de convolution choisi ;</li>
<li>Une <strong>étape de <em>détection</em></strong> où une fonction non-linéaire est appliquée au tenseur obtenu en sortie de l’étape de convolution ;</li>
<li>Une <strong>étape de <em>pooling</em></strong> où chaque canal du tenseur en entrée voit sa hauteur et largeur réduite à l’aide une fonction qui remplace chaque valeur par une statistique impliquant les valeurs des pixels voisins (fréquemment, la valeur maximale dans un voisinage rectangulaire : c’est l’opération de <em>max pooling</em>).</li>
</ul>
<p>La succession de ces opérations est résumée dans la <a href="#fig-nn-convol" class="quarto-xref">Figure&nbsp;5</a></p>
<div id="fig-nn-convol" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-nn-convol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://editor.analyticsvidhya.com/uploads/59954intro%20to%20CNN.JPG" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-nn-convol-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Illustration d’une succession de séquences d’un réseau convolutionnel. Emprunté à https://www.analyticsvidhya.com/blog/2022/01/convolutional-neural-network-an-overview/
</figcaption>
</figure>
</div>
<p>Les tenseurs obtenus en sortie des couches convolutives sont appelés <em>activation maps</em> ou <em>feature maps</em>. Chaque <em>feature map</em> peut s’interpréter comme une carte qui indique les endroits où on peut trouver une <em>feature</em> particulière (par exemple un bord, une texture, une partie d’un objet, etc.) au sein de l’image. Les <em>features</em> pertinentes (c’est-à-dire les coefficients des filtres de convolution utilisés) sont apprises par le réseau de neurones au cours de la phase d’entraînement. On peut voir ces <em>features</em> comme des structures latentes qui combinées ensemble génèrent un objet sur l’image finale.</p>
<p>Les réseaux de neurones convolutifs présentent plusieurs caractéristiques essentielles pour des tâches de vision par ordinateur, qui expliquent en partie leur succès : une invariance (relative) à la translation, la rotation et à l’échelle. Ces caractéristiques permettent aux modèles d’abstraire l’identité d’un objet de détails spécifiques aux images données en entrée tels que la position et l’orientation de cet objet par rapport à la caméra.</p>
</section>
<section id="segmentation-sémantique" class="level2">
<h2 class="anchored" data-anchor-id="segmentation-sémantique">Segmentation sémantique</h2>
<p>La segmentation sémantique est une tâche de vision par ordinateur qui consiste à associer une étiquette ou une catégorie à chaque pixel d’une image (illustration en <a href="#fig-segmentation" class="quarto-xref">Figure&nbsp;7</a>). Plusieurs architectures de réseaux de neurones convolutifs entraînées sur des gros jeux d’entraînement obtiennent des performances très élevées sur des jeux de données d’évaluation de référence, comme l’architecture <code>DeepLabV3</code> <span class="citation" data-cites="chen-17">(<a href="#ref-chen-17" role="doc-biblioref">Chen et al. 2017</a>)</span>. Les principaux frameworks de Deep Learning fournissent des implémentations de modèles de segmentation sémantique (avec ou sans coefficients pré-entraînés) : c’est le cas du package <code>Python</code> <a href="https://pytorch.org/vision/"><code>torchvision</code></a> par exemple qui propose une implémentation des modèles <code>DeepLabV3</code>, <code>FCN</code> et <code>LRASPP</code>.</p>
<div id="fig-segmentation" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="img/images/image_segmentation_input.jpeg" class="img-fluid figure-img"></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><img src="img/images/image_segmentation_output.png" class="img-fluid figure-img"></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Segmentation sémantique effectuée sur une photo de chat (partie gauche de la Figure). Sur le masque de segmentation (partie droite de la Figure), les pixels verts sont associés à la classe <em>chat</em> tandis que les pixels roses sont associés à la classe <em>arrière-plan</em>. Source : <a href="https://huggingface.co/tasks/image-segmentation">Hugging Face</a>.
</figcaption>
</figure>
</div>
<div id="fig-segmentation" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://nanonets.com/blog/content/images/2020/08/1_Hz6t-tokG1niaUfmcysusw.jpeg" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-segmentation-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Un autre exemple de segmentation sémantique, issu de ce <a href="https://nanonets.com/blog/semantic-image-segmentation-2020/">blog</a>
</figcaption>
</figure>
</div>
</section>
</section>
<section id="application-à-la-statistique-publique" class="level1">
<h1>Application à la statistique publique</h1>
<p>La statistique publique, et plus largement l’administration, peut désirer tirer parti des méthodes de vision par ordinateur de plusieurs manières. La suite de ce chapitre va développer quelques cas d’usages, non exhaustifs, des données satellites pour la statistique publique. Les cas d’usage sont très nombreux et ne seront pas tous évoqués. Par exemple, pour en savoir plus sur la production des données LIDAR de l’IGN, il est recommandé de lire <a href="https://geoservices.ign.fr/lidarhd">cette page</a>.</p>
<section id="utilisation-de-données-dobservation-satellitaire" class="level2">
<h2 class="anchored" data-anchor-id="utilisation-de-données-dobservation-satellitaire">Utilisation de données d’observation satellitaire</h2>
<section id="nature-de-la-donnée" class="level3">
<h3 class="anchored" data-anchor-id="nature-de-la-donnée">Nature de la donnée</h3>
<p>Dans le domaine des données d’<em>Earth Observation</em>, qui regroupent en fait différentes sources de données (radars, <a href="http://geoconfluences.ens-lyon.fr/glossaire/orthophotographie">orthophotographies</a>…), les données photographiques issues de satellites ont une place de choix. Celles-ci permettent d’observer les territoires, que ce soit leur topologie ou leur usage et potentiellement d’en tirer des enseignements à diffuser sous la forme de statistiques publiques. Par exemple, l’utilisation de données satellitaires peut permettre d’améliorer la granularité spatiale et temporelles de statistiques publiées aujourd’hui sur la production agricole (part du territoire cultivé, nature des cultures…).</p>
<p>De manière générale, ces données ont beaucoup de potentiel lorsqu’elles sont utilisées en combinaison avec d’autres sources de données lorsqu’il s’agit de pallier des insuffisances ou des manques concernant les données traditionnellement utilisées pour la statistique publique. Par exemple, <span class="citation" data-cites="steele-17">Steele et al. (<a href="#ref-steele-17" role="doc-biblioref">2017</a>)</span> combinent données de satellites et données de téléphonie mobile pour estimer des taux de pauvreté. En France, les départements et régions d’outre-mer sont particulièrement concernés. Les données satellites permettraient d’y combler des imperfections des données administratives. Par exemple, les parcelles cadastrales y sont parfois mal identifiées ou rarement mises à jour. Les données satellitaires peuvent être utilisées pour fiabiliser cette information.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Les données d’<em>Earth Observation</em> présentent des difficultés d’utilisation non-négligeables dans un contexte de production statistique :</p>
<ul>
<li>Il faut au moment de la production de la statistique désirée s’assurer que l’on parvient à des résultats statistiquement robustes ;</li>
<li>Produire des statistiques de manière récurrente à partir d’une source de données demande d’avoir du recul sur le fonctionnement de la chaîne de traitement en production. Comme les données d’<em>Earth Observation</em> ne sont aujourd’hui utilisées que par peu d’instituts statistiques, il est difficile d’avoir un tel recul sans soi-même avoir une chaîne de traitement qui tourne depuis plusieurs années ;</li>
<li>Pour de nombreuses applications, on souhaite utiliser des images avec une résolution élevée mais aussi exploiter la haute fréquence temporelle de passage de certains satellites. Dans un tel cadre les données d’<em>Earth Observation</em> ont souvent un volume très important. Entraîner des modèles pertinents (les modèles de <em>Deep Learning</em> <em>state-of-the-art</em> sont complexes) demande d’avoir des ressources informatiques adaptées à disposition ;</li>
<li>Selon les besoins, la résolution disponible peut ne pas correspondre aux besoins de la statistique.</li>
</ul>
</div>
</div>
</section>
<section id="fournisseurs-de-données" class="level3">
<h3 class="anchored" data-anchor-id="fournisseurs-de-données">Fournisseurs de données</h3>
<p>Des acteurs publient des données satellitaires en <em>open data</em> :</p>
<ul>
<li>La <code>NASA</code> à travers son programme historique <a href="https://landsat.gsfc.nasa.gov/data/"><code>Landsat</code></a>. Les dernières générations des satellites Landsat recueillent des images dans une dizaine de bandes spectrales (bandes visibles mais aussi bandes infrarouges) avec une résolution spatiale de 30 mètres (pour les bandes visibles) ;</li>
<li>L’<code>Agence spatiale européenne (ESA)</code> a lancé le programme <a href="https://sentinel.esa.int/web/sentinel/sentinel-data-access"><code>Sentinel-2</code></a> en 2015. Les images des satellites <code>Sentinel-2</code> sont aussi disponibles en <em>open data</em>, sur 12 bandes avec une résolution spatiale de 10 mètres, plus fine que celle des images de <code>Landsat</code>. La périodicité de la couverture des satellites <code>Sentinel-2</code> est relativement faible : ces derniers repassent au-dessus des mêmes zones tous les cinq jours.</li>
</ul>
<p>Des entreprises privées collectent aussi des images avec leurs propres satellites, parfois avec des meilleures résolutions que les images disponibles en libre accès, ce qui peut être nécessaire en fonction du cas d’usage envisagé. De manière générale, il y a toutefois un arbitrage à faire entre le détail local des mesures (résolution radiométrique, nombre de bandes spectrales) et la résolution spatiale des images. La richesse des images issues de satellites réside plutôt dans la première dimensions, alors que les orthophotographies par exemple sont à privilégier si on désire une plus haute résolution spatiale.</p>
</section>
<section id="pipeline" class="level3">
<h3 class="anchored" data-anchor-id="pipeline">Pipeline</h3>
<p>Le traitement d’images de satellites se divise de manière classique en trois parties <span class="citation" data-cites="plan-sat">(<a href="#ref-plan-sat" role="doc-biblioref">Direction de la recherche et de l’innovation 2018</a>)</span> :</p>
<ul>
<li>d’abord vient le <strong>pré-traitement</strong> des données, qui inclut le stockage, le <em>data managment</em>, le contrôle de la qualité des données, l’inclusion d’autres sources et l’identification d’outils appropriés pour l’analyse.</li>
<li>Ce pré-traitement est suivi par une <strong>phase d’analyse</strong>, où l’on définit les indicateurs à calculer, les données à utiliser et où l’on applique la méthode analytique choisie.</li>
<li>Enfin, au cours de la <strong>phase d’évaluation</strong>, on collecte et on interprète les résultats de l’analyse.</li>
</ul>
<p>Des méthodes historiques existent pour analyser des images de satellites (pour <em>in fine</em> produire des statistiques). Par exemple, l’utilisation de modèle physiques pour prédire la valeur d’une variable d’intérêt à partir de l’observation empirique de certaine bandes, ou encore de méthodes d’analyse d’images traditionnelles où des informations spatiales, relatives à des motifs, à des textures, etc. sert à segmenter l’image sous supervision humaine (OBIA). Récemment, le Machine Learning (et en particulier le Deep Learning) a fourni des outils d’analyse puissants facilement applicables aux images satellites.</p>
</section>
<section id="cas-dusage" class="level3">
<h3 class="anchored" data-anchor-id="cas-dusage">Cas d’usage</h3>
<p>Les cas d’usage potentiels d’utilisation de ces données pour la statistique publique touchent de nombreux thèmes, qui incluent :</p>
<ul>
<li>La supervision des forêts, de l’agriculture, des masses d’eau ;</li>
<li>L’urbanisation et les infrastructures ;</li>
<li>La pollution environnementale et la qualité de l’air atmosphérique ;</li>
</ul>
<p>En particulier, l’analyse d’images satellite peut permettre de calculer des indicateurs comme la proportion de surface agricole en agriculture intensive ou en agriculture durable, le pourcentage de masses d’eau présentant une bonne qualité de l’eau ambiante, la couverture forestière dans le cadre d’une gestion forestière durable, la perte nette permanente de forêts, etc.</p>
<p>Plusieurs cas d’usage précis ont été ciblés aujourd’hui pour la statistique publique en France et donnent ou vont donner lieu à des travaux expérimentaux.</p>
<ol type="1">
<li><p>Un des cas d’usage identifiés depuis un moment déjà est l’utilisation d’images satellites pour calculer les <strong>statistiques sur l’occupation et l’usage des sols sur le territoire français</strong>. Aujourd’hui, ces statistiques sont tirées de l’enquête <code>Teruti</code> conduite par le Bureau des statistiques structurelles environnementales et forestières du SSP (Ministère de l’Agriculture).</p>
<p>Un échantillon de points est observé sur le terrain sur un cycle de 3 ans permettant d’estimer l’occupation des sols avec une précision qui reste satisfaisante à l’échelon départemental. L’échantillonnage des points se fait à partir de sources multiples, dont des données satellitaires (satellite SPOT) et des orthophotographies de l’IGN. En outre, une phase de validation des résultats de l’enquête est réalisée à partir d’une couche d’exploitation du sol issue de données de Sentinel-2 et réalisée de manière automatique est le Centre d’Etudes Spatiales sur la BIOsphere à Toulouse.</p>
<p>Des travaux sont actuellement en cours pour encore davantage améliorer la phase d’échantillonnage à l’aide d’images satellitaires. En outre, une méthode automatique donnant des couches d’exploitation des sols avec une précision suffisante pour les besoins de la statistique publique pourrait permettre de diffuser des statistiques plus régulièrement qu’avec l’enquête Teruti et avec une granularité territoriale plus fine.</p></li>
<li><p>Les <strong>parcelles cadastrales</strong> sont parfois mal identifiées dans les départements et région d’outre-mer, en particulier en Guyane et à Mayotte. Or ces parcelles sont utilisées pour des tirages d’échantillon par l’Insee, pour le recensement de la population par exemple.</p>
<p>Ici encore, des modèles de segmentation retournant des couches d’exploitation et d’usage des sols peuvent être utilisés pour consolider l’information disponible sur les parcelles cadastrales. Dans le cadre d’une expérimentation, un modèle de segmentation <code>U-Net</code> <span class="citation" data-cites="ronneberger-15">(<a href="#ref-ronneberger-15" role="doc-biblioref">Ronneberger, Fischer, and Brox 2015</a>)</span> pré-entraîné sur le jeu de données <a href="https://www.image-net.org/"><code>ImageNet</code></a> a été fine-tuné sur un sous-échantillon du jeu annoté <code>S2GLC</code> (<code>Sentinel-2 Global Land Cover</code>). Ce modèle prend en entrée une image satellite et renvoie une prédiction pixel par pixel de la catégorie de terrain (en 10 classes), comme illustré en <a href="#fig-satellite" class="quarto-xref">Figure&nbsp;8</a>. S’il est assez précis sur la catégorie <em>surfaces artificielles et construction</em>, ses prédictions pourraient servir à consolider les données cadastrales.</p></li>
</ol>
<div id="fig-satellite" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-satellite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 2.9%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 29.4%;justify-content: center;">
<p><img src="img/images/image_satellite.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 2.9%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 29.4%;justify-content: center;">
<p><img src="img/images/predicted_segmentation.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 2.9%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 29.4%;justify-content: center;">
<p><img src="img/images/real_segmentation.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 2.9%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-satellite-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;8: À gauche : image satellite issue de Sentinel-2. Au milieu : segmentation prédite par le modèle U-Net. À droite : vraie segmentation de l’image.
</figcaption>
</figure>
</div>
<ol start="3" type="1">
<li>L’<strong>enquête sur la structure des exploitations agricoles</strong> (Bureau des statistiques structurelles environnementales et forestières du SSP) dont la prochaine édition aura lieu en 2023 pose des questions sur les vergers. Il n’existe aujourd’hui pas de source administrative permettant de consolider les résultats de l’enquête sur cette thématique.Ainsi, un projet d’expérimentation utilisant des orthophotographies pour dénombrer le nombre d’arbres et la surface associée est envisagé. La librairie <a href="https://github.com/weecology/DeepForest-pytorch"><code>DeepForest</code></a> propose des modèles pré-entraînés pour faire de la détection d’arbres (voir <a href="#fig-detection" class="quarto-xref">Figure&nbsp;9</a>) et pourra servir de point de départ pour cette expérimentation.</li>
</ol>
<div id="fig-detection" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-detection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/images/detection_arbre.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-detection-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;9: Détection d’arbres sur une orthophotographie à l’aide de la librairie <code>DeepForest</code>.
</figcaption>
</figure>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Plusieurs questions méthodologiques essentielles se posent lorsqu’on exploite des données satellitaires grâce à des méthodes de Deep Learning :</p>
<ul>
<li>Architectures des modèles</li>
<li>Utilisation des différentes bandes</li>
<li>Pré-traitements sur les images : détection et suppression de nuages, amélioration de la résolution ;</li>
<li>Transférabilité des modèles : est-ce qu’un modèle entraîné sur des images provenant d’un satellite fonctionnera correctement avec des images provenant d’un autre satellite ? Ou avec un réentraînement minimal ?</li>
</ul>
<p>Un enjeu majeur est l’obtention de <strong>données annotées</strong> (même si le pré-entraînement de modèles sur des jeux de données énormes réduit le besoin de données annotées pour la tâche considérée). Pour des tâches de prédiction de l’utilisation du sol, on peut par exemple mobiliser la base de données géographiques <a href="https://www.statistiques.developpement-durable.gouv.fr/corine-land-cover-0"><code>CORINE Land Cover</code></a>, un inventaire biophysique qui fournit une photographie complète de l’occupation des sols, à des fréquences régulières.</p>
<p>Elle est issue de l’interprétation visuelle d’images satellitaires, avec des données complémentaires d’appui. Les classes d’occupation correspondent à une nomenclature comportant 44 postes.</p>
</div>
</div>
</section>
</section>
<section id="la-reconnaissance-optique-de-caractères" class="level2">
<h2 class="anchored" data-anchor-id="la-reconnaissance-optique-de-caractères">La reconnaissance optique de caractères</h2>
<p>L’administration française a été historiquement une grande productrice de fichiers sous format papier. Même si la numérisation des sources de collectes administratives réduit le volume de production papier, ce dernier mode de collecte est encore d’usage. Afin de réduire le temps de numérisation, il est donc utile de mettre en oeuvre des routines automatisées. Dans la même veine, l’administration a longtemps mis en oeuvre des publications (tableaux ou graphiques) sous format papier. Être en mesure de valoriser ce patrimoine de connaissance est un enjeu pour la recherche.</p>
<p>La reconnaissance optique de caractères (souvent abrégée par <code>OCR</code> pour <em>Optical character recognition</em>) désigne la tâche de conversion de texte manuscrit ou imprimé en texte encodé par un ordinateur. C’est une tâche essentielle pour exploiter des documents disponibles sous la forme d’images numériques.</p>
<p>Développer son propre moteur d’OCR est une tâche très complexe mais heureusement des moteurs <em>open source</em> existent. <a href="https://github.com/tesseract-ocr/tesseract"><code>Tesseract</code></a> est un logiciel pour la reconnaissance de caractères <em>open source</em> depuis 2015. <code>Tesseract</code> offre plusieurs moteurs depuis sa version 4 : en plus du moteur historique, un moteur basé sur le Deep Learning (réseaux de neurones LSTM) est aujourd’hui disponible.</p>
<section id="application-extraction-dinformations-de-documents-scannés-photographiés" class="level3">
<h3 class="anchored" data-anchor-id="application-extraction-dinformations-de-documents-scannés-photographiés">Application : extraction d’informations de documents scannés photographiés</h3>
<p>Des documents scannés ou photographies peuvent souvent constituer une source d’information précieuse pour la production de statistiques publiques.</p>
<p>Par exemple, la Direction des Statistiques d’Entreprises (DSE) à l’Insee effectue de manière périodique un <em>profilage</em> des groupes de sociétés. Pour la statistique publique la notion d’entreprise est souvent associée à une définition purement juridique, c’est-à-dire à la notion d’unité légale inscrite au répertoire Sirene. Toutefois, aujourd’hui certaines unités légales sont détenues par d’autres et peuvent ainsi perdre une partie de leur autonomie. Le <em>profilage</em> consiste à identifier au sein des groupes les entreprises au sens économique, puis à collecter et calculer des statistiques sur ces nouveaux contours.</p>
<p>La plupart des catégories de sociétés ont l’obligation de déposer annuellement leurs comptes sociaux au Registre du commerce et des sociétés (RCS), afin d’en garantir la transparence. Les documents à déposer incluent les comptes annuels (bilan actif et passif, compte de résultats et annexes), le rapport de gestion pour les sociétés cotées, les documents portant sur l’affectation du résultat, etc. Dans le cas où une société possède des filiales ou participations au moins à hauteur de 10% du capital, elle doit inclure dans ses comptes sociaux un tableau <em>des filiales et participations</em> (voir <a href="#fig-filiales-ex" class="quarto-xref">Figure&nbsp;10</a>) offrant une vision financière synthétique des différentes filiales et participations détenues. Ce tableau est très utile pour consolider le profilage d’un groupe, car il centralise des informations qui sont difficiles à obtenir par ailleurs.</p>
<div id="fig-filiales-ex" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-filiales-ex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="img/images/filiales_ex.png" class="img-fluid figure-img" style="width:60.0%">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-filiales-ex-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;10: Exemple d’un tableau <em>des filiales et participations</em> figurant dans les comptes sociaux d’une société.
</figcaption>
</figure>
</div>
<p>Aujourd’hui, les profileurs de la DSE utilisent les comptes sociaux de manière manuelle. Ils récupèrent les comptes sociaux, souvent sous la forme de documents scannés, depuis une interface de programmation mise à disposition par l’Institut National de la Propriété Industrielle (INPI) et pour chaque groupe qui les intéresse, cherchent eux-mêmes l’emplacement du tableau <em>des filiales et participations</em> dans le document puis récupèrent les informations pertinentes pour la consolidation. La reconnaissance optique de caractères peut permettre de traiter automatiquement (au moins en partie) les comptes sociaux, ce qui permettrait à la fois de dégager du temps aux profileurs pour des activités à plus forte valeur ajoutée, mais aussi de consolider plus de comptes.</p>
<p>Une chaîne de traitement complète envisagée pour l’extraction d’un tableaux filiales et participations est décrite ci-dessous :</p>
<ul>
<li>On récupère l’exemplaire des comptes sociaux d’intérêt via un appel à l’API de l’INPI ;</li>
<li>Un document est en général constitué de plusieurs pages. Pour identifier la page sur laquelle se trouve le tableau des <em>filiales et participations</em>, tout le texte de chaque page du document est extrait à l’aide d’un moteur de reconnaissance de caractères. Puis un modèle de forêt aléatoire qui a été entraîné sur des observations annotées à la main prend en entrée la totalité des mots présents sur chaque page, pour renvoyer en sortie une probabilité que le tableau des <em>filiales et participations</em> y soit présent. Pour un document donné, on retient la page avec la probabilité de sortie la plus élevée si cette dernière dépasse un certain seuil fixé empiriquement.</li>
<li>L’extraction à proprement parler du tableau se fait ensuite en plusieurs étapes :
<ul>
<li>D’abord l’image est pré-traitée : elle est remise droite dans le cas où le document a été scanné de travers, les couleurs sont inversées si on repère une zone de l’image où du texte blanc figure sur une zone sombre, etc. ;</li>
<li>On applique ensuite le modèle de segmentation <code>TableNet</code> <span class="citation" data-cites="paliwal-20">(<a href="#ref-paliwal-20" role="doc-biblioref">Paliwal et al. 2020</a>)</span> à l’image, qui retourne deux masques : le premier masque indique l’emplacement des tableaux au sein de l’image, et le deuxième indique l’emplacement des colonnes au sein de l’image (voir <a href="#fig-tablenet" class="quarto-xref">Figure&nbsp;11</a>). Ce modèle a été entraîné à partir du jeu de données annotées <a href="https://www.icst.pku.edu.cn/cpdp/sjzy/">Marmot</a> disponible en libre accès sur Internet et optionnellement à partir de données supplémentaires des comptes sociaux annotées à la main ;</li>
<li>Les masques sont post-traités dans l’étape suivante où des artefacts sont retirés, la table et les colonnes sont remplis lorsque des <em>trous</em> apparaissent sur les masques, etc. ;</li>
<li>Le contenu de chaque colonne est extrait (chaque caractère accompagné de sa position sur l’image) grâce à un moteur de reconnaissance optique de caractères (par exemple Tesseract) ;</li>
<li>Les colonnes sont alignées pour reconstituer la table aussi bien que possible ;</li>
<li>On identifie les colonnes de la table utile pour la consolidation des comptes grâce à l’utilisation d’expressions régulières et d’une distance textuelle ;</li>
<li>Le tableau avec les noms de colonnes nettoyés est enfin exporté (par exemple en format csv).</li>
</ul></li>
</ul>
<div id="fig-tablenet" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tablenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 11.1%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: center;">
<p><img src="img/images/table_mask.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 3.7%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 37.0%;justify-content: center;">
<p><img src="img/images/column_mask.png" class="img-fluid figure-img"></p>
</div>
<div class="quarto-figure-spacer quarto-layout-cell" style="flex-basis: 11.1%;justify-content: flex-start;">
<p>&nbsp;</p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tablenet-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;11: Exemple de masques bruts obtenus en sortie de TableNet. À gauche, le masque indiquant l’emplacement de la table. À droite, le masque indiquant l’emplacement des colonnes.
</figcaption>
</figure>
</div>
</section>
<section id="extraction-dinformation-de-tickets-de-caisse" class="level3">
<h3 class="anchored" data-anchor-id="extraction-dinformation-de-tickets-de-caisse">Extraction d’information de tickets de caisse</h3>
<p>L’enquête Budget des Familles, réalisée par la Direction des statistiques démographiques et sociales (DSDS) de l’Insee, repose traditionnellement sur la collecte de tickets de caisse dont les champs sont manuellement repris et numérisés par les enquêteurs<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Toutefois, il existe aujourd’hui des méthodes pour automatiser cette extraction en utilisant des moteurs de reconnaissance optique de caractères.</p>
<p>Une première idée envisageable est d’utiliser un moteur d’OCR pour récupérer ligne par ligne le texte figurant sur un ticket de caisse puis d’extraire l’information sous forme structurée avec une approche basée sur des règles. Les tickets de caisse se ressemblant en général beaucoup, cette approche fonctionne convenablement sur cette tâche quelque soit le ticket, mais elle présente tout de même des défauts de généralisabilité. Une approche <em>Deep Learning end-to-end</em> est préférable, même si elle nécessite des données annotées. De telles méthodes ont été testées dans le cadre de compétitions (notamment sur les jeux de données <a href="https://rrc.cvc.uab.es/?ch=13&amp;com=introduction">SROIE 2019</a> et <a href="https://github.com/clovaai/cord">Cord</a>) et ont donné de bons résultats.</p>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-chen-17" class="csl-entry" role="listitem">
Chen, Liang-Chieh, George Papandreou, Florian Schroff, and Hartwig Adam. 2017. <span>“Rethinking Atrous Convolution for Semantic Image Segmentation.”</span> <em>CoRR</em> abs/1706.05587. <a href="http://arxiv.org/abs/1706.05587">http://arxiv.org/abs/1706.05587</a>.
</div>
<div id="ref-plan-sat" class="csl-entry" role="listitem">
Direction de la recherche et de l’innovation, Commissariat général au développement durable –. 2018. <em>Plan d’applications Satellitaires 2018 - Des Solutions Spatiales Pour Connaître Le Territoire</em>.
</div>
<div id="ref-lecun-89" class="csl-entry" role="listitem">
LeCun, Y., B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard, and L. D. Jackel. 1989. <span>“Backpropagation Applied to Handwritten Zip Code Recognition.”</span> <em>Neural Computation</em> 1 (4): 541–51. <a href="https://doi.org/10.1162/neco.1989.1.4.541">https://doi.org/10.1162/neco.1989.1.4.541</a>.
</div>
<div id="ref-paliwal-20" class="csl-entry" role="listitem">
Paliwal, Shubham, Vishwanath D, Rohit Rahul, Monika Sharma, and Lovekesh Vig. 2020. <span>“TableNet: Deep Learning Model for End-to-End Table Detection and Tabular Data Extraction from Scanned Document Images.”</span> <em>CoRR</em> abs/2001.01469. <a href="http://arxiv.org/abs/2001.01469">http://arxiv.org/abs/2001.01469</a>.
</div>
<div id="ref-ronneberger-15" class="csl-entry" role="listitem">
Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. 2015. <span>“U-Net: Convolutional Networks for Biomedical Image Segmentation.”</span> <em>CoRR</em> abs/1505.04597. <a href="http://arxiv.org/abs/1505.04597">http://arxiv.org/abs/1505.04597</a>.
</div>
<div id="ref-sobel-73" class="csl-entry" role="listitem">
Sobel, Irwin, and Gary Feldman. 1973. <span>“A 3×3 Isotropic Gradient Operator for Image Processing.”</span> <em>Pattern Classification and Scene Analysis</em>, January, 271–72.
</div>
<div id="ref-steele-17" class="csl-entry" role="listitem">
Steele, Jessica E., Pål Roe Sundsøy, Carla Pezzulo, Victor A. Alegana, Tomas J. Bird, Joshua Blumenstock, Johannes Bjelland, et al. 2017. <span>“Mapping Poverty Using Mobile Phone and Satellite Data.”</span> <em>Journal of The Royal Society Interface</em> 14 (127): 20160690. <a href="https://doi.org/10.1098/rsif.2016.0690">https://doi.org/10.1098/rsif.2016.0690</a>.
</div>
<div id="ref-voulodimos-18" class="csl-entry" role="listitem">
Voulodimos, Athanasios, Nikolaos Doulamis, Anastasios Doulamis, and Eftychios Protopapadakis. 2018. <span>“Deep Learning for Computer Vision: A Brief Review.”</span> <em>Computational Intelligence and Neuroscience</em> 2018: 1–13. <a href="https://doi.org/10.1155/2018/7068349">https://doi.org/10.1155/2018/7068349</a>.
</div>
</div>


</section>
</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Plus précisément, les ménages enquêtés se voient confier un carnet de dépenses qu’ils doivent remplir pendant une certaine période. Pour certaines dépenses les carnets sont renseignés à la main par un membre du ménage. Pour d’autres, le ménage a la possibilité d’inclure dans le carnet des tickets de caisse. Jusqu’à présent les enquêteurs étaient chargés de recopier le contenu des tickets de caisse pour rendre ces données exploitables.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/InseeFrLab\.github\.io\/cours-nouvelles-donnees-site\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./textes_exemples.html" class="pagination-link" aria-label="Application 2">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text">Application 2</span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./images_exemples.html" class="pagination-link" aria-label="Application">
        <span class="nav-page-text">Application</span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<ol start="3" type="a">
<li>2022, Lino Galiana and Tom Seimandi</li>
</ol>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/InseeFrLab/cours-nouvelles-donnees-site/edit/main/images.qmd" class="toc-action"><i class="bi bi-github"></i>Edit this page</a></li><li><a href="https://github.com/InseeFrLab/cours-nouvelles-donnees-site/issues/new" class="toc-action"><i class="bi empty"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/InseeFrLab/cours-nouvelles-donnees-site">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>