---
title: "Données textuelles et non structurées"
---

Les données textuelles sont aujourd'hui parmi les types de données les
plus prometteurs pour la statistique publique
et l'un des champs les plus actifs de la recherche
en _data science_.
Pour cause, de plus en plus de services existent
sur le _web_ qui conduisent à la collecte de données textuelles. En outre,
des nouvelles méthodes pour collecter et traiter ces 
traces numériques particulières ont été développées dans les
dernières années.

Une partie des méthodes d'analyse qui appartiennent
à la palette des compétences des _data scientists_
spécialistes du traitement de données textuelles sont en réalité assez
anciennes.
Par exemple, la [distance de Levensthein](https://en.wikipedia.org/wiki/Levenshtein_distance)
a été proposée
pour la première fois en 1965, l'ancêtre des réseaux
de neurone actuels est le perceptron qui date de 1957, etc.[^1]
Néanmoins, le fait que certaines entreprises du net
basent leur _business model_ sur le traitement
et la valorisation de la donnée
textuelle, notamment Google, Facebook et X, a amené 
à renouveler le domaine. 

[^1]: Pour remonter plus loin dans la ligne du temps des
données textuelles, on peut penser
au [`Soundex`](https://en.wikipedia.org/wiki/Soundex),
un algorithme d'indexation des textes dans
les annuaires dont l'objectif était de permettre
de classer à la suite des noms qui ne déviaient que
par une différence typographique et non sonore.

La statistique publique s'appuie également sur la collecte
et le traitement de données textuelles. Les collectes
de données officielles ne demandent pas exclusivement
d'informations sous le forme de texte. Les premières informations
demandées sont généralement un état civil, une adresse, etc.
C'est ensuite, en fonction du thème de l'enquête, que d'autres
informations textuelles seront collectées : un nom
d'entreprise, un titre de profession, etc. Les données
administratives elles-aussi comportent souvent des informations
textuelles. Ces données défient l'analyse statistique car
cette dernière, qui vise à détecter des grandes structures
à partir d'observations multiples, doit s'adapter à la différence
des données textuelles : le langage est un champ où
certaines des notions usuelles de la statistique (distance, similarité notamment)
doivent être revues.

Ce chapitre propose un panorama très incomplet de l'apport
des données non structurées, principalement textuelles, 
pour la statistique et l'analyse de données. Nous évoquerons
plusieurs sources ou méthodes de collecte. Nous ferons
quelques détours par des exemples pour aller plus
loin.

# Webscraping

## Présentation

Le [webscraping](https://fr.wikipedia.org/wiki/Web_scraping) est une méthode de collecte de données qui repose
sur le moissonnage d'objets de grande dimension (des pages web)
afin d'en extraire des informations ponctuelles (du texte, des nombres...). Elle désigne les techniques d'extraction du contenu des sites Internet. C'est une pratique très utile pour toute personne souhaitant travailler sur des informations disponibles en ligne, mais n'existant pas forcément sous la forme de fichiers exportables.

## Enjeux pour la statistique publique

Le *webscraping* présente un certain nombre d'enjeux légaux, qui ne seront pas enseignés dans ce cours. En particulier, la Commission nationale de l'informatique et des libertés (CNIL) a publié en 2020 de nouvelles directives sur le *webscraping* reprécisant qu'aucune donnée ne peut être réutilisée à l'insu de la personne à laquelle elle appartient.

Le *webscraping* est un domaine où la reproductibilité est compliquée à mettre en oeuvre. Une page *web* évolue
régulièrement et d'une page web à l'autre, la structure peut
être très différente ce qui rend certains codes difficilement généralisables.
Par conséquent, la meilleure manière d'avoir un programme fonctionnel est
de comprendre la structure d'une page web et dissocier les éléments exportables
à d'autres cas d'usages des requêtes *ad hoc*.

Un code qui fonctionne aujourd'hui peut ainsi très bien ne plus fonctionner
au bout de quelques semaines. Il apparaît
préférable de privilégier les API
qui sont un accès en apparence plus compliqué mais en fait plus fiable à moyen terme.
Cette difficulté à construire une extraction de données pérenne par
_webscraping_ est une illustration du principe _"there is no free lunch"_.
La donnée est au cœur du business model de nombreux acteurs, il est donc logique que des restrictions au moissonnage soient mises en place. 

Les APIs sont un mode d'accès de plus en plus généralisé à des données.
Cela permet un lien direct entre fournisseurs et utilisateurs de données,
un peu sous la forme d'un contrat. Si les données sont ouvertes avec restrictions, on utilise des clés d'authentification.
Avec les API, on structure sa demande de données sous forme de requête paramétrée (source désirée, nombre de lignes, champs...)
et le fournisseur de données y répond, généralement sous la forme d'un résultat au format `JSON`. 
`Python` et `JavaScript` sont deux outils très populaires pour récupérer de la donnée
selon cette méthode.
Pour plus de détails, vous pouvez explorer le
[chapitre sur les API dans le cours de `Python` de l'ENSAE](https://pythonds.linogaliana.fr/api/).

On n'est pas à l'abri de mauvaises surprises avec les
APIs (indisponibilité, limite atteinte de requêtes...)
mais cela permet un lien plus direct avec la dernière donnée publiée par un producteur.
L'avantage de l'API est qu'il s'agit d'un service du fournisseur de données, qui en tant
que service va amener un producteur à essayer de répondre à une
demande dans la mesure du possible. 
Le _webscraping_ étant un mode d'accès à la donnée plus opportuniste, 
où le réel objectif du producteur de données n'est pas de fournir de la
donnée mais une page _web_, il n'y a aucune garantie de service ou 
de continuité. 


## Exemples dans la statistique publique

- Projet Jocas : scrapping des offres d'emplois.


## Implémentations

`Python` est le langage le plus utilisé
par les _scrappers_. 
`BeautifulSoup` sera suffisant quand vous voudrez travailler sur des pages HTML statiques. Dès que les informations que vous recherchez sont générées via l'exécution de scripts [`Javascript`](https://fr.wikipedia.org/wiki/JavaScript), il vous faudra passer par des outils comme [`Selenium`](https://selenium-python.readthedocs.io/) ou [`Playwright`](https://scrapfly.io/blog/posts/web-scraping-with-playwright-and-python).
De même, si vous ne connaissez pas l'URL, il faudra passer par un framework comme [`Scrapy`](https://scrapy.org/), qui passe facilement d'une page à une autre ("crawl"). Scrapy est plus complexe à manipuler que `BeautifulSoup` : si vous voulez plus de détails, rendez-vous sur la page du [tutoriel `Scrapy`](https://doc.scrapy.org/en/latest/intro/tutorial.html).
Pour plus de détails, voir le [TP sur le webscraping en 2e année de l'ENSAE](https://pythonds.linogaliana.fr/webscraping/).

Les utilisateurs de `R` privilégieront `httr` and `rvest` qui sont les packages les plus
utilisés. 
Il est intéressant d'accorder de l'attention à 
[`polite`](https://github.com/dmi3kno/polite). Ce package vise à récupérer
des données en suivant les [recommandations de bonnes pratiques](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01) sur 
le sujet, notamment de respecter les instructions dans
`robots.txt` (_"The three pillars of a polite session are seeking permission, taking slowly and never asking twice"_).

# Réseaux sociaux

Les réseaux sociaux sont l'une des sources textuelles
les plus communes. C'est leur usage à des fins commerciales
qui a amené les entreprises du net à renouveler
le champ de l'analyse textuelles qui bénéficie
au-delà de leur champ d'origine. 

Nous entrons un peu plus en détail sur ces données dans le chapitre **Nowcasting**.

# Les modèles de langage 

Un modèle de langage est un modèle statistique qui modélise la distribution de séquences de mots, plus généralement de séquences de symboles discrets (lettres, phonèmes, mots), dans une langue naturelle. Un des objectifs de ces modèles est
de pouvoir transformer des objets (textes) situés dans un espace d'origine de très grande dimension, qui de plus
utilise des éléments contextuels, en informations situés dans un espace de dimension réduite. Il s'agit ainsi de transformer des éléments d'un corpus,
par exemple des mots, en vecteurs multidimensionnels sur lequel on peut
ensuite par exemple appliquer des opérations arithmétiques. 
Un modèle de langage peut par exemple servir à prédire le mot suivant une séquence de mots
ou la similarité de sens entre deux phrases.

## Du bag of words aux modèles de langage

L'objectif du traitement du langage naturel (ou _Natural Langage Processing_ - NLP)
est de transformer une information de très haute
dimension (une langue est un objet éminemment complexe)
en information à dimension plus limitée qui peut
être exploitée par un ordinateur.

La première approche pour entrer dans l'analyse d'un 
texte est généralement l'approche _bag of words_ ou _topic modeling_.
Dans la première, il s'agit
de formaliser un texte sous forme d'un ensemble de mots
où on va piocher plus ou moins fréquemment dans un sac de 
mots possibles.
Dans la seconde, il s'agit de modéliser le processus de choix
de mots en deux étapes (modèle de mélange) : d'abord un choix
de thème puis, au sein de ce thème, un choix de mots plus ou
moins fréquents selon le thème.

Dans ces deux approches, l'objet central est la matrice document-terme. 
Elle formalise les fréquences d'occurrence de mots dans des textes ou
des thèmes. Néanmoins, il s'agit d'une matrice très creuse: même
un texte au vocabulaire très riche n'explore qu'une petite partie
du dictionnaire des mots possibles. 

L’idée derrière les plongements lexicaux (_embeddings_) est de proposer une information plus
condensée qui permet néanmoins de capturer les grandes structures
d'un texte. Il s'agit par exemple de résumer l'ensemble
d'un corpus en un nombre relativement restreint de dimensions. Ces
dimensions ne sont pas prédéterminées mais plutôt inférées
par un modèle qui essaie de trouver la meilleure partition 
des dimensions pour rapprocher les termes équivalents.
Chacune de ces dimensions va représenter un facteur latent, c’est à dire une variable inobservée, de la même manière que les composantes principales produites par une ACP.
Techniquement, au lieu de représenter les documents par des vecteurs sparse de très grande dimension (la taille du vocabulaire) comme on l’a fait jusqu’à présent, on va les représenter par des vecteurs denses (continus) de dimension réduite (en général, autour de 100-300).

## Intérêt des modèles de langage

Par exemple, un humain sait qu'un document contenant le mot _"Roi"_
et un autre document contenant le mot _"Reine"_ ont beaucoup de chance
d'aborder des sujets semblables.

::: {#fig-word2vec}
![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/w2v_vecto.png)

Schéma illustratif de `word2vec`.
:::

Pourtant, une vectorisation de type comptage ou TF-IDF
ne permet pas de saisir cette similarité :
le calcul d'une mesure de similarité (norme euclidienne ou similarité cosinus)
entre les deux vecteurs donnera une valeur très faible, puisque les mots utilisés sont différents.

A l'inverse, un modèle `word2vec` (voir @fig-word2vec) bien entraîné va capter
qu'il existe un facteur latent de type _"royauté"_,
et la similarité entre les vecteurs associés aux deux mots sera forte.

La magie va même plus loin : le modèle captera aussi qu'il existe un
facteur latent de type _"genre"_,
et va permettre de construire un espace sémantique dans lequel les
relations arithmétiques entre vecteurs ont du sens ;
par exemple (voir @fig-embeddings) :

$$\text{king} - \text{man} + \text{woman} ≈ \text{queen}$$

Chaque mot est représenté par un vecteur de taille fixe (comprenant $n$ nombres),
de façon à ce que deux mots dont le sens est proche possèdent des représentations numériques proches. Ainsi les mots « chat » et « chaton » devraient avoir des vecteurs de plongement assez similaires, eux-mêmes également assez proches de celui du mot « chien » et plus éloignés de la représentation du mot « maison ».

_Comment ces modèles sont-ils entraînés ?_

Via une tâche de prédiction résolue par un réseau de neurones simple.
L'idée fondamentale est que la signification d'un mot se comprend
en regardant les mots qui apparaissent fréquemment dans son voisinage.
Pour un mot donné, on va donc essayer de prédire les mots
qui apparaissent dans une fenêtre autour du mot cible.

En répétant cette tâche de nombreuses fois et sur un corpus suffisamment varié,
on obtient finalement des *embeddings* pour chaque mot du vocabulaire,
qui présentent les propriétés discutées précédemment.

::: {#fig-embeddings}
![](img/images/word_embedding.png){ width=70% }

Illustration des *word embeddings*.
:::

## Les modèles de langage aujourd'hui

Les modèles de langage aujourd’hui

La méthode de construction d’un plongement lexical présentée précédemment correspond à celle de l’algorithme [`Word2Vec`](https://fr.wikipedia.org/wiki/Word2vec), un modèle _open-source_ développé par une équipe de `Google` en 2013.
`Word2Vec` a constitué une avancée majeure en introduisant des représentations vectorielles denses apprises à partir des contextes d’apparition des mots, et a joué un rôle pionnier dans le développement des modèles de plongement lexical modernes.

Le modèle [`GloVe`](https://nlp.stanford.edu/projects/glove/) constitue une autre approche historique importante [@pennington2014glove]. Développé en 2014 à Stanford, il repose sur l’exploitation globale des statistiques de co-occurrences entre mots dans un corpus, via la construction puis l’optimisation d’une matrice de co-occurrences. Contrairement à `Word2Vec`, `GloVe` n’est pas formulé comme un réseau de neurones prédictif, mais comme un problème d’optimisation visant à approximer ces statistiques globales.

À partir de la fin des années 2010, les modèles de langage fondés sur l’architecture *Transformer* ont profondément modifié l’approche des plongements lexicaux. Le modèle de langage [`BERT`](https://jalammar.github.io/illustrated-bert/), développé par `Google` en 2019 [@devlin2018bert], produit des plongements contextualisés, c’est-à-dire dépendants du contexte d’apparition du mot dans la phrase, contrairement aux modèles statiques comme `Word2Vec` ou `GloVe`. Plusieurs déclinaisons multilingues ou spécialisées ont depuis été développées, notamment pour le français, telles que [`CamemBERT`](https://camembert-model.fr/) ou [`FlauBERT`](https://github.com/getalp/Flaubert).

Le modèle [`FastText`](https://fasttext.cc/), développé en 2016 par une équipe de `Facebook AI Research`, s’inscrit dans la continuité de `Word2Vec` tout en introduisant des améliorations importantes. En plus des mots, il apprend des représentations de sous-unités lexicales, en particulier des n-grams de caractères (sous-séquences de caractères de taille \\(n\\), par exemple _« tar »_, _« art »_ et _« rte »_ sont les trigrammes du mot _« tarte »_), mais également des n-grams de mots. Cette modélisation rend le modèle plus robuste aux variations morphologiques, aux fautes d’orthographe, aux mots rares et à certaines expressions multi-mots. Son implémentation a également été optimisée pour permettre un entraînement rapide sur de grands corpus.

Plus récemment, les modèles de langage génératifs de grande taille (*Large Language Models*, LLMs), basés sur l’architecture Transformer, ont marqué un changement d’échelle. Le modèle `GPT-3` (*Generative Pre-trained Transformer 3*), développé par la société `OpenAI` et rendu public en 2020 [@brown2020language], compte 175 milliards de paramètres et a servi de point de départ à de nombreuses applications industrielles. Ces modèles sont aujourd’hui utilisés pour la génération de texte, de code, le résumé automatique ou l’assistance conversationnelle, par exemple dans `GitHub Copilot`.

Au-delà de la famille des modèles GPT, d’autres grands modèles de langage ont été développés par différents acteurs industriels et académiques, tels que `Claude` (Anthropic), `Mistral`, `Grok` (xAI) ou les modèles utilisés par `Perplexity`. Bien qu’ils reposent sur des principes architecturaux similaires, ces modèles diffèrent par leur taille, leurs données d’entraînement, leurs objectifs d’optimisation et leurs domaines d’application.

Dans ce contexte, le champ du _prompt engineering_ s’est fortement développé. Il regroupe un ensemble de pratiques visant à formuler efficacement des instructions textuelles afin d’exploiter les capacités des modèles de langage de grande taille, notamment pour extraire des informations pertinentes, discriminer les thèmes d’un texte ou guider la génération de contenus, sans recourir à un entraînement spécifique supplémentaire.


## Utilisation dans un processus de création de contenu créatif 

La publication par l'organisation [Open AI](https://openai.com/) de
ses premiers modèles de génération d’images à partir de texte, en particulier
[DALL·E 2](https://openai.com/dall-e-2/)
(un jeu de mots mêlant Dali et WALL·E), a suscité un intérêt médiatique et scientifique inédit
autour des usages créatifs de l’intelligence artificielle.
Ces modèles ont contribué à rendre visibles au grand public les capacités génératives
des modèles d’apprentissage profond, au-delà des tâches traditionnelles de classification ou de prédiction.

Un compte X ([Weird Dall-E Mini Generations](https://twitter.com/weirddalle))
a ainsi proposé de nombreuses générations d’images à la fois surprenantes,
drôles ou incongrues, illustrant le potentiel créatif — mais aussi les limites —
de ces modèles. Le blogueur tech Casey Newton a notamment évoqué une
[révolution créative dans le monde de l’IA](https://www.platformer.news/p/how-dall-e-could-power-a-creative),
liée à la démocratisation de ces outils auprès d’artistes, de designers et de créateurs de contenu.

La @fig-shiba montre un exemple d’image générée par `DALL·E 2`.

::: {#fig-shiba}
![](https://upload.wikimedia.org/wikipedia/commons/2/2b/A_Shiba_Inu_dog_wearing_a_beret_and_black_turtleneck_DALLE2.jpg){ width=70% }

_"A Shiba Inu dog wearing a beret and black turtleneck"_
:::

Les modèles générateurs d’images tels que `DALL·E`, `Stable Diffusion` ou leurs successeurs
reposent, schématiquement, sur une architecture en plusieurs étapes :

- le contenu textuel de la requête est analysé et encodé par un modèle de langage
de grande taille (par exemple un modèle de la famille `GPT` ou un encodeur de type Transformer) ;
- cette représentation latente du texte est ensuite utilisée pour guider un modèle
génératif d’images, entraîné sur de vastes corpus image–texte, afin de produire
une image cohérente avec la description fournie.

![](https://raw.githubusercontent.com/patrickvonplaten/scientific_images/master/stable_diffusion.png){ fig-align="center" }

`Stable Diffusion` constitue une alternative plus ouverte et plus accessible que `DALL·E`,
notamment pour les utilisateurs de `Python`, car il peut être exécuté localement
ou via des plateformes open source, et adapté à des usages spécifiques
(par exemple par *fine-tuning* ou contrôle du style).

![](https://huggingface.co/blog/assets/98_stable_diffusion/stable_diffusion_12_1.png){ fig-align="center" }

Si vous êtes intéressés par ce type de modèles, vous pouvez
[tester les exemples du cours de Python de l’ENSAE](https://pythonds.linogaliana.fr/dalle/).
Par exemple, l’instruction  
__"Chuck Norris fighting against Zeus on Mount Olympus in an epic Mortal Kombat scene"__  
permet de générer une image telle que celle présentée ci-dessous, ou bien
d’explorer librement vos propres descriptions textuelles :

![](https://minio.lab.sspcloud.fr/lgaliana/generative-art/pythonds/chuck.png){ fig-align="center" }

::: {.callout-note}

**Encadré – Enjeux et craintes dans le monde créatif**

Le développement rapide des modèles génératifs suscite de fortes inquiétudes dans les industries culturelles et créatives.
Les artistes, auteurs et musiciens s’interrogent sur l’utilisation de leurs œuvres pour l’entraînement de ces modèles,
souvent sans consentement explicite ni rémunération. Des craintes portent également sur la substitution partielle
de la création humaine par des contenus générés automatiquement (images, musiques, scénarios),
ainsi que sur les implications juridiques en matière de droits d’auteur et de propriété intellectuelle.
:::




## Modèles de langage dans la statistique publique

Au sein de la statistique publique, le principal cas d'usage de l'analyse textuelle est la codification automatique.
La codification automatique consiste à classer des libellés textuels dans des nomenclatures officielles (et généralement hiérarchiques).

Voici les principaux projets de codification automatique à ce jour :

- catégorisation des professions des individus dans la nomenclature des PCS (*Professions et catégories socioprofessionnelles en France*) ;
- catégorisation de l'activité des entreprises dans la nomenclature d'activité APE (*Activité principale des entreprises* issue de la Nomenclature des activités françaises) ;
- catégorisation des produits de consommation dans la nomenclature COICOP (*Classification of Individual Consumption by Purpose*).
- Catégorisation des causes de décès à partir des certificats de décès.
- catégorisation des compétences dans les annonces d'offres d'emploi.
- etc.

A `l’Insee`, plusieurs modèles de classification de libellés textuels dans des nomenclatures reposent sur l’algorithme de plongement lexical [`FastText`](https://fasttext.cc/). Bien que l'Insee sorte progressivement de l'utilisation de cet librairie (dont la maintenance a cessé), les fondements méthodologiques restent au coeur du nouvel outil développé et maintenu en interne : [`torchTextClassifiers`](https://github.com/InseeFrLab/torchTextClassifiers)


- pour apparier des sources à partir de champs textuels qui ne sont pas nécessairement identiques ;
- pour catégoriser des données dans une nomenclature normalisée à partir de champs libres.


# Conclusion

Les données textuelles, de plus en plus disponibles, offrent des opportunités inédites pour la statistique publique, mais nécessitent des méthodes adaptées. Leur analyse, autrefois limitée à des outils simples, s’appuie désormais sur des modèles avancés (embeddings, classification automatique) pour structurer et exploiter ces informations non organisées.
Le webscraping et les API permettent d’accéder à ces données, mais posent des défis techniques (reproductibilité, évolutivité) et éthiques (respect des droits, qualité). Les modèles de langage modernes transforment ces textes en données exploitables, facilitant la codification automatique (nomenclatures PCS, APE, etc.) et l’analyse sémantique.
Cependant, leur utilisation exige de concilier innovation et rigueur méthodologique, notamment pour garantir la fiabilité des résultats et la protection des données. Ce champ reste en évolution, avec un potentiel majeur pour automatiser et enrichir les processus statistiques.

# References

::: {#refs}
:::
